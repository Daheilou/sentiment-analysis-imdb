{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeled_train_clean_reviews_stemmed.csv'\n",
    "TEST_DF = BASE_DIR + 'test_clean_reviews_stemmed.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews and 25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0)\n",
    "test = pd.read_csv(TEST_DF, header = 0)\n",
    "# Prevent 'float' object has no attribute 'lower' error in keras tokenizer\n",
    "# https://stackoverflow.com/questions/34724246/attributeerror-float-object-has-no-attribute-lower\n",
    "labeled_train[\"review\"] = labeled_train[\"review\"].astype(str)\n",
    "test[\"review\"] = test[\"review\"].astype(str)\n",
    "print \"Read %d labeled train reviews and %d test reviews\" % (labeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage\n",
    "\n",
    "Check if test[\"sentiment\"] is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Kaggle tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the word2vec model vocabulary: 11986\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 300  # Word vector dimensionality\n",
    "MIN_WORD_COUNT = 40  # Minimum word count. Kaggle set to 40, to avoid attaching too much importance to individual movie titles.\n",
    "NUM_THREADS = 4  # Number of threads to run in parallel\n",
    "CONTEXT = 10  # Context window size\n",
    "DOWNSAMPLING = 1e-3  # Downsample setting for frequent words\n",
    "WORD2VEC_MODEL_FILE = BASE_DIR + \\\n",
    "    \"word2vec_model_\" + \\\n",
    "    str(EMBEDDING_DIM) + \"dim_\" + \\\n",
    "    str(MIN_WORD_COUNT) + \"minwords_\" + \\\n",
    "    str(CONTEXT) + \"context_\" + \\\n",
    "    \"stemmed\"\n",
    "\n",
    "word2vec_model = Word2Vec.load(WORD2VEC_MODEL_FILE)\n",
    "\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "word2vec_model.init_sims(replace=True)\n",
    "\n",
    "print(\"Number of words in the word2vec model vocabulary: %d\" % len(word2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "Keras Tokenizer found 70223 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# We vectorize the text corpus by turning each text into a sequence of integers\n",
    "# Each integer is the index of a token in the dictionary\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NUM_WORDS_FOR_KERAS_TOKENIZER = 200000\n",
    "#\n",
    "# num_words: the maximum number of words to keep, based on frequency.\n",
    "keras_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS_FOR_KERAS_TOKENIZER)\n",
    "#\n",
    "\n",
    "labeled_train_reviews = labeled_train[\"review\"].tolist()\n",
    "test_reviews = test[\"review\"].tolist()\n",
    "all_reviews = labeled_train_reviews + test_reviews\n",
    "\n",
    "print(len(all_reviews))\n",
    "\n",
    "\n",
    "# fit_on_texts accepts a list of strings, a generator of strings or \n",
    "# a list of list of strings. In the last case, it assumes each entry of the lists to be a token.\n",
    "# Here we provide a list of strings.\n",
    "keras_tokenizer.fit_on_texts(all_reviews)\n",
    "word_index = keras_tokenizer.word_index\n",
    "print('Keras Tokenizer found %s unique tokens' % len(word_index))\n",
    "# Not stemming: 101376 unique tokens. Stemming: 70223 unique tokens\n",
    "#\n",
    "# texts_to_sequences transforms each text in texts to a sequence of integers.\n",
    "train_sequences = keras_tokenizer.texts_to_sequences(labeled_train_reviews)\n",
    "test_sequences = keras_tokenizer.texts_to_sequences(test_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pad all text sequences to the same length.\n",
    "# By default zeros are padded at the front.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set max length for each review sequence.\n",
    "MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN = 500\n",
    "\n",
    "train_pad_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)\n",
    "test_pad_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 58269\n"
     ]
    }
   ],
   "source": [
    "# Prepare word embedding matrix\n",
    "\n",
    "# Choose the smaller number of the two as column length of the matrix\n",
    "num_words = min(MAX_NUM_WORDS_FOR_KERAS_TOKENIZER, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv.vocab:\n",
    "        embedding_matrix[i] = word2vec_model.wv.get_vector(word)\n",
    "# Null word embeddings are words that don't exist in the embedding matrix\n",
    "# and are therefore represented as zero vectors.\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly split 20000 pad sequences for training, 5000 for validation\n"
     ]
    }
   ],
   "source": [
    "# Split train_sequences into train and validation. Ratio: 80/20\n",
    "VALIDATION_SPLIT = 0.2\n",
    "np.random.seed(1234)\n",
    "\n",
    "# \n",
    "perm = np.random.permutation(len(train_sequences))\n",
    "index_train = perm[:int(len(train_sequences)*(1-VALIDATION_SPLIT))]\n",
    "index_val = perm[int(len(train_sequences)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "x_train = train_pad_sequences[index_train]\n",
    "x_val = train_pad_sequences[index_val]\n",
    "y_train = labeled_train[\"sentiment\"][index_train].tolist()\n",
    "y_val = labeled_train[\"sentiment\"][index_val].tolist()\n",
    "\n",
    "print('Randomly split %d pad sequences for training, %d for validation' % (len(x_train) ,len(x_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_pad_sequences\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rnn_model():\n",
    "    model = Sequential()\n",
    "    \n",
    "    LSTM_UNITS = 100\n",
    "    LSTM_DROPOUT = 0.\n",
    "    LSTM_RECCURENT_DROPOUT = 0.\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    lstm_layer = LSTM(LSTM_UNITS, dropout=LSTM_DROPOUT, \n",
    "                      recurrent_dropout=LSTM_RECCURENT_DROPOUT)\n",
    "    bilstm_layer = Bidirectional(lstm_layer)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(bilstm_layer)\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_model = rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 300)          21067200  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200)               320800    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 201       \n",
      "=================================================================\n",
      "Total params: 21,388,201\n",
      "Trainable params: 321,001\n",
      "Non-trainable params: 21,067,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 521s 26ms/step - loss: 0.4818 - acc: 0.7703 - val_loss: 0.6006 - val_acc: 0.6932\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 509s 25ms/step - loss: 0.4132 - acc: 0.8170 - val_loss: 0.3484 - val_acc: 0.8550\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 509s 25ms/step - loss: 0.4204 - acc: 0.8134 - val_loss: 0.4256 - val_acc: 0.8348\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 507s 25ms/step - loss: 0.3928 - acc: 0.8284 - val_loss: 0.3534 - val_acc: 0.8598\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 513s 26ms/step - loss: 0.3639 - acc: 0.8466 - val_loss: 0.4019 - val_acc: 0.8238\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 511s 26ms/step - loss: 0.3282 - acc: 0.8657 - val_loss: 0.3019 - val_acc: 0.8792\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 512s 26ms/step - loss: 0.3264 - acc: 0.8649 - val_loss: 0.3316 - val_acc: 0.8734\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 509s 25ms/step - loss: 0.3182 - acc: 0.8718 - val_loss: 0.2897 - val_acc: 0.8848\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 508s 25ms/step - loss: 0.2936 - acc: 0.8813 - val_loss: 0.3533 - val_acc: 0.8502\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 507s 25ms/step - loss: 0.2951 - acc: 0.8822 - val_loss: 0.3242 - val_acc: 0.8686\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7f23fb1a50>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size: number of samples per gradient update\n",
    "lstm_model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_lstm = lstm_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_binary_lstm = map(lambda predict: 1 if predict > 0.5 else 0, y_test_pred_lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for LSTM model is : 0.8715.\n"
     ]
    }
   ],
   "source": [
    "print(\"The AUC score for LSTM model is : %.4f.\" %roc_auc_score(y_test, y_test_pred_binary_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote bilstm_100units_2dropout_10epoch.csv\n"
     ]
    }
   ],
   "source": [
    "%%script false\n",
    "# Write the test results\n",
    "# output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": y_test_pred_binary_lstm})\n",
    "# output.to_csv(os.path.join('../', 'output', \"bilstm_100units_2dropout_10epoch.csv\"), index=False, quoting=3)\n",
    "# print \"Wrote bilstm_100units_2dropout_10epoch.csv\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
