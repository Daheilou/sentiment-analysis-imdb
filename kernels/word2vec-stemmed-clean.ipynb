{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec_model_300dim_40minwords_10context', 'sampleSubmission.csv', 'word2vec_model_300dim_40minwords_10context_stemmed', 'labeledTrainData.tsv', 'test_submission.csv', 'testData.tsv', 'unlabeledTrainData.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeledTrainData.tsv'\n",
    "UNLABELED_TRAIN_DF = BASE_DIR + 'unlabeledTrainData.tsv'\n",
    "TEST_DF = BASE_DIR + 'testData.tsv'\n",
    "print(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 50000 unlabeled train reviews, and 25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv(UNLABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "test = pd.read_csv(TEST_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "print \"Read %d labeled train reviews, %d unlabeled train reviews, \" \\\n",
    "          \"and %d test reviews\" % (labeled_train[\"review\"].size, unlabeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage\n",
    "\n",
    "Check if test[\"sentiment\"] is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Kaggle tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def review_to_clean_review(review, remove_numbers=True, stem_words=False):\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    \n",
    "    if remove_numbers:\n",
    "        review_text = re.sub(\"[0-9]\", \" \", review_text)\n",
    "    else:\n",
    "        review_text = review_text.replace('0', ' zero ')\n",
    "        review_text = review_text.replace('1', ' one ')\n",
    "        review_text = review_text.replace('2', ' two ')\n",
    "        review_text = review_text.replace('3', ' three ')\n",
    "        review_text = review_text.replace('4', ' four ')\n",
    "        review_text = review_text.replace('5', ' five ')\n",
    "        review_text = review_text.replace('6', ' six ')\n",
    "        review_text = review_text.replace('7', ' seven ')\n",
    "        review_text = review_text.replace('8', ' eight ')\n",
    "        review_text = review_text.replace('9', ' nine ')\n",
    "    \n",
    "    review_text = review_text.lower()\n",
    "    \n",
    "    if stem_words:\n",
    "        words = review_text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        review_text = \" \".join(words)\n",
    "        \n",
    "    return(review_text)\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "\n",
    "    words = review_to_clean_review(review, remove_numbers, stem_words).split()\n",
    "\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords, remove_numbers, stem_words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labled train review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:272: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labled train review 5000 of 25000\n",
      "Labled train review 10000 of 25000\n",
      "Labled train review 15000 of 25000\n",
      "Labled train review 20000 of 25000\n",
      "Unlabled train review 0 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 5000 of 50000\n",
      "Unlabled train review 10000 of 50000\n",
      "Unlabled train review 15000 of 50000\n",
      "Unlabled train review 20000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 25000 of 50000\n",
      "Unlabled train review 30000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:272: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 35000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 40000 of 50000\n",
      "Unlabled train review 45000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# Initialize an empty list of sentences\n",
    "sentences = []\n",
    "# Parsing sentences from training set\n",
    "counter = 0.\n",
    "for review in labeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer, remove_numbers=False, stem_words=True)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Labled train review %d of %d\" % (counter, len(labeled_train[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "counter = 0.\n",
    "# Parsing sentences from unlabeled set\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer, remove_numbers=False, stem_words=True)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Unlabled train review %d of %d\" % (counter, len(unlabeled_train[\"review\"]))\n",
    "    counter = counter + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(np.random.choice(sentences, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'this is strang to say the least as everyon can clear see that uma thurman doe not belong under the categori of hot rainn wilson s perform is also far from hot'\n",
      " u'especi sinc the end result is their hair be straighten'\n",
      " u'but then whi not just get up and walk across the countri to watch tv in anoth state'\n",
      " u'the film also has a veri good sens of humour and it made me laugh'\n",
      " u'this vignett deal with the fragil hold of saniti on the conscious mind'\n",
      " u'the perform were brilliant it is question whi no nomin were offer'\n",
      " u'and at least those show were meant for children this show howev is by far wors'\n",
      " u'the cast is surpris good the plot move well and the contrast involv in the film are strike there are two main subplot which revolv around the one nine four one pearl harbor attack first is the relationship between mp colonel jason forrest his wife and vari other under forrest s command notabl his xo captain calvin lanford a particular effect perform by robert wagner'\n",
      " u'i first saw this as it was origin present on tv in one nine seven eight'\n",
      " u'the bad bit the rest boy doe it draaaaaaaaagggggg']\n"
     ]
    }
   ],
   "source": [
    "output_texts = [\" \".join(sentence) for sentence in sentences]\n",
    "print(np.random.choice(output_texts, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_texts = [\"\".join(sentence) for sentence in sentences]\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": sentences})\n",
    "output.to_csv(os.path.join(os.path.dirname(__file__), '../', 'output', \"Word2Vec_AverageVectors.csv\"), index=False, quoting=3)\n",
    "print \"Wrote clean.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 06:29:16,103 : INFO : collecting all words and their counts\n",
      "2019-01-06 06:29:16,105 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-06 06:29:16,178 : INFO : PROGRESS: at sentence #10000, processed 229166 words, keeping 12465 word types\n",
      "2019-01-06 06:29:16,236 : INFO : PROGRESS: at sentence #20000, processed 458297 words, keeping 17070 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Word2Vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 06:29:16,293 : INFO : PROGRESS: at sentence #30000, processed 680652 words, keeping 20370 word types\n",
      "2019-01-06 06:29:16,354 : INFO : PROGRESS: at sentence #40000, processed 910089 words, keeping 23125 word types\n",
      "2019-01-06 06:29:16,412 : INFO : PROGRESS: at sentence #50000, processed 1132423 words, keeping 25365 word types\n",
      "2019-01-06 06:29:16,471 : INFO : PROGRESS: at sentence #60000, processed 1356968 words, keeping 27283 word types\n",
      "2019-01-06 06:29:16,529 : INFO : PROGRESS: at sentence #70000, processed 1583238 words, keeping 29024 word types\n",
      "2019-01-06 06:29:16,588 : INFO : PROGRESS: at sentence #80000, processed 1805500 words, keeping 30603 word types\n",
      "2019-01-06 06:29:16,648 : INFO : PROGRESS: at sentence #90000, processed 2032837 words, keeping 32223 word types\n",
      "2019-01-06 06:29:16,708 : INFO : PROGRESS: at sentence #100000, processed 2257707 words, keeping 33579 word types\n",
      "2019-01-06 06:29:16,768 : INFO : PROGRESS: at sentence #110000, processed 2480677 words, keeping 34827 word types\n",
      "2019-01-06 06:29:16,828 : INFO : PROGRESS: at sentence #120000, processed 2705814 words, keeping 36183 word types\n",
      "2019-01-06 06:29:16,890 : INFO : PROGRESS: at sentence #130000, processed 2934773 words, keeping 37353 word types\n",
      "2019-01-06 06:29:16,951 : INFO : PROGRESS: at sentence #140000, processed 3150424 words, keeping 38376 word types\n",
      "2019-01-06 06:29:17,017 : INFO : PROGRESS: at sentence #150000, processed 3379002 words, keeping 39556 word types\n",
      "2019-01-06 06:29:17,082 : INFO : PROGRESS: at sentence #160000, processed 3604642 words, keeping 40629 word types\n",
      "2019-01-06 06:29:17,141 : INFO : PROGRESS: at sentence #170000, processed 3831024 words, keeping 41628 word types\n",
      "2019-01-06 06:29:17,202 : INFO : PROGRESS: at sentence #180000, processed 4054617 words, keeping 42599 word types\n",
      "2019-01-06 06:29:17,269 : INFO : PROGRESS: at sentence #190000, processed 4282897 words, keeping 43461 word types\n",
      "2019-01-06 06:29:17,341 : INFO : PROGRESS: at sentence #200000, processed 4509999 words, keeping 44301 word types\n",
      "2019-01-06 06:29:17,402 : INFO : PROGRESS: at sentence #210000, processed 4734668 words, keeping 45212 word types\n",
      "2019-01-06 06:29:17,470 : INFO : PROGRESS: at sentence #220000, processed 4962787 words, keeping 46134 word types\n",
      "2019-01-06 06:29:17,529 : INFO : PROGRESS: at sentence #230000, processed 5188480 words, keeping 46986 word types\n",
      "2019-01-06 06:29:17,590 : INFO : PROGRESS: at sentence #240000, processed 5419180 words, keeping 47854 word types\n",
      "2019-01-06 06:29:17,650 : INFO : PROGRESS: at sentence #250000, processed 5636219 words, keeping 48699 word types\n",
      "2019-01-06 06:29:17,709 : INFO : PROGRESS: at sentence #260000, processed 5859234 words, keeping 49469 word types\n",
      "2019-01-06 06:29:17,768 : INFO : PROGRESS: at sentence #270000, processed 6083817 words, keeping 50416 word types\n",
      "2019-01-06 06:29:17,828 : INFO : PROGRESS: at sentence #280000, processed 6312682 words, keeping 51640 word types\n",
      "2019-01-06 06:29:17,896 : INFO : PROGRESS: at sentence #290000, processed 6538799 words, keeping 52754 word types\n",
      "2019-01-06 06:29:17,963 : INFO : PROGRESS: at sentence #300000, processed 6766532 words, keeping 53755 word types\n",
      "2019-01-06 06:29:18,031 : INFO : PROGRESS: at sentence #310000, processed 6994910 words, keeping 54734 word types\n",
      "2019-01-06 06:29:18,093 : INFO : PROGRESS: at sentence #320000, processed 7222831 words, keeping 55770 word types\n",
      "2019-01-06 06:29:18,160 : INFO : PROGRESS: at sentence #330000, processed 7447731 words, keeping 56687 word types\n",
      "2019-01-06 06:29:18,229 : INFO : PROGRESS: at sentence #340000, processed 7680440 words, keeping 57629 word types\n",
      "2019-01-06 06:29:18,294 : INFO : PROGRESS: at sentence #350000, processed 7906671 words, keeping 58485 word types\n",
      "2019-01-06 06:29:18,361 : INFO : PROGRESS: at sentence #360000, processed 8130069 words, keeping 59345 word types\n",
      "2019-01-06 06:29:18,428 : INFO : PROGRESS: at sentence #370000, processed 8360199 words, keeping 60161 word types\n",
      "2019-01-06 06:29:18,490 : INFO : PROGRESS: at sentence #380000, processed 8588323 words, keeping 61069 word types\n",
      "2019-01-06 06:29:18,550 : INFO : PROGRESS: at sentence #390000, processed 8821215 words, keeping 61810 word types\n",
      "2019-01-06 06:29:18,609 : INFO : PROGRESS: at sentence #400000, processed 9047140 words, keeping 62546 word types\n",
      "2019-01-06 06:29:18,670 : INFO : PROGRESS: at sentence #410000, processed 9271446 words, keeping 63263 word types\n",
      "2019-01-06 06:29:18,730 : INFO : PROGRESS: at sentence #420000, processed 9495586 words, keeping 64024 word types\n",
      "2019-01-06 06:29:18,791 : INFO : PROGRESS: at sentence #430000, processed 9726512 words, keeping 64795 word types\n",
      "2019-01-06 06:29:18,853 : INFO : PROGRESS: at sentence #440000, processed 9956570 words, keeping 65539 word types\n",
      "2019-01-06 06:29:18,913 : INFO : PROGRESS: at sentence #450000, processed 10183262 words, keeping 66378 word types\n",
      "2019-01-06 06:29:18,975 : INFO : PROGRESS: at sentence #460000, processed 10419261 words, keeping 67158 word types\n",
      "2019-01-06 06:29:19,036 : INFO : PROGRESS: at sentence #470000, processed 10650184 words, keeping 67775 word types\n",
      "2019-01-06 06:29:19,094 : INFO : PROGRESS: at sentence #480000, processed 10873555 words, keeping 68500 word types\n",
      "2019-01-06 06:29:19,155 : INFO : PROGRESS: at sentence #490000, processed 11103736 words, keeping 69256 word types\n",
      "2019-01-06 06:29:19,214 : INFO : PROGRESS: at sentence #500000, processed 11328543 words, keeping 69892 word types\n",
      "2019-01-06 06:29:19,276 : INFO : PROGRESS: at sentence #510000, processed 11556927 words, keeping 70593 word types\n",
      "2019-01-06 06:29:19,337 : INFO : PROGRESS: at sentence #520000, processed 11783512 words, keeping 71267 word types\n",
      "2019-01-06 06:29:19,396 : INFO : PROGRESS: at sentence #530000, processed 12011039 words, keeping 71877 word types\n",
      "2019-01-06 06:29:19,455 : INFO : PROGRESS: at sentence #540000, processed 12238852 words, keeping 72537 word types\n",
      "2019-01-06 06:29:19,514 : INFO : PROGRESS: at sentence #550000, processed 12467490 words, keeping 73212 word types\n",
      "2019-01-06 06:29:19,572 : INFO : PROGRESS: at sentence #560000, processed 12691615 words, keeping 73861 word types\n",
      "2019-01-06 06:29:19,633 : INFO : PROGRESS: at sentence #570000, processed 12923711 words, keeping 74431 word types\n",
      "2019-01-06 06:29:19,694 : INFO : PROGRESS: at sentence #580000, processed 13148083 words, keeping 75087 word types\n",
      "2019-01-06 06:29:19,754 : INFO : PROGRESS: at sentence #590000, processed 13376619 words, keeping 75733 word types\n",
      "2019-01-06 06:29:19,814 : INFO : PROGRESS: at sentence #600000, processed 13601946 words, keeping 76294 word types\n",
      "2019-01-06 06:29:19,873 : INFO : PROGRESS: at sentence #610000, processed 13825965 words, keeping 76952 word types\n",
      "2019-01-06 06:29:19,935 : INFO : PROGRESS: at sentence #620000, processed 14055595 words, keeping 77503 word types\n",
      "2019-01-06 06:29:19,995 : INFO : PROGRESS: at sentence #630000, processed 14283003 words, keeping 78066 word types\n",
      "2019-01-06 06:29:20,056 : INFO : PROGRESS: at sentence #640000, processed 14506788 words, keeping 78692 word types\n",
      "2019-01-06 06:29:20,116 : INFO : PROGRESS: at sentence #650000, processed 14735829 words, keeping 79295 word types\n",
      "2019-01-06 06:29:20,175 : INFO : PROGRESS: at sentence #660000, processed 14961517 words, keeping 79864 word types\n",
      "2019-01-06 06:29:20,235 : INFO : PROGRESS: at sentence #670000, processed 15187871 words, keeping 80381 word types\n",
      "2019-01-06 06:29:20,295 : INFO : PROGRESS: at sentence #680000, processed 15415849 words, keeping 80912 word types\n",
      "2019-01-06 06:29:20,356 : INFO : PROGRESS: at sentence #690000, processed 15640987 words, keeping 81482 word types\n",
      "2019-01-06 06:29:20,417 : INFO : PROGRESS: at sentence #700000, processed 15872499 words, keeping 82074 word types\n",
      "2019-01-06 06:29:20,476 : INFO : PROGRESS: at sentence #710000, processed 16098551 words, keeping 82560 word types\n",
      "2019-01-06 06:29:20,538 : INFO : PROGRESS: at sentence #720000, processed 16326874 words, keeping 83036 word types\n",
      "2019-01-06 06:29:20,600 : INFO : PROGRESS: at sentence #730000, processed 16556101 words, keeping 83571 word types\n",
      "2019-01-06 06:29:20,661 : INFO : PROGRESS: at sentence #740000, processed 16780279 words, keeping 84127 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 06:29:20,719 : INFO : PROGRESS: at sentence #750000, processed 17001852 words, keeping 84599 word types\n",
      "2019-01-06 06:29:20,778 : INFO : PROGRESS: at sentence #760000, processed 17224202 words, keeping 85068 word types\n",
      "2019-01-06 06:29:20,842 : INFO : PROGRESS: at sentence #770000, processed 17454566 words, keeping 85644 word types\n",
      "2019-01-06 06:29:20,904 : INFO : PROGRESS: at sentence #780000, processed 17687962 words, keeping 86160 word types\n",
      "2019-01-06 06:29:20,966 : INFO : PROGRESS: at sentence #790000, processed 17918070 words, keeping 86665 word types\n",
      "2019-01-06 06:29:21,002 : INFO : collected 86996 word types from a corpus of 18042771 raw words and 795538 sentences\n",
      "2019-01-06 06:29:21,004 : INFO : Loading a fresh vocabulary\n",
      "2019-01-06 06:29:21,069 : INFO : effective_min_count=40 retains 11986 unique words (13% of original 86996, drops 75010)\n",
      "2019-01-06 06:29:21,070 : INFO : effective_min_count=40 leaves 17678536 word corpus (97% of original 18042771, drops 364235)\n",
      "2019-01-06 06:29:21,103 : INFO : deleting the raw counts dictionary of 86996 items\n",
      "2019-01-06 06:29:21,106 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2019-01-06 06:29:21,108 : INFO : downsampling leaves estimated 13094829 word corpus (74.1% of prior 17678536)\n",
      "2019-01-06 06:29:21,135 : INFO : estimated required memory for 11986 words and 300 dimensions: 34759400 bytes\n",
      "2019-01-06 06:29:21,137 : INFO : resetting layer weights\n",
      "2019-01-06 06:29:21,330 : INFO : training model with 4 workers on 11986 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-06 06:29:22,337 : INFO : EPOCH 1 - PROGRESS: at 4.96% examples, 650608 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:23,341 : INFO : EPOCH 1 - PROGRESS: at 10.23% examples, 664484 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:24,347 : INFO : EPOCH 1 - PROGRESS: at 15.45% examples, 668513 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:25,365 : INFO : EPOCH 1 - PROGRESS: at 20.72% examples, 668633 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:26,366 : INFO : EPOCH 1 - PROGRESS: at 25.88% examples, 669493 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:27,369 : INFO : EPOCH 1 - PROGRESS: at 31.10% examples, 671013 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:28,373 : INFO : EPOCH 1 - PROGRESS: at 36.34% examples, 671885 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:29,383 : INFO : EPOCH 1 - PROGRESS: at 41.52% examples, 672215 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:30,384 : INFO : EPOCH 1 - PROGRESS: at 46.63% examples, 672296 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:31,387 : INFO : EPOCH 1 - PROGRESS: at 51.87% examples, 673632 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:32,390 : INFO : EPOCH 1 - PROGRESS: at 57.03% examples, 674121 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:33,390 : INFO : EPOCH 1 - PROGRESS: at 62.16% examples, 674706 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:34,392 : INFO : EPOCH 1 - PROGRESS: at 67.35% examples, 675083 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:35,398 : INFO : EPOCH 1 - PROGRESS: at 72.60% examples, 675696 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:36,408 : INFO : EPOCH 1 - PROGRESS: at 77.86% examples, 676124 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:37,409 : INFO : EPOCH 1 - PROGRESS: at 83.01% examples, 675892 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:38,415 : INFO : EPOCH 1 - PROGRESS: at 88.18% examples, 675986 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:39,418 : INFO : EPOCH 1 - PROGRESS: at 93.33% examples, 675732 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:40,419 : INFO : EPOCH 1 - PROGRESS: at 98.48% examples, 675604 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:40,674 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 06:29:40,688 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 06:29:40,691 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 06:29:40,696 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 06:29:40,697 : INFO : EPOCH - 1 : training on 18042771 raw words (13094011 effective words) took 19.4s, 676242 effective words/s\n",
      "2019-01-06 06:29:41,706 : INFO : EPOCH 2 - PROGRESS: at 5.07% examples, 663288 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:42,718 : INFO : EPOCH 2 - PROGRESS: at 10.39% examples, 671604 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:43,719 : INFO : EPOCH 2 - PROGRESS: at 15.62% examples, 674622 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:44,721 : INFO : EPOCH 2 - PROGRESS: at 20.88% examples, 675900 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:45,722 : INFO : EPOCH 2 - PROGRESS: at 26.10% examples, 676832 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:46,728 : INFO : EPOCH 2 - PROGRESS: at 31.33% examples, 676812 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-06 06:29:47,732 : INFO : EPOCH 2 - PROGRESS: at 36.50% examples, 675836 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:48,737 : INFO : EPOCH 2 - PROGRESS: at 41.73% examples, 676923 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:49,748 : INFO : EPOCH 2 - PROGRESS: at 46.90% examples, 676591 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:50,760 : INFO : EPOCH 2 - PROGRESS: at 52.16% examples, 676909 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:51,762 : INFO : EPOCH 2 - PROGRESS: at 57.30% examples, 677202 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:52,764 : INFO : EPOCH 2 - PROGRESS: at 62.40% examples, 676768 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:53,771 : INFO : EPOCH 2 - PROGRESS: at 67.58% examples, 676713 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:54,779 : INFO : EPOCH 2 - PROGRESS: at 72.83% examples, 677194 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-06 06:29:55,788 : INFO : EPOCH 2 - PROGRESS: at 78.08% examples, 677509 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:56,789 : INFO : EPOCH 2 - PROGRESS: at 83.30% examples, 677683 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:57,800 : INFO : EPOCH 2 - PROGRESS: at 88.46% examples, 677427 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:58,807 : INFO : EPOCH 2 - PROGRESS: at 93.67% examples, 677362 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:29:59,812 : INFO : EPOCH 2 - PROGRESS: at 98.79% examples, 677040 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:00,019 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 06:30:00,029 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 06:30:00,035 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 06:30:00,036 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 06:30:00,038 : INFO : EPOCH - 2 : training on 18042771 raw words (13096221 effective words) took 19.3s, 677259 effective words/s\n",
      "2019-01-06 06:30:01,046 : INFO : EPOCH 3 - PROGRESS: at 5.02% examples, 655639 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:02,058 : INFO : EPOCH 3 - PROGRESS: at 10.23% examples, 661076 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:03,067 : INFO : EPOCH 3 - PROGRESS: at 15.40% examples, 663147 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:04,075 : INFO : EPOCH 3 - PROGRESS: at 20.67% examples, 666196 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:05,076 : INFO : EPOCH 3 - PROGRESS: at 25.83% examples, 667407 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:06,084 : INFO : EPOCH 3 - PROGRESS: at 30.99% examples, 667613 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:07,085 : INFO : EPOCH 3 - PROGRESS: at 36.17% examples, 668303 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:08,095 : INFO : EPOCH 3 - PROGRESS: at 41.30% examples, 668128 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:09,098 : INFO : EPOCH 3 - PROGRESS: at 46.36% examples, 667723 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:10,098 : INFO : EPOCH 3 - PROGRESS: at 51.43% examples, 667626 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:11,112 : INFO : EPOCH 3 - PROGRESS: at 56.60% examples, 667995 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:12,114 : INFO : EPOCH 3 - PROGRESS: at 61.67% examples, 668355 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 06:30:13,116 : INFO : EPOCH 3 - PROGRESS: at 66.82% examples, 668689 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:14,122 : INFO : EPOCH 3 - PROGRESS: at 71.99% examples, 669264 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:15,129 : INFO : EPOCH 3 - PROGRESS: at 77.20% examples, 669754 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:16,132 : INFO : EPOCH 3 - PROGRESS: at 82.35% examples, 669829 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:17,142 : INFO : EPOCH 3 - PROGRESS: at 87.54% examples, 670112 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:18,145 : INFO : EPOCH 3 - PROGRESS: at 92.74% examples, 670627 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:19,151 : INFO : EPOCH 3 - PROGRESS: at 97.87% examples, 670597 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:19,533 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 06:30:19,549 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 06:30:19,551 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 06:30:19,555 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 06:30:19,556 : INFO : EPOCH - 3 : training on 18042771 raw words (13095650 effective words) took 19.5s, 671041 effective words/s\n",
      "2019-01-06 06:30:20,563 : INFO : EPOCH 4 - PROGRESS: at 5.07% examples, 665946 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:21,567 : INFO : EPOCH 4 - PROGRESS: at 10.39% examples, 675184 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:22,575 : INFO : EPOCH 4 - PROGRESS: at 15.62% examples, 675271 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:23,591 : INFO : EPOCH 4 - PROGRESS: at 20.94% examples, 675711 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:24,597 : INFO : EPOCH 4 - PROGRESS: at 26.21% examples, 677318 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:25,605 : INFO : EPOCH 4 - PROGRESS: at 31.51% examples, 678194 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:26,612 : INFO : EPOCH 4 - PROGRESS: at 36.78% examples, 678782 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:27,623 : INFO : EPOCH 4 - PROGRESS: at 41.95% examples, 678122 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:28,624 : INFO : EPOCH 4 - PROGRESS: at 47.12% examples, 678290 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:29,625 : INFO : EPOCH 4 - PROGRESS: at 52.32% examples, 678404 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:30,626 : INFO : EPOCH 4 - PROGRESS: at 57.46% examples, 678599 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:31,627 : INFO : EPOCH 4 - PROGRESS: at 62.56% examples, 678149 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:32,631 : INFO : EPOCH 4 - PROGRESS: at 67.75% examples, 678202 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:33,631 : INFO : EPOCH 4 - PROGRESS: at 72.99% examples, 678908 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:34,638 : INFO : EPOCH 4 - PROGRESS: at 78.23% examples, 679200 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:35,643 : INFO : EPOCH 4 - PROGRESS: at 83.46% examples, 679134 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:36,648 : INFO : EPOCH 4 - PROGRESS: at 88.68% examples, 679437 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:37,649 : INFO : EPOCH 4 - PROGRESS: at 93.90% examples, 679498 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:38,654 : INFO : EPOCH 4 - PROGRESS: at 99.08% examples, 679433 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:38,804 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 06:30:38,814 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 06:30:38,823 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 06:30:38,825 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 06:30:38,826 : INFO : EPOCH - 4 : training on 18042771 raw words (13093754 effective words) took 19.3s, 679638 effective words/s\n",
      "2019-01-06 06:30:39,844 : INFO : EPOCH 5 - PROGRESS: at 5.18% examples, 676199 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:40,846 : INFO : EPOCH 5 - PROGRESS: at 10.44% examples, 677821 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:41,846 : INFO : EPOCH 5 - PROGRESS: at 15.68% examples, 678790 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:42,855 : INFO : EPOCH 5 - PROGRESS: at 20.99% examples, 679628 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:43,861 : INFO : EPOCH 5 - PROGRESS: at 26.26% examples, 680468 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:44,868 : INFO : EPOCH 5 - PROGRESS: at 31.51% examples, 679816 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:45,876 : INFO : EPOCH 5 - PROGRESS: at 36.78% examples, 680195 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:46,882 : INFO : EPOCH 5 - PROGRESS: at 41.95% examples, 679772 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:47,885 : INFO : EPOCH 5 - PROGRESS: at 47.06% examples, 678845 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:48,887 : INFO : EPOCH 5 - PROGRESS: at 52.27% examples, 678923 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:49,895 : INFO : EPOCH 5 - PROGRESS: at 57.46% examples, 679242 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:50,899 : INFO : EPOCH 5 - PROGRESS: at 62.56% examples, 678561 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:51,900 : INFO : EPOCH 5 - PROGRESS: at 67.75% examples, 678692 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:52,902 : INFO : EPOCH 5 - PROGRESS: at 72.94% examples, 678773 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:53,905 : INFO : EPOCH 5 - PROGRESS: at 78.13% examples, 678813 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:54,913 : INFO : EPOCH 5 - PROGRESS: at 83.40% examples, 679031 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:55,915 : INFO : EPOCH 5 - PROGRESS: at 88.58% examples, 679004 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:56,922 : INFO : EPOCH 5 - PROGRESS: at 93.84% examples, 679288 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:57,927 : INFO : EPOCH 5 - PROGRESS: at 99.02% examples, 679170 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:30:58,090 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 06:30:58,098 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 06:30:58,105 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 06:30:58,107 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 06:30:58,108 : INFO : EPOCH - 5 : training on 18042771 raw words (13094475 effective words) took 19.3s, 679449 effective words/s\n",
      "2019-01-06 06:30:58,110 : INFO : training on a 90213855 raw words (65474111 effective words) took 96.8s, 676540 effective words/s\n",
      "2019-01-06 06:30:58,118 : INFO : saving Word2Vec object under ../input/word2vec_model_300dim_40minwords_10context_stemmed, separately None\n",
      "2019-01-06 06:30:58,120 : INFO : not storing attribute vectors_norm\n",
      "2019-01-06 06:30:58,121 : INFO : not storing attribute cum_table\n",
      "2019-01-06 06:30:58,279 : INFO : saved ../input/word2vec_model_300dim_40minwords_10context_stemmed\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec\n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 300  # Word vector dimensionality\n",
    "MIN_WORD_COUNT = 40  # Minimum word count. Kaggle set to 40, to avoid attaching too much importance to individual movie titles.\n",
    "NUM_THREADS = 4  # Number of threads to run in parallel\n",
    "CONTEXT = 10  # Context window size\n",
    "DOWNSAMPLING = 1e-3  # Downsample setting for frequent words\n",
    "WORD2VEC_MODEL_FILE = BASE_DIR + \\\n",
    "    \"word2vec_model_\" + \\\n",
    "    str(EMBEDDING_DIM) + \"dim_\" + \\\n",
    "    str(MIN_WORD_COUNT) + \"minwords_\" + \\\n",
    "    str(CONTEXT) + \"context_\" +\\\n",
    "    \"stemmed\"\n",
    "\n",
    "print \"Training the Word2Vec model...\"\n",
    "model = Word2Vec(sentences, workers=NUM_THREADS, \\\n",
    "                 size=EMBEDDING_DIM, min_count=MIN_WORD_COUNT, \\\n",
    "                 window=CONTEXT, sample=DOWNSAMPLING, seed=1)\n",
    "model.save(WORD2VEC_MODEL_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
