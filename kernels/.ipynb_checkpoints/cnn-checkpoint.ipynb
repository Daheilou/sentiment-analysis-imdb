{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec_model_300dim_40minwords_10context', 'sampleSubmission.csv', 'labeledTrainData.tsv', 'test_submission.csv', 'testData.tsv', 'unlabeledTrainData.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeledTrainData.tsv'\n",
    "UNLABELED_TRAIN_DF = BASE_DIR + 'unlabeledTrainData.tsv'\n",
    "TEST_DF = BASE_DIR + 'testData.tsv'\n",
    "print(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 50000 unlabeled train reviews, and 25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv(UNLABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "test = pd.read_csv(TEST_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "print \"Read %d labeled train reviews, %d unlabeled train reviews, \" \\\n",
    "          \"and %d test reviews\" % (labeled_train[\"review\"].size, unlabeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage\n",
    "\n",
    "Check if test[\"sentiment\"] is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Kaggle tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_clean_review(review, remove_numbers=True):\n",
    "    # Function to convert a document to a clean document,\n",
    "    # optionally removing numbers.  Returns a string.\n",
    "    #\n",
    "    # 1. Remove HTML using lxml parser, ranked best by bs4\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    #\n",
    "    # TODO: Clean the text! stemming?\n",
    "    # https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings#L92\n",
    "    # https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    #  \n",
    "    # 2. Remove non-letters and non-numbers\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    #\n",
    "    # 3. Optionally remove numbers\n",
    "    if remove_numbers:\n",
    "        review_text = re.sub(\"[0-9]\", \" \", review_text)\n",
    "    else:\n",
    "        review_text = review_text.replace('0', ' zero ')\n",
    "        review_text = review_text.replace('1', ' one ')\n",
    "        review_text = review_text.replace('2', ' two ')\n",
    "        review_text = review_text.replace('3', ' three ')\n",
    "        review_text = review_text.replace('4', ' four ')\n",
    "        review_text = review_text.replace('5', ' five ')\n",
    "        review_text = review_text.replace('6', ' six ')\n",
    "        review_text = review_text.replace('7', ' seven ')\n",
    "        review_text = review_text.replace('8', ' eight ')\n",
    "        review_text = review_text.replace('9', ' nine ')\n",
    "    # 6. Return a cleaned string\n",
    "    return(review_text.lower())\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False, remove_numbers=True):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    # 1. Clean review, split it into words\n",
    "    words = review_to_clean_review(review).split()\n",
    "    #\n",
    "    # 2. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 6. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=True):\n",
    "    # Function to split a review into parsed sentences. Returns a\n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, \\\n",
    "                                                        remove_stopwords, remove_numbers))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the word2vec model vocabulary: 16490\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 300  # Word vector dimensionality\n",
    "MIN_WORD_COUNT = 40  # Minimum word count. Kaggle set to 40, to avoid attaching too much importance to individual movie titles.\n",
    "NUM_THREADS = 4  # Number of threads to run in parallel\n",
    "CONTEXT = 10  # Context window size\n",
    "DOWNSAMPLING = 1e-3  # Downsample setting for frequent words\n",
    "WORD2VEC_MODEL_FILE = BASE_DIR + \\\n",
    "    \"word2vec_model_\" + \\\n",
    "    str(EMBEDDING_DIM) + \"dim_\" + \\\n",
    "    str(MIN_WORD_COUNT) + \"minwords_\" + \\\n",
    "    str(CONTEXT) + \"context\"\n",
    "\n",
    "word2vec_model = Word2Vec.load(WORD2VEC_MODEL_FILE)\n",
    "\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "word2vec_model.init_sims(replace=True)\n",
    "\n",
    "print(\"Number of words in the word2vec model vocabulary: %d\" % len(word2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train review 0 of 25000\n",
      "Train review 5000 of 25000\n",
      "Train review 10000 of 25000\n",
      "Train review 15000 of 25000\n",
      "Train review 20000 of 25000\n",
      "Test review 0 of 25000\n",
      "Test review 5000 of 25000\n",
      "Test review 10000 of 25000\n",
      "Test review 15000 of 25000\n",
      "Test review 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "train_clean_reviews = []\n",
    "# Getting clean reviews from training set\n",
    "counter = 0.\n",
    "for review in labeled_train[\"review\"]:\n",
    "    clean_review = review_to_clean_review(review, remove_numbers=False)\n",
    "    train_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Train review %d of %d\" % (counter, len(labeled_train[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "# train_clean_reviews = map(\n",
    "#     lambda review: review_to_clean_review(review, remove_numbers=False),\n",
    "#     labeled_train[\"review\"])\n",
    "\n",
    "test_clean_reviews = []\n",
    "# Getting clean review from testing set\n",
    "counter = 0.\n",
    "for review in test[\"review\"]:\n",
    "    clean_review = review_to_clean_review(review, remove_numbers=False)\n",
    "    test_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Test review %d of %d\" % (counter, len(test[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "# test_clean_reviews = map(\n",
    "#     lambda review: review_to_clean_review(review, remove_numbers=False),\n",
    "#     test[\"review\"])\n",
    "\n",
    "all_clean_reviews = train_clean_reviews + test_clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_clean_reviews[0:2])\n",
    "# print(test_clean_reviews[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Tokenizer found 101376 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# We vectorize the text corpus by turning each text into a sequence of integers\n",
    "# Each integer is the index of a token in the dictionary\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NUM_WORDS_FOR_KERAS_TOKENIZER = 200000\n",
    "#\n",
    "# num_words: the maximum number of words to keep, based on frequency.\n",
    "keras_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS_FOR_KERAS_TOKENIZER)\n",
    "#\n",
    "# fit_on_texts accepts a list of strings, a generator of strings or \n",
    "# a list of list of strings. In the last case, it assumes each entry of the lists to be a token.\n",
    "# Here we provide a list of strings.\n",
    "keras_tokenizer.fit_on_texts(all_clean_reviews)\n",
    "word_index = keras_tokenizer.word_index\n",
    "print('Keras Tokenizer found %s unique tokens' % len(word_index))\n",
    "#\n",
    "# texts_to_sequences transforms each text in texts to a sequence of integers.\n",
    "train_sequences = keras_tokenizer.texts_to_sequences(train_clean_reviews)\n",
    "test_sequences = keras_tokenizer.texts_to_sequences(test_clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pad all text sequences to the same length.\n",
    "# By default zeros are padded at the front.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set max length for each review sequence.\n",
    "MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN = 500\n",
    "\n",
    "train_pad_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)\n",
    "test_pad_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 84920\n"
     ]
    }
   ],
   "source": [
    "# Prepare word embedding matrix\n",
    "\n",
    "# Choose the smaller number of the two as column length of the matrix\n",
    "num_words = min(MAX_NUM_WORDS_FOR_KERAS_TOKENIZER, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv.vocab:\n",
    "        embedding_matrix[i] = word2vec_model.wv.get_vector(word)\n",
    "# Null word embeddings are words that don't exist in the embedding matrix\n",
    "# and are therefore represented as zero vectors.\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly split 20000 pad sequences for training, 5000 for validation\n"
     ]
    }
   ],
   "source": [
    "# Split train_sequences into train and validation. Ratio: 80/20\n",
    "VALIDATION_SPLIT = 0.2\n",
    "np.random.seed(1234)\n",
    "\n",
    "# \n",
    "perm = np.random.permutation(len(train_sequences))\n",
    "index_train = perm[:int(len(train_sequences)*(1-VALIDATION_SPLIT))]\n",
    "index_val = perm[int(len(train_sequences)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "x_train = train_pad_sequences[index_train]\n",
    "x_val = train_pad_sequences[index_val]\n",
    "y_train = labeled_train[\"sentiment\"][index_train].tolist()\n",
    "y_val = labeled_train[\"sentiment\"][index_val].tolist()\n",
    "\n",
    "print('Randomly split %d pad sequences for training, %d for validation' % (len(x_train) ,len(x_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_pad_sequences\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Conv1D, MaxPooling1D ,GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer1_cnn(dropout=0.2, num_filters=64, kernel_size=2):\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer3_cnn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    NUM_FILTERS = 64\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_5 (Embedding)      (None, 500, 300)          30413100  \n",
      "_________________________________________________________________\n",
      "conv1d_7 (Conv1D)            (None, 499, 64)           38464     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,453,677\n",
      "Trainable params: 40,577\n",
      "Non-trainable params: 30,413,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer1_cnn_model = layer1_cnn()\n",
    "layer1_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 5s 231us/step - loss: 0.4500 - acc: 0.7872 - val_loss: 0.3195 - val_acc: 0.8624\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 4s 198us/step - loss: 0.2934 - acc: 0.8793 - val_loss: 0.2786 - val_acc: 0.8836\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 4s 199us/step - loss: 0.2526 - acc: 0.8974 - val_loss: 0.2656 - val_acc: 0.8850\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 4s 199us/step - loss: 0.2233 - acc: 0.9128 - val_loss: 0.2986 - val_acc: 0.8772\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 4s 199us/step - loss: 0.1954 - acc: 0.9255 - val_loss: 0.2626 - val_acc: 0.8920\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 4s 200us/step - loss: 0.1741 - acc: 0.9349 - val_loss: 0.2602 - val_acc: 0.8950\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 4s 198us/step - loss: 0.1498 - acc: 0.9474 - val_loss: 0.2871 - val_acc: 0.8872\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 4s 197us/step - loss: 0.1283 - acc: 0.9571 - val_loss: 0.2855 - val_acc: 0.8906\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 4s 194us/step - loss: 0.1089 - acc: 0.9641 - val_loss: 0.2850 - val_acc: 0.8954\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 4s 194us/step - loss: 0.0927 - acc: 0.9704 - val_loss: 0.2992 - val_acc: 0.8920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa10999fc90>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_cnn_model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 500, 300)          30413100  \n",
      "_________________________________________________________________\n",
      "conv1d_8 (Conv1D)            (None, 499, 64)           38464     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_9 (Conv1D)            (None, 248, 64)           8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 124, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_10 (Conv1D)           (None, 123, 64)           8256      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_6 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,470,189\n",
      "Trainable params: 57,089\n",
      "Non-trainable params: 30,413,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer3_cnn_model = layer3_cnn()\n",
    "layer3_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 6s 293us/step - loss: 0.4441 - acc: 0.7788 - val_loss: 0.3346 - val_acc: 0.8584\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 5s 249us/step - loss: 0.3161 - acc: 0.8704 - val_loss: 0.3216 - val_acc: 0.8624\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 5s 250us/step - loss: 0.2775 - acc: 0.8898 - val_loss: 0.2875 - val_acc: 0.8812\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 5s 250us/step - loss: 0.2385 - acc: 0.9085 - val_loss: 0.3132 - val_acc: 0.8710\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 5s 248us/step - loss: 0.2161 - acc: 0.9204 - val_loss: 0.2944 - val_acc: 0.8764\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 5s 250us/step - loss: 0.1966 - acc: 0.9267 - val_loss: 0.3181 - val_acc: 0.8798\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 5s 249us/step - loss: 0.1733 - acc: 0.9365 - val_loss: 0.3043 - val_acc: 0.8820\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 5s 250us/step - loss: 0.1445 - acc: 0.9506 - val_loss: 0.3186 - val_acc: 0.8802\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 5s 251us/step - loss: 0.1229 - acc: 0.9591 - val_loss: 0.3769 - val_acc: 0.8664\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 5s 255us/step - loss: 0.1103 - acc: 0.9636 - val_loss: 0.3848 - val_acc: 0.8778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa10a48e690>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer3_cnn_model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 5s 227us/step - loss: 0.4863 - acc: 0.7656 - val_loss: 0.3407 - val_acc: 0.8570\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.3326 - acc: 0.8642 - val_loss: 0.2911 - val_acc: 0.8784\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 4s 193us/step - loss: 0.2863 - acc: 0.8873 - val_loss: 0.2802 - val_acc: 0.8830\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.2599 - acc: 0.9016 - val_loss: 0.2691 - val_acc: 0.8874\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2308 - acc: 0.9134 - val_loss: 0.2573 - val_acc: 0.8942\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2117 - acc: 0.9262 - val_loss: 0.2952 - val_acc: 0.8800\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 4s 193us/step - loss: 0.1903 - acc: 0.9327 - val_loss: 0.2544 - val_acc: 0.8970\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.1726 - acc: 0.9399 - val_loss: 0.2736 - val_acc: 0.8958\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 4s 193us/step - loss: 0.1511 - acc: 0.9496 - val_loss: 0.3158 - val_acc: 0.8800\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.1376 - acc: 0.9546 - val_loss: 0.2993 - val_acc: 0.8932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa1099b3290>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_cnn_model_dropout5 = layer1_cnn(dropout=0.5)\n",
    "layer1_cnn_model_dropout5.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_layer1_cnn = layer1_cnn_model.predict(x_test)\n",
    "y_test_pred_layer1_cnn_dropout5 = layer1_cnn_model_dropout5.predict(x_test)\n",
    "y_test_pred_layer3_cnn = layer3_cnn_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(predicts):\n",
    "    return map(lambda predict: 1 if predict > 0.5 else 0, predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_layer1_cnn = to_binary(y_test_pred_layer1_cnn)\n",
    "y_test_pred_layer1_cnn_dropout5 = to_binary(y_test_pred_layer1_cnn_dropout5)\n",
    "y_test_pred_layer3_cnn = to_binary(y_test_pred_layer3_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_auc_score(model_name, y_test_pred):\n",
    "    print(\"The AUC score for %s is : %.4f.\" % (model_name, roc_auc_score(y_test, y_test_pred)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for layer1 cnn is : 0.8894.\n",
      "The AUC score for layer1 cnn dropout 0.5 is : 0.8886.\n",
      "The AUC score for layer3 cnn is : 0.8846.\n"
     ]
    }
   ],
   "source": [
    "print_auc_score(\"layer1 cnn\", y_test_pred_layer1_cnn)\n",
    "print_auc_score(\"layer1 cnn dropout 0.5\", y_test_pred_layer1_cnn_dropout5)\n",
    "print_auc_score(\"layer3 cnn\", y_test_pred_layer3_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 6s 304us/step - loss: 0.4914 - acc: 0.7683 - val_loss: 0.3345 - val_acc: 0.8604\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 5s 250us/step - loss: 0.3266 - acc: 0.8683 - val_loss: 0.2860 - val_acc: 0.8764\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 5s 250us/step - loss: 0.2808 - acc: 0.8931 - val_loss: 0.2720 - val_acc: 0.8872\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 5s 254us/step - loss: 0.2426 - acc: 0.9090 - val_loss: 0.2555 - val_acc: 0.8948\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 5s 255us/step - loss: 0.2142 - acc: 0.9229 - val_loss: 0.2702 - val_acc: 0.8878\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 5s 255us/step - loss: 0.1874 - acc: 0.9355 - val_loss: 0.2639 - val_acc: 0.8960\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 5s 254us/step - loss: 0.1664 - acc: 0.9438 - val_loss: 0.2633 - val_acc: 0.8968\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 5s 254us/step - loss: 0.1504 - acc: 0.9512 - val_loss: 0.2827 - val_acc: 0.8968\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 5s 254us/step - loss: 0.1179 - acc: 0.9651 - val_loss: 0.2850 - val_acc: 0.9022\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 5s 255us/step - loss: 0.1018 - acc: 0.9704 - val_loss: 0.3365 - val_acc: 0.8894\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0fbe7c3d0>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_cnn_32f_5d = layer1_cnn(dropout=0.5, num_filters=128)\n",
    "layer1_cnn_32f_5d.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer1_cnn_d(dropout=0.2, num_filters=64, kernel_size=2):\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "#     model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 5s 275us/step - loss: 0.4981 - acc: 0.7510 - val_loss: 0.3383 - val_acc: 0.8588\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 4s 197us/step - loss: 0.3588 - acc: 0.8442 - val_loss: 0.3022 - val_acc: 0.8722\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 4s 197us/step - loss: 0.3208 - acc: 0.8652 - val_loss: 0.2831 - val_acc: 0.8820\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 4s 197us/step - loss: 0.2920 - acc: 0.8804 - val_loss: 0.2886 - val_acc: 0.8758\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 4s 197us/step - loss: 0.2657 - acc: 0.8931 - val_loss: 0.2661 - val_acc: 0.8924\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 4s 196us/step - loss: 0.2470 - acc: 0.9011 - val_loss: 0.2576 - val_acc: 0.8922\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 4s 196us/step - loss: 0.2254 - acc: 0.9121 - val_loss: 0.2534 - val_acc: 0.8964\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 4s 196us/step - loss: 0.2050 - acc: 0.9203 - val_loss: 0.2555 - val_acc: 0.8956\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 4s 197us/step - loss: 0.1896 - acc: 0.9278 - val_loss: 0.2577 - val_acc: 0.8946\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 4s 201us/step - loss: 0.1774 - acc: 0.9320 - val_loss: 0.2549 - val_acc: 0.8962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0fa0ba110>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_cnn_d_0 = layer1_cnn_d()\n",
    "layer1_cnn_d_0.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for layer1 cnn_d is : 0.8940.\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_layer1_cnn_d_0 = layer1_cnn_d_0.predict(x_test)\n",
    "y_test_pred_layer1_cnn_d_0 = to_binary(y_test_pred_layer1_cnn_d_0)\n",
    "print_auc_score(\"layer1 cnn_d\", y_test_pred_layer1_cnn_d_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 6s 292us/step - loss: 0.6255 - acc: 0.6316 - val_loss: 0.4245 - val_acc: 0.8384\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 4s 203us/step - loss: 0.4595 - acc: 0.7944 - val_loss: 0.3529 - val_acc: 0.8516\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 4s 202us/step - loss: 0.4015 - acc: 0.8280 - val_loss: 0.3238 - val_acc: 0.8664\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 4s 201us/step - loss: 0.3607 - acc: 0.8513 - val_loss: 0.3026 - val_acc: 0.8720\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 4s 198us/step - loss: 0.3411 - acc: 0.8593 - val_loss: 0.2909 - val_acc: 0.8772\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 4s 202us/step - loss: 0.3191 - acc: 0.8722 - val_loss: 0.2860 - val_acc: 0.8786\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 4s 204us/step - loss: 0.3105 - acc: 0.8752 - val_loss: 0.2756 - val_acc: 0.8862\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 4s 203us/step - loss: 0.2970 - acc: 0.8843 - val_loss: 0.2843 - val_acc: 0.8792\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 4s 201us/step - loss: 0.2869 - acc: 0.8869 - val_loss: 0.2725 - val_acc: 0.8846\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 4s 197us/step - loss: 0.2779 - acc: 0.8889 - val_loss: 0.2692 - val_acc: 0.8904\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 4s 198us/step - loss: 0.2691 - acc: 0.8940 - val_loss: 0.2656 - val_acc: 0.8888\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 4s 199us/step - loss: 0.2604 - acc: 0.8964 - val_loss: 0.2618 - val_acc: 0.8904\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 4s 203us/step - loss: 0.2499 - acc: 0.9009 - val_loss: 0.2581 - val_acc: 0.8906\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 4s 203us/step - loss: 0.2453 - acc: 0.9043 - val_loss: 0.2642 - val_acc: 0.8934\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 4s 203us/step - loss: 0.2387 - acc: 0.9068 - val_loss: 0.2625 - val_acc: 0.8908\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 4s 203us/step - loss: 0.2348 - acc: 0.9083 - val_loss: 0.2595 - val_acc: 0.8940\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 4s 203us/step - loss: 0.2305 - acc: 0.9109 - val_loss: 0.2598 - val_acc: 0.8936\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 4s 204us/step - loss: 0.2203 - acc: 0.9133 - val_loss: 0.2664 - val_acc: 0.8890\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 4s 204us/step - loss: 0.2248 - acc: 0.9115 - val_loss: 0.2631 - val_acc: 0.8886\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 4s 203us/step - loss: 0.2189 - acc: 0.9149 - val_loss: 0.2599 - val_acc: 0.8930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa0f9714910>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_cnn_d_5d = layer1_cnn_d(dropout=0.5)\n",
    "layer1_cnn_d_5d.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for layer1 cnn_d is : 0.8932.\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_layer1_cnn_d_5d = layer1_cnn_d_5d.predict(x_test)\n",
    "y_test_pred_layer1_cnn_d_5d = to_binary(y_test_pred_layer1_cnn_d_5d)\n",
    "print_auc_score(\"layer1 cnn_d\", y_test_pred_layer1_cnn_d_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer3_cnn_1p(dropout=0.2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    NUM_FILTERS = 64\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 8s 424us/step - loss: 0.5101 - acc: 0.7393 - val_loss: 0.3379 - val_acc: 0.8498\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 6s 314us/step - loss: 0.3480 - acc: 0.8579 - val_loss: 0.3009 - val_acc: 0.8748\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 6s 317us/step - loss: 0.2939 - acc: 0.8848 - val_loss: 0.2843 - val_acc: 0.8838\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 6s 312us/step - loss: 0.2659 - acc: 0.8964 - val_loss: 0.2789 - val_acc: 0.8824\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 6s 309us/step - loss: 0.2405 - acc: 0.9105 - val_loss: 0.2822 - val_acc: 0.8846\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 6s 316us/step - loss: 0.2179 - acc: 0.9195 - val_loss: 0.2814 - val_acc: 0.8880\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 6s 316us/step - loss: 0.2013 - acc: 0.9263 - val_loss: 0.2721 - val_acc: 0.8866\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 6s 317us/step - loss: 0.1754 - acc: 0.9367 - val_loss: 0.2828 - val_acc: 0.8874\n",
      "Epoch 9/20\n",
      "19648/20000 [============================>.] - ETA: 0s - loss: 0.1567 - acc: 0.9450"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-a7c41413851b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlayer3_cnn_1p_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer3_cnn_1p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlayer3_cnn_1p_0\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/engine/training_arrays.pyc\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2713\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2714\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2715\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2716\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2717\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2674\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2675\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2676\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1439\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1440\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layer3_cnn_1p_0 = layer3_cnn_1p(dropout=0.5)\n",
    "layer3_cnn_1p_0.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
