{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec_model_300dim_40minwords_10context', 'sampleSubmission.csv', 'word2vec_model_300dim_40minwords_10context_stemmed', 'labeledTrainData.tsv', 'test_submission.csv', 'testData.tsv', 'unlabeledTrainData.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeledTrainData.tsv'\n",
    "UNLABELED_TRAIN_DF = BASE_DIR + 'unlabeledTrainData.tsv'\n",
    "TEST_DF = BASE_DIR + 'testData.tsv'\n",
    "print(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 50000 unlabeled train reviews, and 25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv(UNLABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "test = pd.read_csv(TEST_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "print \"Read %d labeled train reviews, %d unlabeled train reviews, \" \\\n",
    "          \"and %d test reviews\" % (labeled_train[\"review\"].size, unlabeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage\n",
    "\n",
    "Check if test[\"sentiment\"] is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Kaggle tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def review_to_clean_review(review, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    \n",
    "    if remove_numbers:\n",
    "        review_text = re.sub(\"[0-9]\", \" \", review_text)\n",
    "    else:\n",
    "        review_text = review_text.replace('0', ' zero ')\n",
    "        review_text = review_text.replace('1', ' one ')\n",
    "        review_text = review_text.replace('2', ' two ')\n",
    "        review_text = review_text.replace('3', ' three ')\n",
    "        review_text = review_text.replace('4', ' four ')\n",
    "        review_text = review_text.replace('5', ' five ')\n",
    "        review_text = review_text.replace('6', ' six ')\n",
    "        review_text = review_text.replace('7', ' seven ')\n",
    "        review_text = review_text.replace('8', ' eight ')\n",
    "        review_text = review_text.replace('9', ' nine ')\n",
    "    \n",
    "    review_text = review_text.lower()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        words = review_text.split()\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        review_text = \" \".join(words)\n",
    "    \n",
    "    if stem_words:\n",
    "        words = review_text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        review_text = \" \".join(words)\n",
    "    \n",
    "    return(review_text.strip())\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "\n",
    "    words = review_to_clean_review(review, remove_stopwords, remove_numbers, stem_words).split()\n",
    "    return words\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords, remove_numbers, stem_words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'this', u'is', u'an', u'exampl', u'of', u'a', u'movi', u'review'], [u'it', u'has', u'two', u'sentenc']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "example_review = \"This is an example of a movie review. It has two sentences.\"\n",
    "print(review_to_sentences(example_review, tokenizer, remove_numbers=False, stem_words=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labled train review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:272: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labled train review 5000 of 25000\n",
      "Labled train review 10000 of 25000\n",
      "Labled train review 15000 of 25000\n",
      "Labled train review 20000 of 25000\n",
      "Unlabled train review 0 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 5000 of 50000\n",
      "Unlabled train review 10000 of 50000\n",
      "Unlabled train review 15000 of 50000\n",
      "Unlabled train review 20000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 25000 of 50000\n",
      "Unlabled train review 30000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:272: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 35000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 40000 of 50000\n",
      "Unlabled train review 45000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# Initialize an empty list of sentences\n",
    "stemmed_sentences = []\n",
    "sentences = []\n",
    "# Parsing sentences from training set\n",
    "counter = 0.\n",
    "for review in labeled_train[\"review\"]:\n",
    "#     stemmed_sentences += review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=False, stem_words=True)\n",
    "    sentences += review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=False, stem_words=False)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Labled train review %d of %d\" % (counter, len(labeled_train[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "counter = 0.\n",
    "# Parsing sentences from unlabeled set\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "#     stemmed_sentences += review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=False, stem_words=True)\n",
    "    sentences += review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=False, stem_words=False)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Unlabled train review %d of %d\" % (counter, len(unlabeled_train[\"review\"]))\n",
    "    counter = counter + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([u'mr', u'natural', u'don', u't', u'mean', u'sheeit', u'one', u'day', u'while', u'searching', u'through', u'my', u'father', u's', u'closet', u'about', u'one', u'two', u'years', u'ago', u'i', u'happened', u'to', u'come', u'across', u'a', u'couple', u'of', u'comic', u'books'])\n",
      " list([u'the', u'future', u'is', u'a', u'bleak', u'dying', u'place'])\n",
      " list([u'also', u'it', u'is', u'loaded', u'with', u'scientific', u'detail', u'that', u'many', u'will', u'find', u'hard', u'to', u'understand', u'and', u'may', u'get', u'bored', u'with'])\n",
      " list([u'there', u'then', u'follows', u'interminable', u'amounts', u'of', u'wandering', u'about', u'in', u'the', u'desert', u'by', u'cabot', u'and', u'his', u'midget', u'henchman', u'i', u'kid', u'you', u'not'])\n",
      " list([u'walter', u'mattheau', u'and', u'joan', u'plowright', u'are', u'ideal', u'as', u'the', u'neighborly', u'object', u'of', u'dennis', u'affection', u'he', u'the', u'loveable', u'old', u'curmudgeon', u'and', u'she', u'the', u'unfulfilled', u'neighborhood', u'grandmother'])\n",
      " list([u'that', u's', u'the', u'beauty', u'and', u'power', u'of', u'film', u'it', u'can', u'express', u'so', u'much', u'whole', u'lives', u'in', u'a', u'matter', u'of', u'secondsthe', u'shot', u'with', u'the', u'toddler', u'stepping', u'in', u'the', u'puddle', u'of', u'puke', u'could', u'have', u'been', u'improved', u'on'])\n",
      " list([u'the', u'fact', u'that', u'they', u'got', u'an', u'actor', u'from', u'star', u'trek', u'only', u'goes', u'to', u'show', u'how', u'busy', u'he', u'wasn', u't'])\n",
      " list([u'i', u'was', u'hoping', u'for', u'something', u'new', u'after', u'the', u'first', u'home', u'alone', u'which', u'all', u'my', u'friends', u'begged', u'me', u'to', u'go', u'see', u'and', u'my', u'reaction', u'to', u'it', u'was', u'uhhh', u'the', u'three', u'stooges', u'were', u'funnier', u'and', u'i', u'can', u'watch', u'them', u'for', u'free', u'on', u'tv', u'i', u'thought', u'it', u'would', u'have', u'been', u'more', u'realistic', u'and', u'still', u'funny', u'if', u'the', u'bandits', u'were', u'actually', u'menacing'])\n",
      " list([u'the', u'whole', u'movie', u'was', u'the', u'result', u'of', u'catching', u'lightning', u'in', u'a', u'jar'])\n",
      " list([u'there', u'was', u'a', u'plot'])]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(sentences, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_sentences = [\" \".join(sentence).strip() for sentence in sentences]\n",
    "output_sentences = pd.DataFrame(data={\"sentences\": joined_sentences})\n",
    "output_sentences.to_csv(os.path.join('../', 'input', \"sentences_for_word2vec.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train review 0 of 25000\n",
      "Train review 5000 of 25000\n",
      "Train review 10000 of 25000\n",
      "Train review 15000 of 25000\n",
      "Train review 20000 of 25000\n",
      "Test review 0 of 25000\n",
      "Test review 5000 of 25000\n",
      "Test review 10000 of 25000\n",
      "Test review 15000 of 25000\n",
      "Test review 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "labeled_train_clean_reviews = []\n",
    "counter = 0.\n",
    "for review in labeled_train[\"review\"]:\n",
    "#     clean_review_stemmed = review_to_clean_review(review, remove_stopwords=False, remove_numbers=False, stem_words=True)\n",
    "    clean_review = review_to_clean_review(review, remove_stopwords=False, remove_numbers=False, stem_words=False)\n",
    "    labeled_train_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Train review %d of %d\" % (counter, len(labeled_train[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "test_clean_reviews = []\n",
    "counter = 0.\n",
    "for review in test[\"review\"]:\n",
    "#     clean_review_stemmed = review_to_clean_review(review, remove_stopwords=False, remove_numbers=False, stem_words=True)\n",
    "    clean_review = review_to_clean_review(review, remove_stopwords=False, remove_numbers=False, stem_words=False)\n",
    "    test_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Test review %d of %d\" % (counter, len(test[\"review\"]))\n",
    "    counter = counter + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'though i saw this movie years ago  its impact has never left me  stephen rea s depiction of an invetigator is deep and moving  his anguish at not being able to stop the deaths is palpable  everyone in the cast is amazing from sutherland who tries to accommodate him and provide ways for the police to coordinate their efforts  to the troubled citizen x  each day when we are bombarded with stories of mass murderers  i think of this film and the exhausting work the people do who try to find the killers'\n",
      " u'a not bad but also not so great heist film  kirk douglas is a recently released from prison safe cracker who  after turning down an offer from the mob  decides to pull the job himself  he recruits circus gymnast giuliano gemma  mayhem ensues  douglas and gemma soon find themselves pursued by mafia goon romano puppo as well as entangled in a really goofy love triangle with douglas s infinitely patient girlfriend  florinda bolkan   director michele lupo keeps the pace moving quickly and there s at least one excellent and creative car chase sequence involving puppo   gemma  though an italian production  most of the filming appears to have been done in germany  douglas is fine  not just slumming it in an giallo quickie  the striking bolkan gives a terrific performance  the music is by ennio morricone and the cinematography is by the great tonino delli colli  who managed to work with everyone in italy  from wertmuller and fellini to pasolini and leone'\n",
      " u'i have to finish watching a movie once i start  regardless of how bad it is  this movie was agonizing to sit through  the   sparkling   bullets  the reporter with   ninja   like moves  the way the bad guys shoot hundreds and hundreds of bullets and only seem to hit innocent bystanders  the predictable outcome and all the bad acting was just horrible  like the girl who finds the reporter in her friends apartment and goes from   what the heck are you doing in here  holding a bat    to   hey  you re cute  wanna            in like  one   two  seconds     just bad     save yourself an hour and forty minutes and go play with your kids  or dog'\n",
      " u'whoever gave this movie rave reviews needs to see more movies a loser takes his camera and photographs his mental family  the movie is filled with idiots and includes live   teabagging    that should sum it all up for you do not waste your time  you may want to watch the entire movie in the hopes that it gets better as it goes on   it doesn t'\n",
      " u'five  minutes into this movie i was hyperventilating  shaking  and writhing in pain  and not in the good way  the story is about a troupe of idiotic children making prank phone calls to a psycho which is always a good idea  turns out psychos don t like prank phone calls because in  two  minutes time he s at their door killing poor williams mom and dad  well skip ahead  one  five  years and guess what  still prank phone calling people  yep you would of thought that a horrible murder would of deterred them from doing that ever again but no  so after about two hours later and way too many scream ripoffs i realized that this movie gave me nothing but a terrible taste in my mouth and a severe urge to take my own life  this piece of crap isn t even worth laughing at the shoddy production  the   acting    or rutger haurs dwindling career  i love crappy horror movies but this is the most unsatisfying piece i ve ever seen  just don t'\n",
      " u'wow  this film had a huge impact on me  it moved me   it is an amazing story about a girl in cambodia who is sold into the sex trade  i can not stop thinking about the fate of the little girl named holly  the setting of the film is realistic  the film was an eye opener  i can not imagine anyone walking away from it with out wanting to help make a change with this horrifying problem that exsists the content of the film was very very moving  it was one of the best films that i have seen this year  thegirl who plays holly does a fantastic job with her character  ron livinston gives a fantastic performance  the film moved me to tears  it tells an important message that needs to be heard worldwide  everyone should go see this film  i think this film will make a difference  i loved it'\n",
      " u'i watched this movie when i was almost quite a kid  and  naturally  was moved to tears by this story of a fox family  the fantastic scenery at hokkaid    the excellent storytelling and last not least the wonderful soundtrack provide a rare intimacy with the protagonists  i am still searching for some copy of the gorgeous soundtrack  to german viewers it might be useful to know that the defa dubbing is the only one worth listening to  i taped both  defa and br  but i keep viewing the first one only'\n",
      " u'when i first heard about   down to earth    i was pretty excited  i m a pretty big fan a chris rock  he was especially hilarious in dogma  but this film proved to be a disappointment  chris rock s performance was not nearly as good as his past performances  and the movie was just very badly directed as a whole first off  chris rock  he plays the entire movie as a standup comedy routine  obviously  this works fine in the scenes where he s supposed to be doing standup  but in the rest of the movie  it doesn t  even when he s talking to one other character  he seems to think he s trying to make a roomful of people laugh  he has a few funny moments  but they mostly come during the standup scenes  no wonder they decided to make his character a comedian   as for the rest of the cast  they re pretty much there just to give rock someone to talk to  none of them stand out also  the movie was poorly directed  the movie has basically a one joke plot  old white guy acting like a young black comedian  while i prefer movies with more than one joke  this still could have worked  and been quite funny  the problem was  we saw too much of rock and not enough of the old white guy  it s supposed to be funny because it s this white guy telling chris rock s jokes  but for most of the time  we just see chris rock  so it s not nearly as funny  the few scenes where they decide to show us the white guy talking like rock are  in fact  hilarious  if he had been shown more  the movie would have been a lot funnier overall  a few moments of laughter can t make up for the fact that   down to earth   is poorly acted  and poorly directed  this one was a pretty big disappointment rating   four   one  zero'\n",
      " u'apart from the usual stereotypes of the thirties  eugene pallette as the gruff police detective  jack la rue as the   swarthy   italian and of course  james lee as   the chinese cook    this film is the great mystery of a murder in a locked room  for an early  one  nine  three  zero  s film  this step by step   peeling of the veneer of the mystery   is similar to the columbo series  except in this film  you don t have the advantage of knowing who the killer was in advance'\n",
      " u'this great film is composed mostly of documentary footage is currently contained on a dvd along with prelude to war  the great american filmmaker and story teller frank capra made these films which simply and clearly call attention to the main points that caused world war ii and hitler s rise every school child  nay  every american should watch these films today because they are so apropos  history has been repeating itself over and over again  the lord chamberlains are still alive and kicking  the tactics used by the nazis of infiltrating countries through sympathizers and then the communists and now by muslim terrorist groups  are still working to these evil group s advantage by sitting back and letting hitler as early as  one  nine  three  five  be aggressive   france  america and england caused over  five  zero  million people s deaths  americans  french and british today would happily let hitler do exactly the same thing despite the fact that we should have learned from history what happens when you let dictators break treaties these great films may be too simplistic for world war ii history buffs  they don t tell the horrors that the soviet union caused simply because at the time america was teamed up with them  fighting hitler  this film does tell the plain facts and motives that led to the terrible war']\n",
      "[u'wow  there s a lot of venom directed at this movie  and a lot of it is deserved  but it s not the worst movie of all time   that s probably   zoolander    anyway  if you re high on something  really drunk  or just in the mood for a   b minus  minus   movie that you can make fun of  this may be your cup of tea yes  as others said  the first part of the movie makes you think you re watching an updating of the bram stoker novel  two of the main characters are named van helsing and mina  the ship is the demeter  and they re in the carpathian galaxy  you later find that van helsing is a descendant of the original  and he just happens to be on a ship in the  three  zero th century with dracula  suuuuuuuuuuure oh yeah  and to add to the originality  this   spooky ghost ship   movie has another character named ash  sound familiar there s paper thin character development and anachronisms like the aforementioned manual wheelchair  and coolio and tom lister talking  two  zero th century black slang  but what really makes the movie ridiculous is the ending  if that s what happens to the characters  then the previous two hours have been a waste like i said  it you want a dumb movie to play   mystery science theater   with  and your mind is in approximately the same mode required for viewing the   great vegetable rebellion   episode of lost in space  then you may find this diverting  otherwise  put a stake in it  it s done'\n",
      " u'a great movie  the movie was even better then the commercials put on  and believe it or not it was very very inspirational  i really think anyone who walks out of the movie at the end will be inspired one way or another it was kinda corny at the very beginning  but quickly picks up  i laughed  i laughed very hard on some parts  the acting is basically above average  nothing special  but better then average  i can safely say it was the second funniest movie to come out this summer   one st funniest being clerks ii   so after all of that i give it a  seven   one  zero   a high seven  but not quite an eight'\n",
      " u'terrific acting by the  five  stars makes this one a must see based on a stage play  the silver cord is about   mother love   at its worst  the film was very controversial for its implied homosexuality of the younger son and the mother s unnatural   romantic   feelings for both sons irene dunne stars as the new bride  and biologist  who travels with husband  joel mccrea  to visit the family before heading off to new york city for their new jobs  but something seems wrong the mother  laura hope crews  seems rude to the younger son s  eric linden  fianc  e  frances dee   but dunne puts such thoughts aside and ignores a few of the strange things the mother says  then she finds out   her   room is down the hall from mccrea s  and his room adjoins mother s room later she walks in as   mummy   is tucking in mccrea and kissing him  on the lips  good night  mummy has also been working on linden and getting him to doubt his feelings for dee  everything blows up and with dee running into the snowy night toward the frozen pond  as the boys run after her  mummy shouts from the window for boys to come back and get their coats  dee falls through the ice and is rescued as the girls leave the house the following morning  dunne lets mummy know what she thinks of her and her attachment to the boys  but mummy has a tight hold  faking illness and forever boasting of her sacrifices  the girls leave but the boys stay behind       crews is magnificent as the voracious mother  repeating her stage role   it s a part few actresses would dare play  the sexual overtones are incredible for a  one  nine  three  three  film and crews take advantage of her best film role  dunne is also excellent as she tries to maneuver the course without losing mccrea  dee has some excellent scenes after she gets dumped by bewildered linden  all  five  stars are terrific in this drama that is bizarrely underrated and unknown a neglected gem for anyone who likes great acting'\n",
      " u'can t believe that bostock s cup isn t available on a proper video or dvd yet  i ve only seen it once  on a dodgy copy taken off the tv and despite not being a footy fan  at all   thought it was one of the funniest things i ve ever seen the famous sloping pitch of wherever it was  the clueless coach driver    ponty this  ponty that      i ll take the next exit     the pointless plot  it all added up to aching sides being stuck in the us i m desperate for some good british humour but not quite enough to spend the amount that the production company are asking for  c mon  get it out on kosher dvd pronto'\n",
      " u'last evening i had the remarkable pleasure of seeing the movie hitch  i was stunned and amazed  will smith did a phenomenal job in a role that he isn t always associated with  he proved once and for all that he does not need to be holding a gun to be a great actor in a movie  kevin james was also very impressive  i admit that i ve always like him on   the king of queens   but this role really brought him into the light more  and i hope to see him on the big screen a lot more in the near future  the movie was funny and adorable and i recommend it to any guy as the best date movie you could ever imagine  the mix of comedy  romance  and drama left me feeling complete  it s nothing like most   chick flicks   in the sense of tears and sadness  but holds its own and i think can be equally enjoyed by males as well as females  i encourage all couples especially to see this movie but even if you aren t involved with someone you may pick up some great ideas from hitch  two way big thumbs up'\n",
      " u'while it has been many decades since i last read mr  wells   war of the worlds    or   the time machine    or any other of his works  i believe that they were all set in london  or at least  in england  this grade   b      c     movie is set in the eastern part of the united states as was orson welles excellent radio adaptation  six  seven  years ago  however  this film exhibits none of the quality of the narrative style of the mercury playhouse program  thomas howell s emoting would not be acceptable in most high school drama clubs  i was actually embarrassed for him  his rolling around in the grass on the hill crying   my family  my family   was almost laughable  as was his reaction to the death of his brother  of the three film versions of the story that i have seen  this was by far the worst  with gene barry s  one  nine  five  zero  s version the best  additionaally  this was the first time i ever saw a   machine monster   dripping sticky saliva such as did the creature in the   alien'\n",
      " u'perhaps the best movie ever made by director kevin tenney  well  his witchboard is not on the top of my all time horror list   this one is a strange  fascinating mixture between pin and child s play  both better than this one  but not so better  sure  the plot is contrived and perhaps too predictable  but the actors are good  rosalind allen is very pleasant to the eye  and so is candance mckenzie   god bless her for the shower scene    the child actress is very good in interpreting the disturbed daughter and the pinocchio puppet is scary enough to give you a few thrills down the spine  for a b movie not bad at all'\n",
      " u'would be promising if it were a student film  never seems to know where it s going next  as if they decided on each next scene as they were making the film  as a result  much repetition and a deadening pace  there s an absolutely confusing procedure by which the ghosts affect people in this world  it involves the internet  phone calls  a computer science experiment  a suspicious grad student  withdrawn behavior  red rooms  dark stains  and  then  suicides  good luck figuring out the connections  or when a character is a ghost yet or just acting strangely  characters are barely sketched  which could be ok  but they re completely uninteresting  it s far too long and ends just when it reaches a promising scenario  although the conclusion is nonsensical  weakest moment  when   the boss   sits down to give some incoherent advice'\n",
      " u'but of course i was wrong now  i never expected to like the first movie  i m not sure what s up with disney s marketing group  but it seems that every trailer they make for an animated film ends up turning me off as too childish  or silly  or stupid  and yet the movies themselves are usually anything but  and no movie looked worse to me in the trailers than the emperor s new groove  which is why i was quite surprised to actually find myself quite enjoying that film when i finally broke down and saw it  i entered with zero expectations and came out pleasantly entertained despite disney s track record with direct to video sequels  i had nonetheless hoped for a better experience with kronk here    but in the end i was nothing but disappointed  and unfortunately not exactly surprised that i felt that way   there s almost no humor targeted towards adults  the original songs are uninspired and sickly cute  the animation  while not bad  still doesn t come close to emperor  which was no lion king to start with  the main plot  as such  is astoundingly   minor   and is comprised mainly of a sequence of mini plot flashbacks   in fact the while thing felt more like a sequence of pilot episodes for a saturday morning cartoon series than a well conceived single entity david spade gets about four lines throughout the entire movie and there isn t exactly a lot of john goodman either  so overall we re just left with far too much of patrick warburton s kronk   who was entertaining as a secondary character in the first movie but is completely inappropriate as the main lead here although kids might find it somewhat fun  the only thing kronk s new groove managed to do for me is make me want to go back and watch the far superior original'\n",
      " u'yeah  i know my title sucks  i couldn t think of any other title  x  ice is a brilliant first season episode  very interesting idea and good acting as well  the whole worm looking thing was really creepy in my opinion  i ve never been a fan of insects  so all the insect episodes are creepy to me  x  anyway  lets go on to the good and bad things about this episode the good  the parasite thing  awesome scully finally trusting mulder  awww      three  three it was a good idea to put another parasite in the ear  though if someone told me they had to put that thing in my ear    i think the whole cabin would be dead the bad  how did the dog stay alive for so long how didn t huffman get those black spots  or maybe she did  but no one saw it   conclusion  very good episode  especially for season  one    eight   one  zero']\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(labeled_train_clean_reviews, 10))\n",
    "print(np.random.choice(test_clean_reviews, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labeled_train_clean_reviews = \\\n",
    "    pd.DataFrame(data={\"id\": labeled_train[\"id\"], \"sentiment\": labeled_train[\"sentiment\"], \"review\": labeled_train_clean_reviews})\n",
    "output_test_clean_reviews = \\\n",
    "    pd.DataFrame(data={\"id\": test[\"id\"], \"review\": test_clean_reviews})\n",
    "# output_labeled_train_clean_reviews.to_csv(os.path.join('../', 'input', \"labeled_train_clean_reviews_stemmed.csv\"), index=False, quoting=3)\n",
    "# output_test_clean_reviews.to_csv(os.path.join('../', 'input', \"test_clean_reviews_stemmed.csv\"), index=False, quoting=3)\n",
    "\n",
    "output_labeled_train_clean_reviews.to_csv(os.path.join('../', 'input', \"labeled_train_clean_reviews.csv\"), index=False, quoting=3)\n",
    "output_test_clean_reviews.to_csv(os.path.join('../', 'input', \"test_clean_reviews.csv\"), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 08:35:12,161 : INFO : 'pattern' package found; tag filters are available for English\n",
      "2019-01-06 08:35:12,167 : INFO : collecting all words and their counts\n",
      "2019-01-06 08:35:12,168 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-06 08:35:12,233 : INFO : PROGRESS: at sentence #10000, processed 229166 words, keeping 17776 word types\n",
      "2019-01-06 08:35:12,300 : INFO : PROGRESS: at sentence #20000, processed 458297 words, keeping 24948 word types\n",
      "2019-01-06 08:35:12,365 : INFO : PROGRESS: at sentence #30000, processed 680652 words, keeping 30034 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Word2Vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 08:35:12,432 : INFO : PROGRESS: at sentence #40000, processed 910089 words, keeping 34348 word types\n",
      "2019-01-06 08:35:12,496 : INFO : PROGRESS: at sentence #50000, processed 1132423 words, keeping 37761 word types\n",
      "2019-01-06 08:35:12,561 : INFO : PROGRESS: at sentence #60000, processed 1356968 words, keeping 40723 word types\n",
      "2019-01-06 08:35:12,628 : INFO : PROGRESS: at sentence #70000, processed 1583238 words, keeping 43333 word types\n",
      "2019-01-06 08:35:12,691 : INFO : PROGRESS: at sentence #80000, processed 1805500 words, keeping 45714 word types\n",
      "2019-01-06 08:35:12,758 : INFO : PROGRESS: at sentence #90000, processed 2032837 words, keeping 48135 word types\n",
      "2019-01-06 08:35:12,826 : INFO : PROGRESS: at sentence #100000, processed 2257707 words, keeping 50207 word types\n",
      "2019-01-06 08:35:12,896 : INFO : PROGRESS: at sentence #110000, processed 2480677 words, keeping 52081 word types\n",
      "2019-01-06 08:35:12,965 : INFO : PROGRESS: at sentence #120000, processed 2705814 words, keeping 54119 word types\n",
      "2019-01-06 08:35:13,036 : INFO : PROGRESS: at sentence #130000, processed 2934773 words, keeping 55847 word types\n",
      "2019-01-06 08:35:13,102 : INFO : PROGRESS: at sentence #140000, processed 3150424 words, keeping 57346 word types\n",
      "2019-01-06 08:35:13,172 : INFO : PROGRESS: at sentence #150000, processed 3379002 words, keeping 59055 word types\n",
      "2019-01-06 08:35:13,241 : INFO : PROGRESS: at sentence #160000, processed 3604642 words, keeping 60617 word types\n",
      "2019-01-06 08:35:13,310 : INFO : PROGRESS: at sentence #170000, processed 3831024 words, keeping 62077 word types\n",
      "2019-01-06 08:35:13,377 : INFO : PROGRESS: at sentence #180000, processed 4054617 words, keeping 63496 word types\n",
      "2019-01-06 08:35:13,448 : INFO : PROGRESS: at sentence #190000, processed 4282897 words, keeping 64794 word types\n",
      "2019-01-06 08:35:13,517 : INFO : PROGRESS: at sentence #200000, processed 4509999 words, keeping 66087 word types\n",
      "2019-01-06 08:35:13,586 : INFO : PROGRESS: at sentence #210000, processed 4734668 words, keeping 67390 word types\n",
      "2019-01-06 08:35:13,656 : INFO : PROGRESS: at sentence #220000, processed 4962787 words, keeping 68697 word types\n",
      "2019-01-06 08:35:13,724 : INFO : PROGRESS: at sentence #230000, processed 5188480 words, keeping 69958 word types\n",
      "2019-01-06 08:35:13,792 : INFO : PROGRESS: at sentence #240000, processed 5419180 words, keeping 71167 word types\n",
      "2019-01-06 08:35:13,865 : INFO : PROGRESS: at sentence #250000, processed 5636219 words, keeping 72351 word types\n",
      "2019-01-06 08:35:13,934 : INFO : PROGRESS: at sentence #260000, processed 5859234 words, keeping 73478 word types\n",
      "2019-01-06 08:35:14,007 : INFO : PROGRESS: at sentence #270000, processed 6083817 words, keeping 74767 word types\n",
      "2019-01-06 08:35:14,076 : INFO : PROGRESS: at sentence #280000, processed 6312682 words, keeping 76369 word types\n",
      "2019-01-06 08:35:14,145 : INFO : PROGRESS: at sentence #290000, processed 6538799 words, keeping 77839 word types\n",
      "2019-01-06 08:35:14,216 : INFO : PROGRESS: at sentence #300000, processed 6766532 words, keeping 79171 word types\n",
      "2019-01-06 08:35:14,295 : INFO : PROGRESS: at sentence #310000, processed 6994910 words, keeping 80480 word types\n",
      "2019-01-06 08:35:14,369 : INFO : PROGRESS: at sentence #320000, processed 7222831 words, keeping 81808 word types\n",
      "2019-01-06 08:35:14,444 : INFO : PROGRESS: at sentence #330000, processed 7447731 words, keeping 83030 word types\n",
      "2019-01-06 08:35:14,521 : INFO : PROGRESS: at sentence #340000, processed 7680440 words, keeping 84280 word types\n",
      "2019-01-06 08:35:14,592 : INFO : PROGRESS: at sentence #350000, processed 7906671 words, keeping 85425 word types\n",
      "2019-01-06 08:35:14,662 : INFO : PROGRESS: at sentence #360000, processed 8130069 words, keeping 86596 word types\n",
      "2019-01-06 08:35:14,741 : INFO : PROGRESS: at sentence #370000, processed 8360199 words, keeping 87708 word types\n",
      "2019-01-06 08:35:14,824 : INFO : PROGRESS: at sentence #380000, processed 8588323 words, keeping 88878 word types\n",
      "2019-01-06 08:35:14,902 : INFO : PROGRESS: at sentence #390000, processed 8821215 words, keeping 89907 word types\n",
      "2019-01-06 08:35:14,977 : INFO : PROGRESS: at sentence #400000, processed 9047140 words, keeping 90916 word types\n",
      "2019-01-06 08:35:15,054 : INFO : PROGRESS: at sentence #410000, processed 9271446 words, keeping 91880 word types\n",
      "2019-01-06 08:35:15,130 : INFO : PROGRESS: at sentence #420000, processed 9495586 words, keeping 92912 word types\n",
      "2019-01-06 08:35:15,207 : INFO : PROGRESS: at sentence #430000, processed 9726512 words, keeping 93932 word types\n",
      "2019-01-06 08:35:15,281 : INFO : PROGRESS: at sentence #440000, processed 9956570 words, keeping 94906 word types\n",
      "2019-01-06 08:35:15,358 : INFO : PROGRESS: at sentence #450000, processed 10183262 words, keeping 96036 word types\n",
      "2019-01-06 08:35:15,436 : INFO : PROGRESS: at sentence #460000, processed 10419261 words, keeping 97088 word types\n",
      "2019-01-06 08:35:15,511 : INFO : PROGRESS: at sentence #470000, processed 10650184 words, keeping 97933 word types\n",
      "2019-01-06 08:35:15,583 : INFO : PROGRESS: at sentence #480000, processed 10873555 words, keeping 98862 word types\n",
      "2019-01-06 08:35:15,658 : INFO : PROGRESS: at sentence #490000, processed 11103736 words, keeping 99871 word types\n",
      "2019-01-06 08:35:15,729 : INFO : PROGRESS: at sentence #500000, processed 11328543 words, keeping 100765 word types\n",
      "2019-01-06 08:35:15,810 : INFO : PROGRESS: at sentence #510000, processed 11556927 words, keeping 101699 word types\n",
      "2019-01-06 08:35:15,885 : INFO : PROGRESS: at sentence #520000, processed 11783512 words, keeping 102598 word types\n",
      "2019-01-06 08:35:15,971 : INFO : PROGRESS: at sentence #530000, processed 12011039 words, keeping 103400 word types\n",
      "2019-01-06 08:35:16,045 : INFO : PROGRESS: at sentence #540000, processed 12238852 words, keeping 104265 word types\n",
      "2019-01-06 08:35:16,119 : INFO : PROGRESS: at sentence #550000, processed 12467490 words, keeping 105133 word types\n",
      "2019-01-06 08:35:16,192 : INFO : PROGRESS: at sentence #560000, processed 12691615 words, keeping 105997 word types\n",
      "2019-01-06 08:35:16,264 : INFO : PROGRESS: at sentence #570000, processed 12923711 words, keeping 106787 word types\n",
      "2019-01-06 08:35:16,340 : INFO : PROGRESS: at sentence #580000, processed 13148083 words, keeping 107665 word types\n",
      "2019-01-06 08:35:16,415 : INFO : PROGRESS: at sentence #590000, processed 13376619 words, keeping 108501 word types\n",
      "2019-01-06 08:35:16,487 : INFO : PROGRESS: at sentence #600000, processed 13601946 words, keeping 109218 word types\n",
      "2019-01-06 08:35:16,560 : INFO : PROGRESS: at sentence #610000, processed 13825965 words, keeping 110092 word types\n",
      "2019-01-06 08:35:16,633 : INFO : PROGRESS: at sentence #620000, processed 14055595 words, keeping 110837 word types\n",
      "2019-01-06 08:35:16,705 : INFO : PROGRESS: at sentence #630000, processed 14283003 words, keeping 111610 word types\n",
      "2019-01-06 08:35:16,775 : INFO : PROGRESS: at sentence #640000, processed 14506788 words, keeping 112416 word types\n",
      "2019-01-06 08:35:16,850 : INFO : PROGRESS: at sentence #650000, processed 14735829 words, keeping 113196 word types\n",
      "2019-01-06 08:35:16,928 : INFO : PROGRESS: at sentence #660000, processed 14961517 words, keeping 113945 word types\n",
      "2019-01-06 08:35:17,005 : INFO : PROGRESS: at sentence #670000, processed 15187871 words, keeping 114643 word types\n",
      "2019-01-06 08:35:17,079 : INFO : PROGRESS: at sentence #680000, processed 15415849 words, keeping 115354 word types\n",
      "2019-01-06 08:35:17,154 : INFO : PROGRESS: at sentence #690000, processed 15640987 words, keeping 116131 word types\n",
      "2019-01-06 08:35:17,233 : INFO : PROGRESS: at sentence #700000, processed 15872499 words, keeping 116943 word types\n",
      "2019-01-06 08:35:17,318 : INFO : PROGRESS: at sentence #710000, processed 16098551 words, keeping 117596 word types\n",
      "2019-01-06 08:35:17,394 : INFO : PROGRESS: at sentence #720000, processed 16326874 words, keeping 118221 word types\n",
      "2019-01-06 08:35:17,472 : INFO : PROGRESS: at sentence #730000, processed 16556101 words, keeping 118954 word types\n",
      "2019-01-06 08:35:17,546 : INFO : PROGRESS: at sentence #740000, processed 16780279 words, keeping 119668 word types\n",
      "2019-01-06 08:35:17,618 : INFO : PROGRESS: at sentence #750000, processed 17001852 words, keeping 120295 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 08:35:17,689 : INFO : PROGRESS: at sentence #760000, processed 17224202 words, keeping 120930 word types\n",
      "2019-01-06 08:35:17,764 : INFO : PROGRESS: at sentence #770000, processed 17454566 words, keeping 121703 word types\n",
      "2019-01-06 08:35:17,839 : INFO : PROGRESS: at sentence #780000, processed 17687962 words, keeping 122402 word types\n",
      "2019-01-06 08:35:17,915 : INFO : PROGRESS: at sentence #790000, processed 17918070 words, keeping 123066 word types\n",
      "2019-01-06 08:35:17,956 : INFO : collected 123504 word types from a corpus of 18042771 raw words and 795538 sentences\n",
      "2019-01-06 08:35:17,958 : INFO : Loading a fresh vocabulary\n",
      "2019-01-06 08:35:18,056 : INFO : effective_min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2019-01-06 08:35:18,058 : INFO : effective_min_count=40 leaves 17483629 word corpus (96% of original 18042771, drops 559142)\n",
      "2019-01-06 08:35:18,104 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2019-01-06 08:35:18,111 : INFO : sample=0.001 downsamples 49 most-common words\n",
      "2019-01-06 08:35:18,112 : INFO : downsampling leaves estimated 12971161 word corpus (74.2% of prior 17483629)\n",
      "2019-01-06 08:35:18,160 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2019-01-06 08:35:18,161 : INFO : resetting layer weights\n",
      "2019-01-06 08:35:18,463 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-06 08:35:19,482 : INFO : EPOCH 1 - PROGRESS: at 5.07% examples, 651174 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:20,489 : INFO : EPOCH 1 - PROGRESS: at 10.39% examples, 664057 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:21,489 : INFO : EPOCH 1 - PROGRESS: at 15.18% examples, 648249 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:22,493 : INFO : EPOCH 1 - PROGRESS: at 20.10% examples, 643449 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-06 08:35:23,507 : INFO : EPOCH 1 - PROGRESS: at 24.98% examples, 640171 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:24,512 : INFO : EPOCH 1 - PROGRESS: at 29.95% examples, 639747 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:25,519 : INFO : EPOCH 1 - PROGRESS: at 35.01% examples, 639800 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:26,522 : INFO : EPOCH 1 - PROGRESS: at 39.96% examples, 640305 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:27,526 : INFO : EPOCH 1 - PROGRESS: at 44.94% examples, 640687 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:28,532 : INFO : EPOCH 1 - PROGRESS: at 49.91% examples, 641464 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:29,540 : INFO : EPOCH 1 - PROGRESS: at 54.95% examples, 642049 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:30,547 : INFO : EPOCH 1 - PROGRESS: at 59.84% examples, 642000 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:31,549 : INFO : EPOCH 1 - PROGRESS: at 64.77% examples, 641553 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:32,564 : INFO : EPOCH 1 - PROGRESS: at 69.79% examples, 641699 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:33,565 : INFO : EPOCH 1 - PROGRESS: at 74.82% examples, 642422 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:34,569 : INFO : EPOCH 1 - PROGRESS: at 79.86% examples, 642962 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-06 08:35:35,573 : INFO : EPOCH 1 - PROGRESS: at 84.82% examples, 642977 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:36,578 : INFO : EPOCH 1 - PROGRESS: at 89.83% examples, 643392 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:37,579 : INFO : EPOCH 1 - PROGRESS: at 94.91% examples, 643865 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:38,598 : INFO : EPOCH 1 - PROGRESS: at 99.79% examples, 642977 words/s, in_qsize 4, out_qsize 0\n",
      "2019-01-06 08:35:38,604 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 08:35:38,612 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 08:35:38,626 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 08:35:38,628 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 08:35:38,629 : INFO : EPOCH - 1 : training on 18042771 raw words (12970406 effective words) took 20.2s, 643324 effective words/s\n",
      "2019-01-06 08:35:39,644 : INFO : EPOCH 2 - PROGRESS: at 4.80% examples, 617709 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:40,655 : INFO : EPOCH 2 - PROGRESS: at 9.96% examples, 634969 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:41,660 : INFO : EPOCH 2 - PROGRESS: at 14.96% examples, 637425 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:42,665 : INFO : EPOCH 2 - PROGRESS: at 19.99% examples, 638578 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:43,671 : INFO : EPOCH 2 - PROGRESS: at 25.03% examples, 640779 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:44,672 : INFO : EPOCH 2 - PROGRESS: at 30.01% examples, 641464 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:45,682 : INFO : EPOCH 2 - PROGRESS: at 35.11% examples, 642026 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:46,687 : INFO : EPOCH 2 - PROGRESS: at 40.12% examples, 642948 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:47,696 : INFO : EPOCH 2 - PROGRESS: at 45.05% examples, 641835 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:48,705 : INFO : EPOCH 2 - PROGRESS: at 50.02% examples, 642338 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:49,705 : INFO : EPOCH 2 - PROGRESS: at 55.06% examples, 643220 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:50,713 : INFO : EPOCH 2 - PROGRESS: at 59.96% examples, 643035 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:51,725 : INFO : EPOCH 2 - PROGRESS: at 64.83% examples, 641571 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:52,730 : INFO : EPOCH 2 - PROGRESS: at 69.68% examples, 640647 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:53,742 : INFO : EPOCH 2 - PROGRESS: at 74.54% examples, 639583 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:54,756 : INFO : EPOCH 2 - PROGRESS: at 79.41% examples, 638533 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:55,762 : INFO : EPOCH 2 - PROGRESS: at 84.28% examples, 637879 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:56,763 : INFO : EPOCH 2 - PROGRESS: at 89.14% examples, 637490 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:57,771 : INFO : EPOCH 2 - PROGRESS: at 94.01% examples, 636949 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:58,775 : INFO : EPOCH 2 - PROGRESS: at 98.86% examples, 636556 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:35:58,979 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 08:35:58,993 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 08:35:58,999 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 08:35:59,000 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 08:35:59,002 : INFO : EPOCH - 2 : training on 18042771 raw words (12969917 effective words) took 20.4s, 636764 effective words/s\n",
      "2019-01-06 08:36:00,009 : INFO : EPOCH 3 - PROGRESS: at 4.75% examples, 615262 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:01,016 : INFO : EPOCH 3 - PROGRESS: at 9.73% examples, 625117 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:02,016 : INFO : EPOCH 3 - PROGRESS: at 14.79% examples, 633867 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:03,018 : INFO : EPOCH 3 - PROGRESS: at 19.94% examples, 640090 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:04,023 : INFO : EPOCH 3 - PROGRESS: at 25.03% examples, 643421 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:05,029 : INFO : EPOCH 3 - PROGRESS: at 30.11% examples, 645588 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:06,031 : INFO : EPOCH 3 - PROGRESS: at 35.22% examples, 646387 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:07,037 : INFO : EPOCH 3 - PROGRESS: at 40.30% examples, 647668 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:08,042 : INFO : EPOCH 3 - PROGRESS: at 45.39% examples, 648691 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:09,050 : INFO : EPOCH 3 - PROGRESS: at 50.43% examples, 649343 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:10,063 : INFO : EPOCH 3 - PROGRESS: at 55.50% examples, 649475 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 08:36:11,066 : INFO : EPOCH 3 - PROGRESS: at 60.52% examples, 650155 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:12,069 : INFO : EPOCH 3 - PROGRESS: at 65.60% examples, 650794 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:13,069 : INFO : EPOCH 3 - PROGRESS: at 70.73% examples, 651979 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:14,078 : INFO : EPOCH 3 - PROGRESS: at 75.83% examples, 652150 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:15,084 : INFO : EPOCH 3 - PROGRESS: at 80.86% examples, 651926 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:16,098 : INFO : EPOCH 3 - PROGRESS: at 85.77% examples, 650654 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:17,111 : INFO : EPOCH 3 - PROGRESS: at 91.00% examples, 651897 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:18,124 : INFO : EPOCH 3 - PROGRESS: at 96.20% examples, 652253 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:18,847 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 08:36:18,862 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 08:36:18,871 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 08:36:18,873 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 08:36:18,875 : INFO : EPOCH - 3 : training on 18042771 raw words (12970658 effective words) took 19.9s, 652797 effective words/s\n",
      "2019-01-06 08:36:19,895 : INFO : EPOCH 4 - PROGRESS: at 5.02% examples, 646478 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:20,895 : INFO : EPOCH 4 - PROGRESS: at 10.17% examples, 653110 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:21,901 : INFO : EPOCH 4 - PROGRESS: at 15.24% examples, 651646 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-06 08:36:22,908 : INFO : EPOCH 4 - PROGRESS: at 20.44% examples, 654295 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:23,912 : INFO : EPOCH 4 - PROGRESS: at 25.54% examples, 654853 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:24,914 : INFO : EPOCH 4 - PROGRESS: at 30.63% examples, 655433 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:25,915 : INFO : EPOCH 4 - PROGRESS: at 35.84% examples, 657009 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:26,921 : INFO : EPOCH 4 - PROGRESS: at 41.02% examples, 658727 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:27,935 : INFO : EPOCH 4 - PROGRESS: at 46.13% examples, 658694 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:28,936 : INFO : EPOCH 4 - PROGRESS: at 51.26% examples, 659525 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:29,943 : INFO : EPOCH 4 - PROGRESS: at 56.38% examples, 659730 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:30,946 : INFO : EPOCH 4 - PROGRESS: at 61.40% examples, 659543 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:31,947 : INFO : EPOCH 4 - PROGRESS: at 66.41% examples, 658993 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:32,947 : INFO : EPOCH 4 - PROGRESS: at 71.50% examples, 659066 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:33,952 : INFO : EPOCH 4 - PROGRESS: at 76.60% examples, 658973 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:34,965 : INFO : EPOCH 4 - PROGRESS: at 81.67% examples, 658474 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:35,970 : INFO : EPOCH 4 - PROGRESS: at 86.83% examples, 658838 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:36,973 : INFO : EPOCH 4 - PROGRESS: at 91.88% examples, 658805 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:37,977 : INFO : EPOCH 4 - PROGRESS: at 97.02% examples, 658796 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:38,533 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 08:36:38,543 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 08:36:38,557 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 08:36:38,559 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 08:36:38,561 : INFO : EPOCH - 4 : training on 18042771 raw words (12971127 effective words) took 19.7s, 659218 effective words/s\n",
      "2019-01-06 08:36:39,568 : INFO : EPOCH 5 - PROGRESS: at 4.96% examples, 643065 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:40,579 : INFO : EPOCH 5 - PROGRESS: at 10.17% examples, 651751 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:41,601 : INFO : EPOCH 5 - PROGRESS: at 15.40% examples, 654450 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:42,601 : INFO : EPOCH 5 - PROGRESS: at 20.61% examples, 657490 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:43,609 : INFO : EPOCH 5 - PROGRESS: at 25.83% examples, 659831 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:44,610 : INFO : EPOCH 5 - PROGRESS: at 30.94% examples, 659664 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:45,622 : INFO : EPOCH 5 - PROGRESS: at 36.11% examples, 659600 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:46,627 : INFO : EPOCH 5 - PROGRESS: at 41.25% examples, 660295 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:47,632 : INFO : EPOCH 5 - PROGRESS: at 46.19% examples, 658273 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:48,634 : INFO : EPOCH 5 - PROGRESS: at 51.26% examples, 658381 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:49,643 : INFO : EPOCH 5 - PROGRESS: at 56.38% examples, 658582 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:50,660 : INFO : EPOCH 5 - PROGRESS: at 61.51% examples, 658958 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:51,668 : INFO : EPOCH 5 - PROGRESS: at 66.65% examples, 659196 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:52,676 : INFO : EPOCH 5 - PROGRESS: at 71.71% examples, 658887 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:53,682 : INFO : EPOCH 5 - PROGRESS: at 76.87% examples, 659226 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:54,684 : INFO : EPOCH 5 - PROGRESS: at 82.01% examples, 659590 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:55,691 : INFO : EPOCH 5 - PROGRESS: at 87.16% examples, 659830 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:56,712 : INFO : EPOCH 5 - PROGRESS: at 92.28% examples, 659500 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:57,727 : INFO : EPOCH 5 - PROGRESS: at 97.55% examples, 660119 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:36:58,181 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 08:36:58,190 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 08:36:58,197 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 08:36:58,199 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 08:36:58,201 : INFO : EPOCH - 5 : training on 18042771 raw words (12970922 effective words) took 19.6s, 660525 effective words/s\n",
      "2019-01-06 08:36:58,203 : INFO : training on a 90213855 raw words (64853030 effective words) took 99.7s, 650231 effective words/s\n",
      "2019-01-06 08:36:58,205 : INFO : saving Word2Vec object under ../input/word2vec_model_300dim_40minwords_10context_new, separately None\n",
      "2019-01-06 08:36:58,206 : INFO : not storing attribute vectors_norm\n",
      "2019-01-06 08:36:58,209 : INFO : not storing attribute cum_table\n",
      "2019-01-06 08:36:58,376 : INFO : saved ../input/word2vec_model_300dim_40minwords_10context_new\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec\n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 300  # Word vector dimensionality\n",
    "MIN_WORD_COUNT = 40  # Minimum word count. Kaggle set to 40, to avoid attaching too much importance to individual movie titles.\n",
    "NUM_THREADS = 4  # Number of threads to run in parallel\n",
    "CONTEXT = 10  # Context window size\n",
    "DOWNSAMPLING = 1e-3  # Downsample setting for frequent words\n",
    "WORD2VEC_MODEL_FILE = BASE_DIR + \\\n",
    "    \"word2vec_model_\" + \\\n",
    "    str(EMBEDDING_DIM) + \"dim_\" + \\\n",
    "    str(MIN_WORD_COUNT) + \"minwords_\" + \\\n",
    "    str(CONTEXT) + \"context_\" +\\\n",
    "    \"new\"\n",
    "\n",
    "print \"Training the Word2Vec model...\"\n",
    "model = Word2Vec(sentences, workers=NUM_THREADS, \\\n",
    "                 size=EMBEDDING_DIM, min_count=MIN_WORD_COUNT, \\\n",
    "                 window=CONTEXT, sample=DOWNSAMPLING, seed=1)\n",
    "model.save(WORD2VEC_MODEL_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Generate the txt file for "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
