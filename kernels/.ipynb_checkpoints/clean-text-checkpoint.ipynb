{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec_model_300dim_40minwords_10context', 'sampleSubmission.csv', 'word2vec_model_300dim_40minwords_10context_stemmed', 'labeledTrainData.tsv', 'test_submission.csv', 'testData.tsv', 'unlabeledTrainData.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeledTrainData.tsv'\n",
    "UNLABELED_TRAIN_DF = BASE_DIR + 'unlabeledTrainData.tsv'\n",
    "TEST_DF = BASE_DIR + 'testData.tsv'\n",
    "print(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 50000 unlabeled train reviews, and 25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv(UNLABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "test = pd.read_csv(TEST_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "print \"Read %d labeled train reviews, %d unlabeled train reviews, \" \\\n",
    "          \"and %d test reviews\" % (labeled_train[\"review\"].size, unlabeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage\n",
    "\n",
    "Check if test[\"sentiment\"] is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Kaggle tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def review_to_clean_review(review, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    \n",
    "    if remove_numbers:\n",
    "        review_text = re.sub(\"[0-9]\", \" \", review_text)\n",
    "    else:\n",
    "        review_text = review_text.replace('0', ' zero ')\n",
    "        review_text = review_text.replace('1', ' one ')\n",
    "        review_text = review_text.replace('2', ' two ')\n",
    "        review_text = review_text.replace('3', ' three ')\n",
    "        review_text = review_text.replace('4', ' four ')\n",
    "        review_text = review_text.replace('5', ' five ')\n",
    "        review_text = review_text.replace('6', ' six ')\n",
    "        review_text = review_text.replace('7', ' seven ')\n",
    "        review_text = review_text.replace('8', ' eight ')\n",
    "        review_text = review_text.replace('9', ' nine ')\n",
    "    \n",
    "    review_text = review_text.lower()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        words = review_text.split()\n",
    "        stops = set(stopwords.words('english'))\n",
    "        words = [w for w in words if not w in stops]\n",
    "        review_text = \" \".join(words)\n",
    "    \n",
    "    if stem_words:\n",
    "        words = review_text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        review_text = \" \".join(words)\n",
    "    \n",
    "    return(review_text.strip())\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "\n",
    "    words = review_to_clean_review(review, remove_stopwords, remove_numbers, stem_words).split()\n",
    "    return words\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords, remove_numbers, stem_words))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[u'this', u'is', u'an', u'exampl', u'of', u'a', u'movi', u'review'], [u'it', u'has', u'two', u'sentenc']]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "example_review = \"This is an example of a movie review. It has two sentences.\"\n",
    "print(review_to_sentences(example_review, tokenizer, remove_numbers=False, stem_words=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labled train review 0 of 25000\n",
      "Labled train review 5000 of 25000\n",
      "Labled train review 10000 of 25000\n",
      "Labled train review 15000 of 25000\n",
      "Labled train review 20000 of 25000\n",
      "Unlabled train review 0 of 50000\n",
      "Unlabled train review 5000 of 50000\n",
      "Unlabled train review 10000 of 50000\n",
      "Unlabled train review 15000 of 50000\n",
      "Unlabled train review 20000 of 50000\n",
      "Unlabled train review 25000 of 50000\n",
      "Unlabled train review 30000 of 50000\n",
      "Unlabled train review 35000 of 50000\n",
      "Unlabled train review 40000 of 50000\n",
      "Unlabled train review 45000 of 50000\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# Initialize an empty list of sentences\n",
    "stemmed_sentences = []\n",
    "sentences = []\n",
    "# Parsing sentences from training set\n",
    "counter = 0.\n",
    "for review in labeled_train[\"review\"]:\n",
    "#     stemmed_sentences += review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=False, stem_words=True)\n",
    "    sentences += review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=False, stem_words=False)\n",
    "    if counter % 5000. == 0.:\n",
    "#         print \"Labled train review %d of %d\" % (counter, len(labeled_train[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "counter = 0.\n",
    "# Parsing sentences from unlabeled set\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "#     stemmed_sentences += review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=False, stem_words=True)\n",
    "    sentences += review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=False, stem_words=False)\n",
    "    if counter % 5000. == 0.:\n",
    "#         print \"Unlabled train review %d of %d\" % (counter, len(unlabeled_train[\"review\"]))\n",
    "    counter = counter + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([u'the', u'novel', u'which', u'earn', u'ms', u'buck', u'the', u'nobel', u'prize', u'for', u'literatur', u'come', u'aliv', u'under', u'the', u'baton', u'of', u'sydney', u'franklin', u'which', u'along', u'with', u'an', u'excel', u'script', u'recount', u'the', u'stori', u'of', u'peasant', u'farmer', u'wang', u'lung', u'whose', u'father', u'obtain', u'a', u'bride', u'for', u'him', u'a', u'slave', u'girl', u'from', u'the', u'kitchen', u'of', u'a', u'local', u'landlord'])\n",
      " list([u'this', u'box', u'pictur', u'deal', u'with', u'the', u'seedier', u'side', u'of', u'the', u'busi', u'is', u'there', u'ani', u'other'])\n",
      " list([u'his', u'perform', u'bring', u'to', u'mind', u'mifun', u'toshiro', u's', u'kikuchiyo', u'in', u'seven', u'samurai', u'manic', u'bestial', u'and', u'cocksur', u'of', u'himself'])\n",
      " list([u'base', u'on', u'the', u'idea', u'from', u'gackt', u'moon', u'child', u'took', u'place', u'in', u'a', u'poverti', u'stricken', u'countri', u'call', u'mallepa'])\n",
      " list([u'the', u'bate', u'role', u'is', u'absurd', u'clees', u's', u'role', u'has', u'virtual', u'noth', u'to', u'say', u'this', u'amaz', u'encount'])\n",
      " list([u'the', u'final', u'confront', u'has', u'nice', u'idea', u'but', u'again', u'fall', u'flat', u'at', u'it', u'heart', u'i', u'think', u'ultim', u'warrior', u'want', u'to', u'highlight', u'the', u'differ', u'and', u'more', u'import', u'the', u'similar', u'between', u'peac', u'hippi', u'and', u'harden', u'men', u'such', u'as', u'carson', u'and', u'how', u'the', u'two', u'react', u'when', u'face', u'obstacl', u'in', u'a', u'lawless', u'environ'])\n",
      " list([u'she', u'must', u'get', u'hold', u'of', u'a', u'substanti', u'amount', u'of', u'money'])\n",
      " list([u'and', u'what', u'about', u'the', u'ridicul', u'plot', u'to', u'steal', u'five', u'zero', u'grand'])\n",
      " list([u'charl', u'durn', u'jame', u'caan', u'dick', u'van', u'dyke', u'estell', u'parson', u'catherin', u'o', u'hara', u'seymour', u'cassel', u'paul', u'sorvino', u'and', u'kathi', u'bate', u'also', u'star', u'oh', u'wait', u'a', u'minut'])\n",
      " list([u'i', u'like', u'some', u'of', u'the', u'charact', u'name', u'thunderbolt', u'and', u'patch', u'but', u'the', u'other', u'charact', u'like', u'cruella', u'were', u'mediocr'])]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(sentences, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_sentences = [\" \".join(sentence).strip() for sentence in sentences]\n",
    "output_sentences = pd.DataFrame(data={\"sentences\": joined_sentences})\n",
    "output_sentences.to_csv(os.path.join('../', 'output', \"sentences_for_word2vec_stemmed.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train review 0 of 25000\n",
      "Train review 5000 of 25000\n",
      "Train review 10000 of 25000\n",
      "Train review 15000 of 25000\n",
      "Train review 20000 of 25000\n",
      "Test review 0 of 25000\n",
      "Test review 5000 of 25000\n",
      "Test review 10000 of 25000\n",
      "Test review 15000 of 25000\n",
      "Test review 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "labeled_train_clean_reviews = []\n",
    "counter = 0.\n",
    "for review in labeled_train[\"review\"]:\n",
    "    clean_review = review_to_clean_review(review, remove_stopwords=False, remove_numbers=False, stem_words=True)\n",
    "    labeled_train_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Train review %d of %d\" % (counter, len(labeled_train[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "test_clean_reviews = []\n",
    "counter = 0.\n",
    "for review in test[\"review\"]:\n",
    "    clean_review = review_to_clean_review(review, remove_stopwords=False, remove_numbers=False, stem_words=True)\n",
    "    test_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Test review %d of %d\" % (counter, len(test[\"review\"]))\n",
    "    counter = counter + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'i love sabrina it one of my fave show my favourit episod are the one where she turn libbi into a geek the first episod the true love episod and most of the rest from the first seri i do think the colleg episod were not as good as the high school one but they were better than the last seri which was aw valeri was a good charact as she was more round than jenni but jenni was in some brilliant episod hilda and zelda were amaz and there seem to be no explan for where they went libbi was a good charact too i never like morgan or roxi they just weren t as good as her other friend'\n",
      " u'i still don t know whi i forc myself to sit through the whole thing this film wasn t worth the memorex dvd r it was burn on i thought i was watch the end result of a group of middl schooler steal their parent camcord this is by far the worst movi ever made i truli from the bottom of my heart want to sue aaron yamasato for the two hour he stole from my life so appar it s suppos to be bad on purpos howev if you should end up in hell and are forc to watch this nine zero minut coil of doo doo you ll see that yamasato is realli tri hard to make an awesom flick the actor attempt dramat kick ass perform compar to crimson tide but come closer to the marin the crap act is just the tip of the iceberg the camera angl are aw the stori is c movi at best the plot isn t even good enough to be consid b movi calib the dialogu attempt to be dynam and witti but is crap like everyth els rumor has it that a hard copi of the screenplay actual attract fli plus the techno score is annoy not becaus it s techno but becaus it s non stop that s right the music play in the background the whole time act as a sublimin remind of how bad this thing is i don t care what the disclaim claim i don t buy it bot was not made this bad on purpos becaus it take itself way too serious for what it was a joke this film was veri low budget but that is no excus for it record set suck factor great film are born of substanc not budget bot had neither allow me to further articul the overwhelm power of this nine zero minut wast of time if i were have a three way with jessica alba and jessica biel in front of a tv and blood of the samurai came on i d be out of there quicker than steven seagal in execut decis undoubt some peopl will tri to defend the movi two mayb three they ll say it s grindhous chop socki or cheesi in a good way or it s so bad it s good those peopl are idiot a movi is either good or it s bad there s no such thing as a good bad movi but there are such thing as idiot that like crappi movi don t get me wrong there are lot of cornbal not to be taken serious movi out there that are enjoy and entertain slither is one bot is not this suckfest run about an hour and a half and in my humbl opinion it s nine zero minut too long the best thing about this film is the dvd cover so next time you re near the wal mart dvd bargain bin take a look at it don t touch it just look and quiet walk away'\n",
      " u'this movi is just plain dumb from the cast of ralph meeker as mike hammer to the fatuous climax the film is an exercis in wooden predict mike hammer is one of detect fiction s true sociopath unlik marlow and spade who put piec togeth to solv the mysteri hammer break thing apart to get to the truth this film turn hammer into a boob by surround him with bad guy who are well too dumb to get away with anyth one is so poor drawn that he succumb to a popcorn attack other part of the movi are right out of the three stoog play book velda s danc at the barr for instanc or the bad guy who accident stab his boss in the back and the continu break are shame frau blucher is run down the centerlin of the road when the camera is tight on her lower leg but she s way over the side when the camera pull back for a wider shot the worst break howev preced the popcorn attack the bad guy stalk hammer pass a clock second after our hero except the clock show he was seven minut behind our guy to be fair there were some interest camera angl and light and the grand final is so bad that it must been seen which is the onli reason that it get two point out of one zero'\n",
      " u'if somebodi want to make a realli realli bad movi wizard of the lost kingdom realli set a yardstick by which to measur the depth of bad start with the pseudo chewbacca that follow around the main charact some poor schmuck in a baggi white furri costum that look as if it was stitch togeth from discard piec of carpet work your way slowli pain through more not so special effect that thorough deni the viewer from suspens of disbelief add a garden gnome just for the heck of it on second thought skip this movi entir and find someth els to do for an hour and a half'\n",
      " u'just caus showcas sean conneri as a harvard law prof kate capshaw doe she still get work as his wife slight age differ and lawrenc fishburn as a racist southern cop and ed harri in a total over the top rendit of a fundamentalist southern serial killer weird cast but the movi play serious mindf with the audienc don t read if you ever intend to serious watch this film or to ever watch this film serious due to the spoiler first of all i felt myself roll my eye repeat at the liber stereotyp the cop are all sadist and frame this black guy with no evid the coron wit and even the lawyer of the accus collabor against him he is accus of the rape and murder of a young girl becaus he is black conneri is a harvard law prof who give impass speech about the injustic against black and against the barbar death penalti he is approach by the convict man s grandmoth to defend him and re open the trial conneri is stonewal yawn by the small town offici and the good il boy club but find that the case against blair the alleg killer now on death row was all fabric the main evid was his confess which was beaten out of him the beat was administ by a black cop who even play russian roulett to get the confess out of him conneri find out that anoth inmat on death row actual did the murder and after a few tete a tete with a serious overact hannib lecter like ed harri he find out where harri hid the murder weapon he get a re trial and blair is freed i think film over then sudden it turn out that blair is a psychot psycho and that he use white guilt to enlist conneri he concoct the stori with ed harri in return for blair carri out a few murder for harri now blair is on the loos again thank to conneri s delud pc principl the final three zero min are a weird action movi tack onto a legal drama conneri and fishburn fight the serial killer in an allig skin hous on stilt yes you read that right in the everglad that was one weird film so the whole system is corrupt and ineffici the cop are all just bulli and abu graib type tortur but the crimin are realli psychot and deserv to fri truli depress on everi level the system is complet rotten and the pc white guilt type who challeng it are serious delud too two thumb down conneri obvious had to make a mortgag payment or someth'\n",
      " u'this film was made in one nine four three when i think judi was at her peak look wise in her previous film for me and my gal peopl often say that she look emaci well in this film she look perfect she is beauti and show that she has a flair for comedi i think this film is hilari especi at the begin when she is tri to arrang an audit with john thornway one of the funniest scene s in my opinion is judi s rendit of ladi macbeth and when john is look for her at the parti to give her a spank lol one critic i do have is that there is a hole in the plot when john and lili fall in love i mean one minut he despis her and the next they are go out on a date then the next time they meet after that date they are in love anoth point i didn t like was on open night if i were lili i would be furious with john but she isn t it just doesn t make sens but all in all i would have to give this film a one zero becaus it is just wonder and almost perfect'\n",
      " u'utter brilliant power and evoc the most compel documentari seri ever made concern war it s tone offer a stark contrast to the often gung ho attitud toward world war two that the media exhibit rather than opt for scream about the horror of war it allow sir laurenc olivi s quiet voic to take a back seat to the true imag of war corps everywher explos terrifi citizen and soldier broken men indiffer politician mistak that cost thousand of live the suffer of the innoc most of all it truli bring home that mankind is capabl of when all normal rule of civil are remov there is someth distinct hobbesian about man in a true state of natur he will return to a more beast form capabl of crime that will still shock and fascin six zero year on perhap there could be a follow up seri call the centuri at war for the twentieth centuri was truli the centuri of horror i feel it is an ironi of immens magnitud that it took an event which caus the death of five zero million peopl to produc such a compel and excel seri such as this'\n",
      " u'america need the best man possibl to win the game so who do they hire a gymnast oh brother play by kurt thoma who has the necessari skill to win in a game which involv ninja a villag of crazi and richard norton who is told by kurt thoma to keep his hardwar in his pant his exact word i miss this in theater and it s a good reason becaus i would have probabl been kick out due to the laugh i broke into at regular interv the first thing that went through my mind was just how lame these ninja are if a gymnast can kick their ass kurt thoma is like five foot four and he hard strike one as the best man for the job as to the act talent of kurt thoma well if you can t say someth nice in all serious though one has to wonder how much cocain was be use to furnish an idea so stupid onli the decis to cast tara reid as a scientist top the dumb here for one eight year though this held the titl of the dumbest movi i had ever seen not to say i didn t find this unwatch i was laugh so hard i almost choke to death twice onli in the eight zero s could a movi with such a bad idea get made although for the record it is the onli movi to ever featur a hero so wimpi he can t even pull a wedg sword out from the ground this is the wimpiest action movi ever made and one of the most hilari also out of four bad'\n",
      " u'shut it off the prologu with fu manchu s birthday and the open credit of the assassin train is amus then it drop off faster than hair spray with neat look for a cameo by cato in the begin with a figur wink at the audienc'\n",
      " u'for the life of me whi did this film receiv an r rate while it is about flesh eat zombi believ it or not it s actual a pretti good famili friend film at least if your kid are age one zero and older unlik the tradit zombi film this one has an excel sens of humor as well as a tradit valu albeit a bit twist the languag isn t a serious problem there is no nuditi and the film style is definit gear toward kid much like the old tv show eeri indiana yet some knucklehead slap an r rate one it believ me most kid have seen wors violenc than this and it just seem silli to make audienc think this is an adult onli film the stori is set in a parallel type world while the fashion car and more appear circa one nine five three in this bizarro world there has been a fierc recent zombi plagu that result in the zombi war and massiv chang in everyday life at school kid are train in arm combat and there s a cute scene late in the film where the father give his son a handgun and tell him to keep it in his backpack just in case as for life outsid of school it s pretti weird as well as peopl now have learn that zombi aren t such a bad thing heck use shock collar and train they can be made into slave who can do your housework clean street deliv milk or in the case of a realli sick guy be your special friend this film deal with one particular famili that final buy their first zombi slave play by billi connal mom is thrill and her son slowli becom the zombi s friend dad on the other hand isn t convinc as he was forc year earli to kill his own zombi father and he hasn t yet gotten over this funni irrever and uniqu this film need to be seen by a much wider audienc']\n",
      "[u'steal is a crime and these guy kenni yakkel and corbin bernsen look like their go to get away with it i haven t even seen this film but not onli do i know it suck but the fact that it steal the stori of anoth film or two film for that matter is such bull crap and if imdb would allow profan much more than just bull crap that i becom fill with rage and feel as though i should just throw myself out the window and just end it all o k that s a littl much but steal is wors ever seen pontypool it was this awesom littl zombi film made in two zero zero eight this radio host goe into work and then this zombi attack happen we onli see what s go on insid the radio station and the onli guess we have on what s happen on the outsid are the occasion call from their eye in the sky ken loney easili the funniest moment in the film in my mind and the bbc call in for an updat on the situat from all the review i ve read and from the convers i had with my friend who has seen this film and pontypool this film was exact like it in the stori with minor tweak here and there so throw origin and creativ out the window as for the zombi or the infect whatev you want to call them they are the exact same thing just becaus they got infect doe not chang the fact that they walk around mindless and have a never end quench for flesh they are zombi zombi zombi okay one to mani but back to this crappi movi the zombi from what i ve heard are a lot like the one in two eight day later anoth better movi with their insan rage and even more insan quench of flesh this is where zombi movi defin themselv stori doe not matter in a zombi movi as long as your not take the idea of anoth film sure a zombi movi can be enhanc by the stori as is the case with pontypool but the zombi have to be in a way origin two eight day later start the insan rage zombi pontypool i can t even begin to explain those guy without ruin the film the evil dead i think all i have to do is just type the name again becaus come on it s the evil dead romero is the master and he went through all differ type of zombi from the paint face zombi in dawn of the dead to rage zombi in diari of the dead and he did each one with his own uniqu style so after talk about other great zombi movi i think it s about time i explain whi i hate this movi without even see it this movi down right steal the origin stori outlin to pontypool which i think is call plagiar and is illeg in this countri and mani peopl get kick out of school for do this veri thing yet in hollywood it s allow and appar approv of this is not the onli film that take the outlin of anoth movi and just tri to tell it differ the nine zero s is like the worst period for movi ever sure it had some gem but what decad doesn t and it s becaus all the movi were the exact same my favorit the crime movi with a twist so big that you ll never guess it till the veri end they forc feed you one suspect make it seem imposs that it s not anybodi els but him then bam all this time it was this guy you just had to look at the scene where he wasn t there and then when he would mysteri show up out of the blue say hey guy what did i miss oh didn t see that come did ya well i ll leav a clich as my close statement this is one film you ll sure want to miss'\n",
      " u'i just finish watch follow and i thought it was great i rate it eight out of one zero i plan on watch it again with the director s commentari and then again in chronolog order i rent this movi becaus of my fascin of christoph nolan s more recent movi memento follow has some similar this movi was probabl the blueprint for memento even the music in some part is veri similar shoot the movi in black and white give it a mysteri feel the stori and dialogu is realli good the perform of the actor is believ christoph nolan made this movi on a realli low budget i look forward to his next releas insomnia a big budget movi with my favourit actor al pacino'\n",
      " u'the first dh wasn t that great but i realli didn t expect it to be but this horribl movi was just beyond critic i realli tri to look on the bright side and give movi like this a chanc but i just could not find a real good thing about this one i appreci what bill cowel was tri to do but this movi was just soooooo bore the stori of the movi realli isn t that bad in fact it s somewhat origin but the movi form is realli as bad as a lot of peopl say in my opinion this one rank right up there with the off season and dracula three zero zero zero i know a lot of peopl realli trash and put down movi like this but i realli can t think of ani other good thing to say about it'\n",
      " u'usual i m the one critic the twenti someth neanderth for not be abl to appreci a film unless it has plastic t ts gunfight and car chase howev in this case the film might actual have been improv with a few of those addit at least i wouldn t have gotten bore after an hour and chang channel i don t mind surreal and i certain don t mind have to pay attent to find subtleti or hidden mean but there should be some point to the whole thing i didn t get the feel that even the writer or director realli had a broad vision of anyth but were instead just so self absorb in their own pretenti vision that they becam deliber scatter or perhap they just got confus themselv either way i don t care it bore the crap out of me for just over an hour with no save grace although a whole pack of other viewer have fill up this site with excit rave about the alleg symbol and master cinematographi i must respect disagre perhap i didn t minc through enough film class to appreci some inspir techniqu not visibl to mere mortal or perhap this movi was just crap i give it a one and file it next to ishtar'\n",
      " u'this is the stori of a maniac cop who for some reason has it in for a young colleg stud and his mate after they report him to his supervisor who in turn suspend him pend psychiatr evalu he find an opportun to psycholog tortur them when on a bet the kid hack into a depart store s secur and unlock the door onli they get lock in the store along with the weirdo murder and mayhem are afoot and the kid are run around tri to surviv until morn when they may be abl to escap danger game would have been a success cat and mous psycho thriller probabl if it was set in a differ locat the thought of psychot cop chase around a bunch of innoc teenag in a depart store just didn t work especi when he come face to face with his flinch prey quit frequent and yet doe noth serious quit often there was no real confront as would be suffici for this kind of stori and may ve work better if say for exampl the teen were loos in the neighborhood and left to fend for themselv against this weirdo especi if that took a few day while he make them increas paranoid although grant even that is clich d what a shame too that it could not have been a better thriller consid a funki cast of young australian charact even a light heart adventur despit the mad of the villain interspers through the pictur might have even made it a more satisfi pictur instead it start out fresh and sure did have plenti of action sequenc but wound up verg on the ridicul'\n",
      " u'this is the thirteenth falcon film tom conway has lost none of his humour and style and is not show sign of get tire the film has a veri satisfi stori with lot of red herring suspect and dame madg meredith is the good girl of the stori she play it adequ but by no mean set the screen on fire myrna dell is a bad girl and she put on an excel face of stone with eye of agat and you are just wait for her to kill as mani peopl as possibl to cheer herself up edward brophi is back as goldi the sidekick but surpris surpris his manic over act has stop and he is actual under control this is a fine tribut to the directori skill of william a berk who had done so mani western he probabl was not prepar to take ani nonsens from a brooklyn dummi the result is that for the first time possibl in his career brophi was tone down enough actual to add someth to a film rather than tri the viewer s patienc with the irrit behaviour of a retard but unruli six year old it all goe along veri well and is thorough entertain'\n",
      " u'as other review have note this movi is a cross between i e stolen from stori we have seen befor specif this look like clint eastwood in high plain drifter insert into mad max remov clint s cigar and replac with a cigarett remov his hors and give him a high tech motorcycl and voil an updat drifter in this movi the hero is even more blatant a savior than high plain drifter now our hero has long brown hair suffer a wound to his left side and his entri into town is preced by a plea for salvat by the surviv townspeopl a pretti transpar refer to a second come i watch the movi on a hot humid morn sleep was imposs and upon aris at four three zero am there was noth els on tv so the movi serv it purpos while unorigin with charact that are almost comic caricatur the movi is still somewhat entertain at least at four three zero in the morn'\n",
      " u'sever year ago i saw this film without subtitl on televis and despit me not understand a word of what the charact were say i still got the general idea and the mood of the film fascin me no end at long last i saw it again a few week ago my heart skip when i saw the pictur in the televis guid and for eight day until the film was realli shown i told everybodi i knew to go and see it the stori remind me a bit of alfr hitchcock s vertigo a slow brood film about a guy who one day believ he see the girlfriend that disappear year befor what follow is a wild rollercoast ride of flashback chang perspect and realli invent twist in the plot and at the end of the film i was left breathless i had definit not got what i had expect and i had actual alreadi seen the film be prepar to be confus nine out of one zero'\n",
      " u'i agre with andi this is a good movi kevin mckidd s charact is believ throughout the film we re forc to hate him and latter sympathis with him paula sage who play roberta put in a good perform too it s thought provok and emot without ani slush over product credit to director alison peebl and writer andrea gibb for that a veri worthwhil view the pace of the film is just right rais just enough interest in the subject matter to reel you in rather than bombard you with fact in a documentari style nice littl soundtrack to go with the film too again use spare not to distract you from storylin recommend'\n",
      " u'first let me say that although i general appreci mike judg s work i ve been mere tepid in my respons to offic space king of the hill and beavi and butthead i general prefer more intellig comedi and therein lie the ironi with respect to idiocraci in a futur world where the embodi of beavi and butthead s view basest instinct and intellectu capac are the framework of a chaotic messi semi mad max semi blade runner societi where everi trailer trash guy s fantasi becom realiti a man with even averag intellig is threaten and accus of talk gay and the mob mental take over and this world is also incred funni yes it s obvious that carl s jr starbuck costco and fuddruck execut will be horrifi at the twist valu given their product in the year two five zero five there were some miss opportun with the film and the relationship between the time travel the other be an averag intellig woman who s worri about her boyfriend s pimp s retribut could have been stronger the chemistri is there and there don t seem to be too mani women in the futur i did leav with a grin on my face but the experi is a bit better than the memori thus it s my kind of popcorn film and it will be fun to revisit on video recommend fyi stay through the credit for an extra scene']\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(labeled_train_clean_reviews, 10))\n",
    "print(np.random.choice(test_clean_reviews, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_labeled_train_clean_reviews = \\\n",
    "    pd.DataFrame(data={\"id\": labeled_train[\"id\"], \"sentiment\": labeled_train[\"sentiment\"], \"review\": labeled_train_clean_reviews})\n",
    "output_test_clean_reviews = \\\n",
    "    pd.DataFrame(data={\"id\": test[\"id\"], \"review\": test_clean_reviews})\n",
    "output_labeled_train_clean_reviews.to_csv(os.path.join('../', 'output', \"labeled_train_clean_reviews_stemmed.csv\"), index=False, quoting=3)\n",
    "output_test_clean_reviews.to_csv(os.path.join('../', 'output', \"test_clean_reviews_stemmed.csv\"), index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 08:11:48,426 : INFO : 'pattern' package found; tag filters are available for English\n",
      "2019-01-06 08:11:48,432 : INFO : collecting all words and their counts\n",
      "2019-01-06 08:11:48,434 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-01-06 08:11:48,502 : INFO : PROGRESS: at sentence #10000, processed 229166 words, keeping 12465 word types\n",
      "2019-01-06 08:11:48,574 : INFO : PROGRESS: at sentence #20000, processed 458297 words, keeping 17070 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Word2Vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 08:11:48,646 : INFO : PROGRESS: at sentence #30000, processed 680652 words, keeping 20370 word types\n",
      "2019-01-06 08:11:48,721 : INFO : PROGRESS: at sentence #40000, processed 910089 words, keeping 23125 word types\n",
      "2019-01-06 08:11:48,798 : INFO : PROGRESS: at sentence #50000, processed 1132423 words, keeping 25365 word types\n",
      "2019-01-06 08:11:48,868 : INFO : PROGRESS: at sentence #60000, processed 1356968 words, keeping 27283 word types\n",
      "2019-01-06 08:11:48,945 : INFO : PROGRESS: at sentence #70000, processed 1583238 words, keeping 29024 word types\n",
      "2019-01-06 08:11:49,019 : INFO : PROGRESS: at sentence #80000, processed 1805500 words, keeping 30603 word types\n",
      "2019-01-06 08:11:49,092 : INFO : PROGRESS: at sentence #90000, processed 2032837 words, keeping 32223 word types\n",
      "2019-01-06 08:11:49,166 : INFO : PROGRESS: at sentence #100000, processed 2257707 words, keeping 33579 word types\n",
      "2019-01-06 08:11:49,243 : INFO : PROGRESS: at sentence #110000, processed 2480677 words, keeping 34827 word types\n",
      "2019-01-06 08:11:49,317 : INFO : PROGRESS: at sentence #120000, processed 2705814 words, keeping 36183 word types\n",
      "2019-01-06 08:11:49,392 : INFO : PROGRESS: at sentence #130000, processed 2934773 words, keeping 37353 word types\n",
      "2019-01-06 08:11:49,462 : INFO : PROGRESS: at sentence #140000, processed 3150424 words, keeping 38376 word types\n",
      "2019-01-06 08:11:49,530 : INFO : PROGRESS: at sentence #150000, processed 3379002 words, keeping 39556 word types\n",
      "2019-01-06 08:11:49,595 : INFO : PROGRESS: at sentence #160000, processed 3604642 words, keeping 40629 word types\n",
      "2019-01-06 08:11:49,660 : INFO : PROGRESS: at sentence #170000, processed 3831024 words, keeping 41628 word types\n",
      "2019-01-06 08:11:49,725 : INFO : PROGRESS: at sentence #180000, processed 4054617 words, keeping 42599 word types\n",
      "2019-01-06 08:11:49,794 : INFO : PROGRESS: at sentence #190000, processed 4282897 words, keeping 43461 word types\n",
      "2019-01-06 08:11:49,876 : INFO : PROGRESS: at sentence #200000, processed 4509999 words, keeping 44301 word types\n",
      "2019-01-06 08:11:49,956 : INFO : PROGRESS: at sentence #210000, processed 4734668 words, keeping 45212 word types\n",
      "2019-01-06 08:11:50,037 : INFO : PROGRESS: at sentence #220000, processed 4962787 words, keeping 46134 word types\n",
      "2019-01-06 08:11:50,118 : INFO : PROGRESS: at sentence #230000, processed 5188480 words, keeping 46986 word types\n",
      "2019-01-06 08:11:50,198 : INFO : PROGRESS: at sentence #240000, processed 5419180 words, keeping 47854 word types\n",
      "2019-01-06 08:11:50,275 : INFO : PROGRESS: at sentence #250000, processed 5636219 words, keeping 48699 word types\n",
      "2019-01-06 08:11:50,349 : INFO : PROGRESS: at sentence #260000, processed 5859234 words, keeping 49469 word types\n",
      "2019-01-06 08:11:50,424 : INFO : PROGRESS: at sentence #270000, processed 6083817 words, keeping 50416 word types\n",
      "2019-01-06 08:11:50,507 : INFO : PROGRESS: at sentence #280000, processed 6312682 words, keeping 51640 word types\n",
      "2019-01-06 08:11:50,585 : INFO : PROGRESS: at sentence #290000, processed 6538799 words, keeping 52754 word types\n",
      "2019-01-06 08:11:50,672 : INFO : PROGRESS: at sentence #300000, processed 6766532 words, keeping 53755 word types\n",
      "2019-01-06 08:11:50,749 : INFO : PROGRESS: at sentence #310000, processed 6994910 words, keeping 54734 word types\n",
      "2019-01-06 08:11:50,824 : INFO : PROGRESS: at sentence #320000, processed 7222831 words, keeping 55770 word types\n",
      "2019-01-06 08:11:50,898 : INFO : PROGRESS: at sentence #330000, processed 7447731 words, keeping 56687 word types\n",
      "2019-01-06 08:11:50,976 : INFO : PROGRESS: at sentence #340000, processed 7680440 words, keeping 57629 word types\n",
      "2019-01-06 08:11:51,051 : INFO : PROGRESS: at sentence #350000, processed 7906671 words, keeping 58485 word types\n",
      "2019-01-06 08:11:51,125 : INFO : PROGRESS: at sentence #360000, processed 8130069 words, keeping 59345 word types\n",
      "2019-01-06 08:11:51,205 : INFO : PROGRESS: at sentence #370000, processed 8360199 words, keeping 60161 word types\n",
      "2019-01-06 08:11:51,282 : INFO : PROGRESS: at sentence #380000, processed 8588323 words, keeping 61069 word types\n",
      "2019-01-06 08:11:51,359 : INFO : PROGRESS: at sentence #390000, processed 8821215 words, keeping 61810 word types\n",
      "2019-01-06 08:11:51,432 : INFO : PROGRESS: at sentence #400000, processed 9047140 words, keeping 62546 word types\n",
      "2019-01-06 08:11:51,510 : INFO : PROGRESS: at sentence #410000, processed 9271446 words, keeping 63263 word types\n",
      "2019-01-06 08:11:51,585 : INFO : PROGRESS: at sentence #420000, processed 9495586 words, keeping 64024 word types\n",
      "2019-01-06 08:11:51,664 : INFO : PROGRESS: at sentence #430000, processed 9726512 words, keeping 64795 word types\n",
      "2019-01-06 08:11:51,741 : INFO : PROGRESS: at sentence #440000, processed 9956570 words, keeping 65539 word types\n",
      "2019-01-06 08:11:51,825 : INFO : PROGRESS: at sentence #450000, processed 10183262 words, keeping 66378 word types\n",
      "2019-01-06 08:11:51,904 : INFO : PROGRESS: at sentence #460000, processed 10419261 words, keeping 67158 word types\n",
      "2019-01-06 08:11:51,982 : INFO : PROGRESS: at sentence #470000, processed 10650184 words, keeping 67775 word types\n",
      "2019-01-06 08:11:52,056 : INFO : PROGRESS: at sentence #480000, processed 10873555 words, keeping 68500 word types\n",
      "2019-01-06 08:11:52,132 : INFO : PROGRESS: at sentence #490000, processed 11103736 words, keeping 69256 word types\n",
      "2019-01-06 08:11:52,207 : INFO : PROGRESS: at sentence #500000, processed 11328543 words, keeping 69892 word types\n",
      "2019-01-06 08:11:52,283 : INFO : PROGRESS: at sentence #510000, processed 11556927 words, keeping 70593 word types\n",
      "2019-01-06 08:11:52,358 : INFO : PROGRESS: at sentence #520000, processed 11783512 words, keeping 71267 word types\n",
      "2019-01-06 08:11:52,431 : INFO : PROGRESS: at sentence #530000, processed 12011039 words, keeping 71877 word types\n",
      "2019-01-06 08:11:52,506 : INFO : PROGRESS: at sentence #540000, processed 12238852 words, keeping 72537 word types\n",
      "2019-01-06 08:11:52,581 : INFO : PROGRESS: at sentence #550000, processed 12467490 words, keeping 73212 word types\n",
      "2019-01-06 08:11:52,656 : INFO : PROGRESS: at sentence #560000, processed 12691615 words, keeping 73861 word types\n",
      "2019-01-06 08:11:52,734 : INFO : PROGRESS: at sentence #570000, processed 12923711 words, keeping 74431 word types\n",
      "2019-01-06 08:11:52,809 : INFO : PROGRESS: at sentence #580000, processed 13148083 words, keeping 75087 word types\n",
      "2019-01-06 08:11:52,883 : INFO : PROGRESS: at sentence #590000, processed 13376619 words, keeping 75733 word types\n",
      "2019-01-06 08:11:52,958 : INFO : PROGRESS: at sentence #600000, processed 13601946 words, keeping 76294 word types\n",
      "2019-01-06 08:11:53,032 : INFO : PROGRESS: at sentence #610000, processed 13825965 words, keeping 76952 word types\n",
      "2019-01-06 08:11:53,106 : INFO : PROGRESS: at sentence #620000, processed 14055595 words, keeping 77503 word types\n",
      "2019-01-06 08:11:53,186 : INFO : PROGRESS: at sentence #630000, processed 14283003 words, keeping 78066 word types\n",
      "2019-01-06 08:11:53,274 : INFO : PROGRESS: at sentence #640000, processed 14506788 words, keeping 78692 word types\n",
      "2019-01-06 08:11:53,355 : INFO : PROGRESS: at sentence #650000, processed 14735829 words, keeping 79295 word types\n",
      "2019-01-06 08:11:53,435 : INFO : PROGRESS: at sentence #660000, processed 14961517 words, keeping 79864 word types\n",
      "2019-01-06 08:11:53,512 : INFO : PROGRESS: at sentence #670000, processed 15187871 words, keeping 80381 word types\n",
      "2019-01-06 08:11:53,589 : INFO : PROGRESS: at sentence #680000, processed 15415849 words, keeping 80912 word types\n",
      "2019-01-06 08:11:53,665 : INFO : PROGRESS: at sentence #690000, processed 15640987 words, keeping 81482 word types\n",
      "2019-01-06 08:11:53,742 : INFO : PROGRESS: at sentence #700000, processed 15872499 words, keeping 82074 word types\n",
      "2019-01-06 08:11:53,823 : INFO : PROGRESS: at sentence #710000, processed 16098551 words, keeping 82560 word types\n",
      "2019-01-06 08:11:53,900 : INFO : PROGRESS: at sentence #720000, processed 16326874 words, keeping 83036 word types\n",
      "2019-01-06 08:11:53,977 : INFO : PROGRESS: at sentence #730000, processed 16556101 words, keeping 83571 word types\n",
      "2019-01-06 08:11:54,053 : INFO : PROGRESS: at sentence #740000, processed 16780279 words, keeping 84127 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 08:11:54,128 : INFO : PROGRESS: at sentence #750000, processed 17001852 words, keeping 84599 word types\n",
      "2019-01-06 08:11:54,204 : INFO : PROGRESS: at sentence #760000, processed 17224202 words, keeping 85068 word types\n",
      "2019-01-06 08:11:54,284 : INFO : PROGRESS: at sentence #770000, processed 17454566 words, keeping 85644 word types\n",
      "2019-01-06 08:11:54,362 : INFO : PROGRESS: at sentence #780000, processed 17687962 words, keeping 86160 word types\n",
      "2019-01-06 08:11:54,441 : INFO : PROGRESS: at sentence #790000, processed 17918070 words, keeping 86665 word types\n",
      "2019-01-06 08:11:54,487 : INFO : collected 86996 word types from a corpus of 18042771 raw words and 795538 sentences\n",
      "2019-01-06 08:11:54,489 : INFO : Loading a fresh vocabulary\n",
      "2019-01-06 08:11:55,809 : INFO : effective_min_count=40 retains 11986 unique words (13% of original 86996, drops 75010)\n",
      "2019-01-06 08:11:55,811 : INFO : effective_min_count=40 leaves 17678536 word corpus (97% of original 18042771, drops 364235)\n",
      "2019-01-06 08:11:55,850 : INFO : deleting the raw counts dictionary of 86996 items\n",
      "2019-01-06 08:11:55,856 : INFO : sample=0.001 downsamples 50 most-common words\n",
      "2019-01-06 08:11:55,858 : INFO : downsampling leaves estimated 13094829 word corpus (74.1% of prior 17678536)\n",
      "2019-01-06 08:11:55,888 : INFO : estimated required memory for 11986 words and 300 dimensions: 34759400 bytes\n",
      "2019-01-06 08:11:55,889 : INFO : resetting layer weights\n",
      "2019-01-06 08:11:56,115 : INFO : training model with 4 workers on 11986 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-06 08:11:57,127 : INFO : EPOCH 1 - PROGRESS: at 4.69% examples, 611545 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:11:58,130 : INFO : EPOCH 1 - PROGRESS: at 9.55% examples, 620027 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:11:59,135 : INFO : EPOCH 1 - PROGRESS: at 14.41% examples, 622304 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:00,136 : INFO : EPOCH 1 - PROGRESS: at 19.21% examples, 622395 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:01,142 : INFO : EPOCH 1 - PROGRESS: at 24.09% examples, 624646 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:02,143 : INFO : EPOCH 1 - PROGRESS: at 28.92% examples, 625412 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:03,150 : INFO : EPOCH 1 - PROGRESS: at 33.80% examples, 625427 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:04,155 : INFO : EPOCH 1 - PROGRESS: at 38.64% examples, 626631 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:05,155 : INFO : EPOCH 1 - PROGRESS: at 43.42% examples, 627059 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:06,159 : INFO : EPOCH 1 - PROGRESS: at 48.27% examples, 627781 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:07,160 : INFO : EPOCH 1 - PROGRESS: at 53.15% examples, 628611 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:08,172 : INFO : EPOCH 1 - PROGRESS: at 57.99% examples, 629284 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:09,182 : INFO : EPOCH 1 - PROGRESS: at 62.90% examples, 630006 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:10,195 : INFO : EPOCH 1 - PROGRESS: at 67.86% examples, 630980 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:11,207 : INFO : EPOCH 1 - PROGRESS: at 72.77% examples, 631409 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:12,210 : INFO : EPOCH 1 - PROGRESS: at 77.69% examples, 632110 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:13,212 : INFO : EPOCH 1 - PROGRESS: at 82.63% examples, 632692 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:14,216 : INFO : EPOCH 1 - PROGRESS: at 87.54% examples, 633273 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:15,226 : INFO : EPOCH 1 - PROGRESS: at 92.51% examples, 633926 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:16,231 : INFO : EPOCH 1 - PROGRESS: at 97.45% examples, 634324 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:16,718 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 08:12:16,735 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 08:12:16,738 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 08:12:16,741 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 08:12:16,742 : INFO : EPOCH - 1 : training on 18042771 raw words (13096121 effective words) took 20.6s, 635003 effective words/s\n",
      "2019-01-06 08:12:17,753 : INFO : EPOCH 2 - PROGRESS: at 4.86% examples, 633250 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:18,754 : INFO : EPOCH 2 - PROGRESS: at 9.90% examples, 642280 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:19,760 : INFO : EPOCH 2 - PROGRESS: at 14.96% examples, 646596 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:20,764 : INFO : EPOCH 2 - PROGRESS: at 19.99% examples, 647032 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:21,767 : INFO : EPOCH 2 - PROGRESS: at 24.98% examples, 647735 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:22,768 : INFO : EPOCH 2 - PROGRESS: at 29.95% examples, 648237 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:23,770 : INFO : EPOCH 2 - PROGRESS: at 35.06% examples, 649547 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:24,772 : INFO : EPOCH 2 - PROGRESS: at 40.07% examples, 650581 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:25,780 : INFO : EPOCH 2 - PROGRESS: at 45.10% examples, 651023 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:26,782 : INFO : EPOCH 2 - PROGRESS: at 50.08% examples, 651672 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:27,784 : INFO : EPOCH 2 - PROGRESS: at 55.11% examples, 652176 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:28,796 : INFO : EPOCH 2 - PROGRESS: at 60.07% examples, 652084 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:29,805 : INFO : EPOCH 2 - PROGRESS: at 65.10% examples, 652215 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:30,812 : INFO : EPOCH 2 - PROGRESS: at 70.13% examples, 652360 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:31,827 : INFO : EPOCH 2 - PROGRESS: at 75.20% examples, 652646 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:32,836 : INFO : EPOCH 2 - PROGRESS: at 80.26% examples, 652708 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:33,843 : INFO : EPOCH 2 - PROGRESS: at 85.21% examples, 652362 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:34,851 : INFO : EPOCH 2 - PROGRESS: at 90.28% examples, 652847 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:35,856 : INFO : EPOCH 2 - PROGRESS: at 95.43% examples, 653395 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-06 08:12:36,739 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 08:12:36,749 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 08:12:36,762 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 08:12:36,764 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 08:12:36,766 : INFO : EPOCH - 2 : training on 18042771 raw words (13093530 effective words) took 20.0s, 653995 effective words/s\n",
      "2019-01-06 08:12:37,780 : INFO : EPOCH 3 - PROGRESS: at 4.91% examples, 638928 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:38,791 : INFO : EPOCH 3 - PROGRESS: at 10.06% examples, 649052 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:39,795 : INFO : EPOCH 3 - PROGRESS: at 15.18% examples, 653727 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:40,798 : INFO : EPOCH 3 - PROGRESS: at 20.33% examples, 656271 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:41,808 : INFO : EPOCH 3 - PROGRESS: at 25.43% examples, 656937 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:42,812 : INFO : EPOCH 3 - PROGRESS: at 30.46% examples, 656894 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:43,824 : INFO : EPOCH 3 - PROGRESS: at 35.56% examples, 656021 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:44,832 : INFO : EPOCH 3 - PROGRESS: at 40.63% examples, 656573 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:45,836 : INFO : EPOCH 3 - PROGRESS: at 45.69% examples, 657411 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:46,842 : INFO : EPOCH 3 - PROGRESS: at 50.76% examples, 657960 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:47,845 : INFO : EPOCH 3 - PROGRESS: at 55.83% examples, 658569 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 08:12:48,853 : INFO : EPOCH 3 - PROGRESS: at 60.84% examples, 658724 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:49,864 : INFO : EPOCH 3 - PROGRESS: at 65.92% examples, 658812 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:50,872 : INFO : EPOCH 3 - PROGRESS: at 71.01% examples, 658978 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:51,881 : INFO : EPOCH 3 - PROGRESS: at 76.05% examples, 658625 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:52,882 : INFO : EPOCH 3 - PROGRESS: at 81.03% examples, 658142 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:53,883 : INFO : EPOCH 3 - PROGRESS: at 86.05% examples, 658161 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:54,892 : INFO : EPOCH 3 - PROGRESS: at 91.12% examples, 658314 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:55,899 : INFO : EPOCH 3 - PROGRESS: at 96.31% examples, 658874 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:56,617 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 08:12:56,631 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 08:12:56,633 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 08:12:56,637 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 08:12:56,638 : INFO : EPOCH - 3 : training on 18042771 raw words (13094189 effective words) took 19.9s, 659049 effective words/s\n",
      "2019-01-06 08:12:57,649 : INFO : EPOCH 4 - PROGRESS: at 4.96% examples, 648248 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:58,651 : INFO : EPOCH 4 - PROGRESS: at 10.12% examples, 656586 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:12:59,658 : INFO : EPOCH 4 - PROGRESS: at 15.24% examples, 658019 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:00,666 : INFO : EPOCH 4 - PROGRESS: at 20.38% examples, 658807 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:01,679 : INFO : EPOCH 4 - PROGRESS: at 25.54% examples, 660153 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:02,691 : INFO : EPOCH 4 - PROGRESS: at 30.69% examples, 660998 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:03,694 : INFO : EPOCH 4 - PROGRESS: at 35.78% examples, 660350 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:04,706 : INFO : EPOCH 4 - PROGRESS: at 40.90% examples, 661056 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:05,708 : INFO : EPOCH 4 - PROGRESS: at 45.98% examples, 661567 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:06,714 : INFO : EPOCH 4 - PROGRESS: at 51.03% examples, 661689 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:07,718 : INFO : EPOCH 4 - PROGRESS: at 56.11% examples, 661838 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:08,719 : INFO : EPOCH 4 - PROGRESS: at 61.12% examples, 662103 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:09,720 : INFO : EPOCH 4 - PROGRESS: at 66.19% examples, 662485 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:10,725 : INFO : EPOCH 4 - PROGRESS: at 71.28% examples, 662523 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:11,730 : INFO : EPOCH 4 - PROGRESS: at 76.43% examples, 663050 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:12,731 : INFO : EPOCH 4 - PROGRESS: at 81.51% examples, 663198 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:13,739 : INFO : EPOCH 4 - PROGRESS: at 86.61% examples, 663126 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:14,742 : INFO : EPOCH 4 - PROGRESS: at 91.67% examples, 663178 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:15,748 : INFO : EPOCH 4 - PROGRESS: at 96.80% examples, 663150 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:16,375 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 08:13:16,383 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 08:13:16,395 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 08:13:16,398 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 08:13:16,399 : INFO : EPOCH - 4 : training on 18042771 raw words (13095791 effective words) took 19.8s, 662852 effective words/s\n",
      "2019-01-06 08:13:17,405 : INFO : EPOCH 5 - PROGRESS: at 4.96% examples, 650893 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:18,408 : INFO : EPOCH 5 - PROGRESS: at 10.12% examples, 658238 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-06 08:13:19,416 : INFO : EPOCH 5 - PROGRESS: at 15.29% examples, 661319 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:20,426 : INFO : EPOCH 5 - PROGRESS: at 20.50% examples, 662808 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:21,426 : INFO : EPOCH 5 - PROGRESS: at 25.49% examples, 660574 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:22,428 : INFO : EPOCH 5 - PROGRESS: at 30.52% examples, 660146 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:23,429 : INFO : EPOCH 5 - PROGRESS: at 35.68% examples, 660870 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:24,436 : INFO : EPOCH 5 - PROGRESS: at 40.79% examples, 661845 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:25,441 : INFO : EPOCH 5 - PROGRESS: at 45.87% examples, 661956 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:26,451 : INFO : EPOCH 5 - PROGRESS: at 51.03% examples, 663252 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:27,452 : INFO : EPOCH 5 - PROGRESS: at 56.11% examples, 663447 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:28,459 : INFO : EPOCH 5 - PROGRESS: at 61.12% examples, 663272 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:29,471 : INFO : EPOCH 5 - PROGRESS: at 66.25% examples, 663487 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:30,474 : INFO : EPOCH 5 - PROGRESS: at 71.34% examples, 663542 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:31,483 : INFO : EPOCH 5 - PROGRESS: at 76.49% examples, 663819 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:32,487 : INFO : EPOCH 5 - PROGRESS: at 81.57% examples, 663838 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:33,492 : INFO : EPOCH 5 - PROGRESS: at 86.72% examples, 664219 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:34,492 : INFO : EPOCH 5 - PROGRESS: at 91.77% examples, 664369 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:35,494 : INFO : EPOCH 5 - PROGRESS: at 96.96% examples, 664800 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 08:13:36,068 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 08:13:36,078 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 08:13:36,090 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 08:13:36,092 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 08:13:36,093 : INFO : EPOCH - 5 : training on 18042771 raw words (13095617 effective words) took 19.7s, 665088 effective words/s\n",
      "2019-01-06 08:13:36,095 : INFO : training on a 90213855 raw words (65475248 effective words) took 100.0s, 654893 effective words/s\n",
      "2019-01-06 08:13:36,096 : INFO : saving Word2Vec object under ../input/word2vec_model_300dim_40minwords_10context_stemmed, separately None\n",
      "2019-01-06 08:13:36,098 : INFO : not storing attribute vectors_norm\n",
      "2019-01-06 08:13:36,100 : INFO : not storing attribute cum_table\n",
      "2019-01-06 08:13:36,230 : INFO : saved ../input/word2vec_model_300dim_40minwords_10context_stemmed\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec\n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 300  # Word vector dimensionality\n",
    "MIN_WORD_COUNT = 40  # Minimum word count. Kaggle set to 40, to avoid attaching too much importance to individual movie titles.\n",
    "NUM_THREADS = 4  # Number of threads to run in parallel\n",
    "CONTEXT = 10  # Context window size\n",
    "DOWNSAMPLING = 1e-3  # Downsample setting for frequent words\n",
    "WORD2VEC_MODEL_FILE = BASE_DIR + \\\n",
    "    \"word2vec_model_\" + \\\n",
    "    str(EMBEDDING_DIM) + \"dim_\" + \\\n",
    "    str(MIN_WORD_COUNT) + \"minwords_\" + \\\n",
    "    str(CONTEXT) + \"context_\" +\\\n",
    "    \"stemmed\"\n",
    "\n",
    "print \"Training the Word2Vec model...\"\n",
    "model = Word2Vec(sentences, workers=NUM_THREADS, \\\n",
    "                 size=EMBEDDING_DIM, min_count=MIN_WORD_COUNT, \\\n",
    "                 window=CONTEXT, sample=DOWNSAMPLING, seed=1)\n",
    "model.save(WORD2VEC_MODEL_FILE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
