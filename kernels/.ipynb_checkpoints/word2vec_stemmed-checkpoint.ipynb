{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec_model_300dim_40minwords_10context', 'sampleSubmission.csv', 'word2vec_model_300dim_40minwords_10context_stemmed', 'labeledTrainData.tsv', 'test_submission.csv', 'testData.tsv', 'unlabeledTrainData.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeledTrainData.tsv'\n",
    "UNLABELED_TRAIN_DF = BASE_DIR + 'unlabeledTrainData.tsv'\n",
    "TEST_DF = BASE_DIR + 'testData.tsv'\n",
    "print(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 50000 unlabeled train reviews, and 25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv(UNLABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "test = pd.read_csv(TEST_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "print \"Read %d labeled train reviews, %d unlabeled train reviews, \" \\\n",
    "          \"and %d test reviews\" % (labeled_train[\"review\"].size, unlabeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage\n",
    "\n",
    "Check if test[\"sentiment\"] is correct "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Kaggle tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def review_to_clean_review(review, remove_numbers=True, stem_words=False):\n",
    "    # Function to convert a document to a clean document,\n",
    "    # optionally removing numbers.  Returns a string.\n",
    "    #\n",
    "    # 1. Remove HTML using lxml parser, ranked best by bs4\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    #\n",
    "    # https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings#L92\n",
    "    # https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    #  \n",
    "    # 2. Remove non-letters and non-numbers\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    #\n",
    "    # 3. Optionally remove numbers\n",
    "    if remove_numbers:\n",
    "        review_text = re.sub(\"[0-9]\", \" \", review_text)\n",
    "    else:\n",
    "        review_text = review_text.replace('0', ' zero ')\n",
    "        review_text = review_text.replace('1', ' one ')\n",
    "        review_text = review_text.replace('2', ' two ')\n",
    "        review_text = review_text.replace('3', ' three ')\n",
    "        review_text = review_text.replace('4', ' four ')\n",
    "        review_text = review_text.replace('5', ' five ')\n",
    "        review_text = review_text.replace('6', ' six ')\n",
    "        review_text = review_text.replace('7', ' seven ')\n",
    "        review_text = review_text.replace('8', ' eight ')\n",
    "        review_text = review_text.replace('9', ' nine ')\n",
    "    \n",
    "    review_text = review_text.lower()\n",
    "    \n",
    "    if stem_words:\n",
    "        words = review_text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        review_text = \" \".join(words)\n",
    "        \n",
    "    # 6. Return a cleaned string\n",
    "    return(review_text)\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    # 1. Clean review, split it into words\n",
    "    words = review_to_clean_review(review, stem_words).split()\n",
    "    #\n",
    "    # 2. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 6. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "    # Function to split a review into parsed sentences. Returns a\n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, \\\n",
    "                                                        remove_stopwords, remove_numbers, stem_words))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labled train review 0 of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:272: UserWarning: \".\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labled train review 5000 of 25000\n",
      "Labled train review 10000 of 25000\n",
      "Labled train review 15000 of 25000\n",
      "Labled train review 20000 of 25000\n",
      "Unlabled train review 0 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 5000 of 50000\n",
      "Unlabled train review 10000 of 50000\n",
      "Unlabled train review 15000 of 50000\n",
      "Unlabled train review 20000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 25000 of 50000\n",
      "Unlabled train review 30000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:272: UserWarning: \"..\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 35000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unlabled train review 40000 of 50000\n",
      "Unlabled train review 45000 of 50000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.py:335: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n"
     ]
    }
   ],
   "source": [
    "# Load the punkt tokenizer\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "# Initialize an empty list of sentences\n",
    "sentences = []\n",
    "# Parsing sentences from training set\n",
    "counter = 0.\n",
    "for review in labeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer, remove_numbers=False, stem_words=True)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Labled train review %d of %d\" % (counter, len(labeled_train[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "counter = 0.\n",
    "# Parsing sentences from unlabeled set\n",
    "for review in unlabeled_train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer, remove_numbers=False, stem_words=True)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Unlabled train review %d of %d\" % (counter, len(unlabeled_train[\"review\"]))\n",
    "    counter = counter + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[list([u'interestingly', u'the', u'film', u'also', u'features', u'two', u'other', u'male', u'characters', u'a', u'shy', u'daffy', u'researcher', u'and', u'an', u'effete', u'snobbish', u'actor', u'whose', u'sexuality', u'could', u'also', u'be', u'questioned'])\n",
      " list([u'having', u'watched', u'this', u'for', u'the', u'third', u'time', u'it', u'kills', u'me', u'to', u'think', u'what', u'jusy', u'garland', u'went', u'through', u'to', u'amuse', u'and', u'entertain', u'us'])\n",
      " list([u'another', u'thankful', u'gesture', u'to', u'his', u'loyal', u'fans', u'in', u'this', u'film', u'is', u'seagal', u'actually', u'does', u'his', u'own', u'fighting', u'and', u'while', u'the', u'fight', u'scenes', u'are', u'less', u'impressive', u'at', u'least', u'it', u's', u'him', u'doing', u'it'])\n",
      " list([u'i', u'didn', u't', u'read', u'any', u'reviews', u'and', u'tried', u'to', u'watch', u'this', u'unintentionally', u'humorous', u'film'])\n",
      " list([u'if', u'there', u'is', u'nothing', u'on', u'television', u'and', u'you', u'have', u'nothing', u'better', u'to', u'do', u'than', u'you', u'won', u't', u'be', u'sorry'])\n",
      " list([u'the', u'deep', u'six', u'made', u'a', u'big', u'impression', u'on', u'me', u'as', u'a', u'kid'])\n",
      " list([u'they', u'attack', u'a', u'compound', u'knocking', u'out', u'the', u'watch', u'tower', u'guards', u'using', u'a', u'highly', u'dubious', u'crossbow', u'despite', u'having', u'silenced', u'firearms'])\n",
      " list([u'does', u'he', u'get', u'the', u'girl'])\n",
      " list([u'that', u'is', u'just', u'because', u'i', u'am', u'insecure'])\n",
      " list([u'it', u'just', u'floats', u'along'])]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(sentences, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 06:08:20,360 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-01-06 06:08:20,365 : INFO : collecting all words and their counts\n",
      "2019-01-06 06:08:20,367 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the Word2Vec model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 06:08:20,438 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2019-01-06 06:08:20,506 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2019-01-06 06:08:20,570 : INFO : PROGRESS: at sentence #30000, processed 671315 words, keeping 30034 word types\n",
      "2019-01-06 06:08:20,636 : INFO : PROGRESS: at sentence #40000, processed 897815 words, keeping 34348 word types\n",
      "2019-01-06 06:08:20,704 : INFO : PROGRESS: at sentence #50000, processed 1116963 words, keeping 37761 word types\n",
      "2019-01-06 06:08:20,777 : INFO : PROGRESS: at sentence #60000, processed 1338404 words, keeping 40723 word types\n",
      "2019-01-06 06:08:20,847 : INFO : PROGRESS: at sentence #70000, processed 1561580 words, keeping 43333 word types\n",
      "2019-01-06 06:08:20,916 : INFO : PROGRESS: at sentence #80000, processed 1780887 words, keeping 45714 word types\n",
      "2019-01-06 06:08:20,986 : INFO : PROGRESS: at sentence #90000, processed 2004996 words, keeping 48135 word types\n",
      "2019-01-06 06:08:21,058 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2019-01-06 06:08:21,128 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2019-01-06 06:08:21,195 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2019-01-06 06:08:21,263 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2019-01-06 06:08:21,330 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2019-01-06 06:08:21,399 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2019-01-06 06:08:21,469 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2019-01-06 06:08:21,537 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2019-01-06 06:08:21,607 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2019-01-06 06:08:21,680 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2019-01-06 06:08:21,752 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2019-01-06 06:08:21,825 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2019-01-06 06:08:21,897 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2019-01-06 06:08:21,969 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2019-01-06 06:08:22,040 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2019-01-06 06:08:22,107 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2019-01-06 06:08:22,178 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2019-01-06 06:08:22,248 : INFO : PROGRESS: at sentence #270000, processed 6000435 words, keeping 74767 word types\n",
      "2019-01-06 06:08:22,319 : INFO : PROGRESS: at sentence #280000, processed 6226314 words, keeping 76369 word types\n",
      "2019-01-06 06:08:22,389 : INFO : PROGRESS: at sentence #290000, processed 6449474 words, keeping 77839 word types\n",
      "2019-01-06 06:08:22,462 : INFO : PROGRESS: at sentence #300000, processed 6674077 words, keeping 79171 word types\n",
      "2019-01-06 06:08:22,533 : INFO : PROGRESS: at sentence #310000, processed 6899391 words, keeping 80480 word types\n",
      "2019-01-06 06:08:22,604 : INFO : PROGRESS: at sentence #320000, processed 7124278 words, keeping 81808 word types\n",
      "2019-01-06 06:08:22,674 : INFO : PROGRESS: at sentence #330000, processed 7346021 words, keeping 83030 word types\n",
      "2019-01-06 06:08:22,749 : INFO : PROGRESS: at sentence #340000, processed 7575533 words, keeping 84280 word types\n",
      "2019-01-06 06:08:22,820 : INFO : PROGRESS: at sentence #350000, processed 7798803 words, keeping 85425 word types\n",
      "2019-01-06 06:08:22,888 : INFO : PROGRESS: at sentence #360000, processed 8019427 words, keeping 86596 word types\n",
      "2019-01-06 06:08:22,966 : INFO : PROGRESS: at sentence #370000, processed 8246619 words, keeping 87708 word types\n",
      "2019-01-06 06:08:23,038 : INFO : PROGRESS: at sentence #380000, processed 8471766 words, keeping 88878 word types\n",
      "2019-01-06 06:08:23,115 : INFO : PROGRESS: at sentence #390000, processed 8701497 words, keeping 89907 word types\n",
      "2019-01-06 06:08:23,188 : INFO : PROGRESS: at sentence #400000, processed 8924446 words, keeping 90916 word types\n",
      "2019-01-06 06:08:23,260 : INFO : PROGRESS: at sentence #410000, processed 9145796 words, keeping 91880 word types\n",
      "2019-01-06 06:08:23,334 : INFO : PROGRESS: at sentence #420000, processed 9366876 words, keeping 92912 word types\n",
      "2019-01-06 06:08:23,408 : INFO : PROGRESS: at sentence #430000, processed 9594413 words, keeping 93932 word types\n",
      "2019-01-06 06:08:23,482 : INFO : PROGRESS: at sentence #440000, processed 9821166 words, keeping 94906 word types\n",
      "2019-01-06 06:08:23,557 : INFO : PROGRESS: at sentence #450000, processed 10044928 words, keeping 96036 word types\n",
      "2019-01-06 06:08:23,633 : INFO : PROGRESS: at sentence #460000, processed 10277688 words, keeping 97088 word types\n",
      "2019-01-06 06:08:23,708 : INFO : PROGRESS: at sentence #470000, processed 10505613 words, keeping 97933 word types\n",
      "2019-01-06 06:08:23,782 : INFO : PROGRESS: at sentence #480000, processed 10725997 words, keeping 98862 word types\n",
      "2019-01-06 06:08:23,850 : INFO : PROGRESS: at sentence #490000, processed 10952741 words, keeping 99871 word types\n",
      "2019-01-06 06:08:23,914 : INFO : PROGRESS: at sentence #500000, processed 11174397 words, keeping 100765 word types\n",
      "2019-01-06 06:08:23,979 : INFO : PROGRESS: at sentence #510000, processed 11399672 words, keeping 101699 word types\n",
      "2019-01-06 06:08:24,044 : INFO : PROGRESS: at sentence #520000, processed 11623020 words, keeping 102598 word types\n",
      "2019-01-06 06:08:24,109 : INFO : PROGRESS: at sentence #530000, processed 11847418 words, keeping 103400 word types\n",
      "2019-01-06 06:08:24,175 : INFO : PROGRESS: at sentence #540000, processed 12072033 words, keeping 104265 word types\n",
      "2019-01-06 06:08:24,241 : INFO : PROGRESS: at sentence #550000, processed 12297571 words, keeping 105133 word types\n",
      "2019-01-06 06:08:24,304 : INFO : PROGRESS: at sentence #560000, processed 12518861 words, keeping 105997 word types\n",
      "2019-01-06 06:08:24,368 : INFO : PROGRESS: at sentence #570000, processed 12747916 words, keeping 106787 word types\n",
      "2019-01-06 06:08:24,433 : INFO : PROGRESS: at sentence #580000, processed 12969412 words, keeping 107665 word types\n",
      "2019-01-06 06:08:24,498 : INFO : PROGRESS: at sentence #590000, processed 13194937 words, keeping 108501 word types\n",
      "2019-01-06 06:08:24,563 : INFO : PROGRESS: at sentence #600000, processed 13417135 words, keeping 109218 word types\n",
      "2019-01-06 06:08:24,626 : INFO : PROGRESS: at sentence #610000, processed 13638158 words, keeping 110092 word types\n",
      "2019-01-06 06:08:24,689 : INFO : PROGRESS: at sentence #620000, processed 13864483 words, keeping 110837 word types\n",
      "2019-01-06 06:08:24,750 : INFO : PROGRESS: at sentence #630000, processed 14088769 words, keeping 111610 word types\n",
      "2019-01-06 06:08:24,822 : INFO : PROGRESS: at sentence #640000, processed 14309552 words, keeping 112416 word types\n",
      "2019-01-06 06:08:24,898 : INFO : PROGRESS: at sentence #650000, processed 14535308 words, keeping 113196 word types\n",
      "2019-01-06 06:08:24,970 : INFO : PROGRESS: at sentence #660000, processed 14758098 words, keeping 113945 word types\n",
      "2019-01-06 06:08:25,045 : INFO : PROGRESS: at sentence #670000, processed 14981482 words, keeping 114643 word types\n",
      "2019-01-06 06:08:25,119 : INFO : PROGRESS: at sentence #680000, processed 15206314 words, keeping 115354 word types\n",
      "2019-01-06 06:08:25,191 : INFO : PROGRESS: at sentence #690000, processed 15428507 words, keeping 116131 word types\n",
      "2019-01-06 06:08:25,267 : INFO : PROGRESS: at sentence #700000, processed 15657213 words, keeping 116943 word types\n",
      "2019-01-06 06:08:25,339 : INFO : PROGRESS: at sentence #710000, processed 15880202 words, keeping 117596 word types\n",
      "2019-01-06 06:08:25,413 : INFO : PROGRESS: at sentence #720000, processed 16105489 words, keeping 118221 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 06:08:25,485 : INFO : PROGRESS: at sentence #730000, processed 16331870 words, keeping 118954 word types\n",
      "2019-01-06 06:08:25,556 : INFO : PROGRESS: at sentence #740000, processed 16552903 words, keeping 119668 word types\n",
      "2019-01-06 06:08:25,627 : INFO : PROGRESS: at sentence #750000, processed 16771230 words, keeping 120295 word types\n",
      "2019-01-06 06:08:25,697 : INFO : PROGRESS: at sentence #760000, processed 16990622 words, keeping 120930 word types\n",
      "2019-01-06 06:08:25,772 : INFO : PROGRESS: at sentence #770000, processed 17217759 words, keeping 121703 word types\n",
      "2019-01-06 06:08:25,842 : INFO : PROGRESS: at sentence #780000, processed 17447905 words, keeping 122402 word types\n",
      "2019-01-06 06:08:25,915 : INFO : PROGRESS: at sentence #790000, processed 17674981 words, keeping 123066 word types\n",
      "2019-01-06 06:08:25,958 : INFO : collected 123504 word types from a corpus of 17798082 raw words and 795538 sentences\n",
      "2019-01-06 06:08:25,959 : INFO : Loading a fresh vocabulary\n",
      "2019-01-06 06:08:26,064 : INFO : effective_min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2019-01-06 06:08:26,066 : INFO : effective_min_count=40 leaves 17238940 word corpus (96% of original 17798082, drops 559142)\n",
      "2019-01-06 06:08:26,119 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2019-01-06 06:08:26,128 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-01-06 06:08:26,130 : INFO : downsampling leaves estimated 12749658 word corpus (74.0% of prior 17238940)\n",
      "2019-01-06 06:08:26,169 : INFO : estimated required memory for 16490 words and 300 dimensions: 47821000 bytes\n",
      "2019-01-06 06:08:26,170 : INFO : resetting layer weights\n",
      "2019-01-06 06:08:26,493 : INFO : training model with 4 workers on 16490 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-01-06 06:08:27,501 : INFO : EPOCH 1 - PROGRESS: at 4.92% examples, 626797 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:28,513 : INFO : EPOCH 1 - PROGRESS: at 10.42% examples, 656029 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:29,519 : INFO : EPOCH 1 - PROGRESS: at 15.84% examples, 664982 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:30,521 : INFO : EPOCH 1 - PROGRESS: at 21.24% examples, 668082 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:31,526 : INFO : EPOCH 1 - PROGRESS: at 26.51% examples, 668355 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:32,536 : INFO : EPOCH 1 - PROGRESS: at 31.91% examples, 669023 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:33,542 : INFO : EPOCH 1 - PROGRESS: at 37.22% examples, 669787 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:34,551 : INFO : EPOCH 1 - PROGRESS: at 42.57% examples, 671201 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:35,553 : INFO : EPOCH 1 - PROGRESS: at 47.84% examples, 671235 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:36,560 : INFO : EPOCH 1 - PROGRESS: at 53.16% examples, 671547 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:37,563 : INFO : EPOCH 1 - PROGRESS: at 58.46% examples, 672658 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:38,569 : INFO : EPOCH 1 - PROGRESS: at 63.83% examples, 673502 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:39,578 : INFO : EPOCH 1 - PROGRESS: at 69.19% examples, 674072 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:40,583 : INFO : EPOCH 1 - PROGRESS: at 74.51% examples, 674217 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:41,584 : INFO : EPOCH 1 - PROGRESS: at 79.86% examples, 674484 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:42,590 : INFO : EPOCH 1 - PROGRESS: at 85.23% examples, 674970 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:43,602 : INFO : EPOCH 1 - PROGRESS: at 90.59% examples, 675182 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:44,608 : INFO : EPOCH 1 - PROGRESS: at 96.03% examples, 675589 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:45,348 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 06:08:45,356 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 06:08:45,361 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 06:08:45,372 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 06:08:45,374 : INFO : EPOCH - 1 : training on 17798082 raw words (12750445 effective words) took 18.9s, 675435 effective words/s\n",
      "2019-01-06 06:08:46,392 : INFO : EPOCH 2 - PROGRESS: at 5.25% examples, 666284 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:47,401 : INFO : EPOCH 2 - PROGRESS: at 10.64% examples, 669655 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:48,405 : INFO : EPOCH 2 - PROGRESS: at 16.01% examples, 672138 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:49,408 : INFO : EPOCH 2 - PROGRESS: at 21.40% examples, 673358 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:50,418 : INFO : EPOCH 2 - PROGRESS: at 26.80% examples, 674709 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:51,425 : INFO : EPOCH 2 - PROGRESS: at 32.23% examples, 675648 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:52,431 : INFO : EPOCH 2 - PROGRESS: at 37.45% examples, 673590 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:53,431 : INFO : EPOCH 2 - PROGRESS: at 42.69% examples, 673376 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:54,438 : INFO : EPOCH 2 - PROGRESS: at 48.00% examples, 673545 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:55,440 : INFO : EPOCH 2 - PROGRESS: at 53.27% examples, 673232 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:56,441 : INFO : EPOCH 2 - PROGRESS: at 58.51% examples, 673610 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:57,443 : INFO : EPOCH 2 - PROGRESS: at 63.83% examples, 673969 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:58,446 : INFO : EPOCH 2 - PROGRESS: at 69.09% examples, 673763 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:08:59,449 : INFO : EPOCH 2 - PROGRESS: at 74.40% examples, 674015 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:00,462 : INFO : EPOCH 2 - PROGRESS: at 79.75% examples, 674085 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:01,458 : INFO : EPOCH 2 - PROGRESS: at 85.00% examples, 673853 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:02,463 : INFO : EPOCH 2 - PROGRESS: at 90.24% examples, 673573 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:03,466 : INFO : EPOCH 2 - PROGRESS: at 95.59% examples, 673372 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:04,289 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 06:09:04,300 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 06:09:04,306 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 06:09:04,310 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 06:09:04,312 : INFO : EPOCH - 2 : training on 17798082 raw words (12749189 effective words) took 18.9s, 673529 effective words/s\n",
      "2019-01-06 06:09:05,331 : INFO : EPOCH 3 - PROGRESS: at 5.19% examples, 658414 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:06,335 : INFO : EPOCH 3 - PROGRESS: at 10.59% examples, 667337 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:07,341 : INFO : EPOCH 3 - PROGRESS: at 15.90% examples, 667866 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:08,343 : INFO : EPOCH 3 - PROGRESS: at 21.24% examples, 668396 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:09,357 : INFO : EPOCH 3 - PROGRESS: at 26.63% examples, 670124 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:10,358 : INFO : EPOCH 3 - PROGRESS: at 31.96% examples, 670218 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:11,360 : INFO : EPOCH 3 - PROGRESS: at 37.22% examples, 670241 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:12,361 : INFO : EPOCH 3 - PROGRESS: at 42.41% examples, 669640 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-06 06:09:13,363 : INFO : EPOCH 3 - PROGRESS: at 47.67% examples, 669731 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:14,370 : INFO : EPOCH 3 - PROGRESS: at 53.05% examples, 670990 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:15,387 : INFO : EPOCH 3 - PROGRESS: at 58.35% examples, 671252 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:16,390 : INFO : EPOCH 3 - PROGRESS: at 63.67% examples, 671713 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-01-06 06:09:17,391 : INFO : EPOCH 3 - PROGRESS: at 68.93% examples, 671751 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:18,403 : INFO : EPOCH 3 - PROGRESS: at 74.23% examples, 671736 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:19,408 : INFO : EPOCH 3 - PROGRESS: at 79.57% examples, 672046 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-06 06:09:20,413 : INFO : EPOCH 3 - PROGRESS: at 84.88% examples, 672258 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:21,419 : INFO : EPOCH 3 - PROGRESS: at 90.12% examples, 672009 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:22,422 : INFO : EPOCH 3 - PROGRESS: at 95.53% examples, 672256 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:23,261 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 06:09:23,269 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 06:09:23,280 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 06:09:23,287 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 06:09:23,289 : INFO : EPOCH - 3 : training on 17798082 raw words (12748490 effective words) took 19.0s, 672087 effective words/s\n",
      "2019-01-06 06:09:24,303 : INFO : EPOCH 4 - PROGRESS: at 5.19% examples, 658105 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:25,307 : INFO : EPOCH 4 - PROGRESS: at 10.59% examples, 667527 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:26,310 : INFO : EPOCH 4 - PROGRESS: at 15.90% examples, 668545 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:27,315 : INFO : EPOCH 4 - PROGRESS: at 21.24% examples, 668534 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:28,324 : INFO : EPOCH 4 - PROGRESS: at 26.57% examples, 669433 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:29,326 : INFO : EPOCH 4 - PROGRESS: at 31.91% examples, 669524 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:30,333 : INFO : EPOCH 4 - PROGRESS: at 37.22% examples, 670187 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:31,347 : INFO : EPOCH 4 - PROGRESS: at 42.52% examples, 670333 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:32,358 : INFO : EPOCH 4 - PROGRESS: at 47.79% examples, 669678 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:33,361 : INFO : EPOCH 4 - PROGRESS: at 53.10% examples, 670477 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:34,366 : INFO : EPOCH 4 - PROGRESS: at 58.35% examples, 670898 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:35,366 : INFO : EPOCH 4 - PROGRESS: at 63.55% examples, 670376 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:36,369 : INFO : EPOCH 4 - PROGRESS: at 68.88% examples, 670943 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:37,373 : INFO : EPOCH 4 - PROGRESS: at 74.07% examples, 670361 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:38,380 : INFO : EPOCH 4 - PROGRESS: at 79.34% examples, 670177 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:39,386 : INFO : EPOCH 4 - PROGRESS: at 84.67% examples, 670487 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:40,397 : INFO : EPOCH 4 - PROGRESS: at 89.97% examples, 670590 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:41,402 : INFO : EPOCH 4 - PROGRESS: at 95.30% examples, 670510 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:42,271 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 06:09:42,278 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 06:09:42,291 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 06:09:42,296 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 06:09:42,297 : INFO : EPOCH - 4 : training on 17798082 raw words (12749577 effective words) took 19.0s, 670891 effective words/s\n",
      "2019-01-06 06:09:43,312 : INFO : EPOCH 5 - PROGRESS: at 5.19% examples, 661422 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:44,322 : INFO : EPOCH 5 - PROGRESS: at 10.59% examples, 667231 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:45,331 : INFO : EPOCH 5 - PROGRESS: at 15.90% examples, 667176 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:46,335 : INFO : EPOCH 5 - PROGRESS: at 21.29% examples, 669349 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:47,337 : INFO : EPOCH 5 - PROGRESS: at 26.63% examples, 670860 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:48,340 : INFO : EPOCH 5 - PROGRESS: at 32.02% examples, 671822 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:49,343 : INFO : EPOCH 5 - PROGRESS: at 37.28% examples, 671544 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:50,355 : INFO : EPOCH 5 - PROGRESS: at 42.63% examples, 672458 words/s, in_qsize 7, out_qsize 0\n",
      "2019-01-06 06:09:51,360 : INFO : EPOCH 5 - PROGRESS: at 47.95% examples, 672707 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:52,363 : INFO : EPOCH 5 - PROGRESS: at 53.27% examples, 673205 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:53,380 : INFO : EPOCH 5 - PROGRESS: at 58.51% examples, 672680 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:54,387 : INFO : EPOCH 5 - PROGRESS: at 63.83% examples, 672863 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:55,389 : INFO : EPOCH 5 - PROGRESS: at 69.09% examples, 672759 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:56,395 : INFO : EPOCH 5 - PROGRESS: at 74.40% examples, 672955 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:57,400 : INFO : EPOCH 5 - PROGRESS: at 79.69% examples, 672640 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:58,406 : INFO : EPOCH 5 - PROGRESS: at 84.94% examples, 672430 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:09:59,412 : INFO : EPOCH 5 - PROGRESS: at 90.24% examples, 672563 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:10:00,420 : INFO : EPOCH 5 - PROGRESS: at 95.65% examples, 672628 words/s, in_qsize 8, out_qsize 0\n",
      "2019-01-06 06:10:01,231 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-01-06 06:10:01,240 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-01-06 06:10:01,247 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-01-06 06:10:01,254 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-01-06 06:10:01,256 : INFO : EPOCH - 5 : training on 17798082 raw words (12748291 effective words) took 18.9s, 672763 effective words/s\n",
      "2019-01-06 06:10:01,257 : INFO : training on a 88990410 raw words (63745992 effective words) took 94.8s, 672693 effective words/s\n",
      "2019-01-06 06:10:01,259 : INFO : saving Word2Vec object under ../input/word2vec_model_300dim_40minwords_10context_stemmed, separately None\n",
      "2019-01-06 06:10:01,260 : INFO : not storing attribute vectors_norm\n",
      "2019-01-06 06:10:01,263 : INFO : not storing attribute cum_table\n",
      "2019-01-06 06:10:01,470 : INFO : saved ../input/word2vec_model_300dim_40minwords_10context_stemmed\n"
     ]
    }
   ],
   "source": [
    "# Import the built-in logging module and configure it so that Word2Vec\n",
    "# creates nice output messages\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', \\\n",
    "                    level=logging.INFO)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 300  # Word vector dimensionality\n",
    "MIN_WORD_COUNT = 40  # Minimum word count. Kaggle set to 40, to avoid attaching too much importance to individual movie titles.\n",
    "NUM_THREADS = 4  # Number of threads to run in parallel\n",
    "CONTEXT = 10  # Context window size\n",
    "DOWNSAMPLING = 1e-3  # Downsample setting for frequent words\n",
    "WORD2VEC_MODEL_FILE = BASE_DIR + \\\n",
    "    \"word2vec_model_\" + \\\n",
    "    str(EMBEDDING_DIM) + \"dim_\" + \\\n",
    "    str(MIN_WORD_COUNT) + \"minwords_\" + \\\n",
    "    str(CONTEXT) + \"context_\" +\\\n",
    "    \"stemmed\"\n",
    "\n",
    "print \"Training the Word2Vec model...\"\n",
    "model = Word2Vec(sentences, workers=NUM_THREADS, \\\n",
    "                 size=EMBEDDING_DIM, min_count=MIN_WORD_COUNT, \\\n",
    "                 window=CONTEXT, sample=DOWNSAMPLING, seed=1)\n",
    "model.save(WORD2VEC_MODEL_FILE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
