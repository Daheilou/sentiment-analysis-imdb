{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec_model_300dim_40minwords_10context', 'sampleSubmission.csv', 'labeledTrainData.tsv', 'test_submission.csv', 'testData.tsv', 'unlabeledTrainData.tsv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeledTrainData.tsv'\n",
    "UNLABELED_TRAIN_DF = BASE_DIR + 'unlabeledTrainData.tsv'\n",
    "TEST_DF = BASE_DIR + 'testData.tsv'\n",
    "print(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviews, 50000 unlabeled train reviews, and 25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "unlabeled_train = pd.read_csv(UNLABELED_TRAIN_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "test = pd.read_csv(TEST_DF, header = 0, delimiter = '\\t', quoting=3)\n",
    "print \"Read %d labeled train reviews, %d unlabeled train reviews, \" \\\n",
    "          \"and %d test reviews\" % (labeled_train[\"review\"].size, unlabeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage\n",
    "\n",
    "Check if test[\"sentiment\"] is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credits: Kaggle tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "def review_to_clean_review(review, remove_numbers=True, stem_words=False):\n",
    "    # Function to convert a document to a clean document,\n",
    "    # optionally removing numbers.  Returns a string.\n",
    "    #\n",
    "    # 1. Remove HTML using lxml parser, ranked best by bs4\n",
    "    review_text = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    #\n",
    "    # TODO: Clean the text! stemming?\n",
    "    # https://www.kaggle.com/lystdo/lstm-with-word2vec-embeddings#L92\n",
    "    # https://www.kaggle.com/currie32/the-importance-of-cleaning-text\n",
    "    #  \n",
    "    # 2. Remove non-letters and non-numbers\n",
    "    review_text = re.sub(\"[^a-zA-Z0-9]\", \" \", review_text)\n",
    "    #\n",
    "    # 3. Optionally remove numbers\n",
    "    if remove_numbers:\n",
    "        review_text = re.sub(\"[0-9]\", \" \", review_text)\n",
    "    else:\n",
    "        review_text = review_text.replace('0', ' zero ')\n",
    "        review_text = review_text.replace('1', ' one ')\n",
    "        review_text = review_text.replace('2', ' two ')\n",
    "        review_text = review_text.replace('3', ' three ')\n",
    "        review_text = review_text.replace('4', ' four ')\n",
    "        review_text = review_text.replace('5', ' five ')\n",
    "        review_text = review_text.replace('6', ' six ')\n",
    "        review_text = review_text.replace('7', ' seven ')\n",
    "        review_text = review_text.replace('8', ' eight ')\n",
    "        review_text = review_text.replace('9', ' nine ')\n",
    "    \n",
    "    review_text = review_text.lower()\n",
    "    \n",
    "    if stem_words:\n",
    "        words = review_text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        words = [stemmer.stem(word) for word in words]\n",
    "        text = \" \".join(stemmed_words)\n",
    "        \n",
    "    # 6. Return a cleaned string\n",
    "    return(review_text)\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False, remove_numbers=True, stem_words=False):\n",
    "    # Function to convert a document to a sequence of words,\n",
    "    # optionally removing stop words.  Returns a list of words.\n",
    "    # 1. Clean review, split it into words\n",
    "    words = review_to_clean_review(review, stem_words).split()\n",
    "    #\n",
    "    # 2. Optionally remove stop words (false by default)\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    #\n",
    "    # 6. Return a list of words\n",
    "    return(words)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, remove_stopwords=False, remove_numbers=True):\n",
    "    # Function to split a review into parsed sentences. Returns a\n",
    "    # list of sentences, where each sentence is a list of words\n",
    "    #\n",
    "    # 1. Use the NLTK tokenizer to split the paragraph into sentences\n",
    "    raw_sentences = tokenizer.tokenize(review.decode('utf8').strip())\n",
    "    #\n",
    "    # 2. Loop over each sentence\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        # If a sentence is empty, skip it\n",
    "        if len(raw_sentence) > 0:\n",
    "            # Otherwise, call review_to_wordlist to get a list of words\n",
    "            sentences.append(review_to_wordlist(raw_sentence, \\\n",
    "                                                        remove_stopwords, remove_numbers))\n",
    "    #\n",
    "    # Return the list of sentences (each sentence is a list of words,\n",
    "    # so this returns a list of lists\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the word2vec model vocabulary: 16490\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 300  # Word vector dimensionality\n",
    "MIN_WORD_COUNT = 40  # Minimum word count. Kaggle set to 40, to avoid attaching too much importance to individual movie titles.\n",
    "NUM_THREADS = 4  # Number of threads to run in parallel\n",
    "CONTEXT = 10  # Context window size\n",
    "DOWNSAMPLING = 1e-3  # Downsample setting for frequent words\n",
    "WORD2VEC_MODEL_FILE = BASE_DIR + \\\n",
    "    \"word2vec_model_\" + \\\n",
    "    str(EMBEDDING_DIM) + \"dim_\" + \\\n",
    "    str(MIN_WORD_COUNT) + \"minwords_\" + \\\n",
    "    str(CONTEXT) + \"context\"\n",
    "\n",
    "word2vec_model = Word2Vec.load(WORD2VEC_MODEL_FILE)\n",
    "\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "word2vec_model.init_sims(replace=True)\n",
    "\n",
    "print(\"Number of words in the word2vec model vocabulary: %d\" % len(word2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train review 0 of 25000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-b9af2a4a0554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mreview\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabeled_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mclean_review\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreview_to_clean_review\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremove_numbers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mtrain_clean_reviews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclean_review\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcounter\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5000.\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4c3a9b0902d8>\u001b[0m in \u001b[0;36mreview_to_clean_review\u001b[0;34m(review, remove_numbers, stem_words)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# 1. Remove HTML using lxml parser, ranked best by bs4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mreview_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreview\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"lxml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;31m#\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# TODO: Clean the text! stemming?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/.local/lib/python2.7/site-packages/bs4/__init__.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m          \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontains_replacement_characters\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m              self.builder.prepare_markup(\n\u001b[0;32m--> 278\u001b[0;31m                  markup, from_encoding, exclude_encodings=exclude_encodings)):\n\u001b[0m\u001b[1;32m    279\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/.local/lib/python2.7/site-packages/bs4/builder/_lxml.pyc\u001b[0m in \u001b[0;36mprepare_markup\u001b[0;34m(self, markup, user_specified_encoding, exclude_encodings, document_declared_encoding)\u001b[0m\n\u001b[1;32m    147\u001b[0m         detector = EncodingDetector(\n\u001b[1;32m    148\u001b[0m             markup, try_encodings, is_html, exclude_encodings)\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mencoding\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencodings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocument_declared_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/.local/lib/python2.7/site-packages/bs4/dammit.pyc\u001b[0m in \u001b[0;36mencodings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    261\u001b[0m         \u001b[0;31m# encoding.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchardet_dammit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmarkup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_usable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtried\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchardet_encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/oscar/.local/lib/python2.7/site-packages/bs4/dammit.pyc\u001b[0m in \u001b[0;36mchardet_dammit\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mchardet_dammit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mchardet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m         \u001b[0;31m#import chardet.constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m#chardet.constants._debug = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/chardet/__init__.pyc\u001b[0m in \u001b[0;36mdetect\u001b[0;34m(byte_str)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mbyte_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbytearray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mdetector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUniversalDetector\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m     \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdetector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/chardet/universaldetector.pyc\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLatin1Prober\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mprober\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_charset_probers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mProbingState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFOUND_IT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m                     self.result = {'encoding': prober.charset_name,\n\u001b[1;32m    213\u001b[0m                                    \u001b[0;34m'confidence'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_confidence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/chardet/charsetgroupprober.pyc\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprober\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/chardet/mbcharsetprober.pyc\u001b[0m in \u001b[0;36mfeed\u001b[0;34m(self, byte_str)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mcoding_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoding_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbyte_str\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcoding_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mMachineState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mERROR\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_clean_reviews = []\n",
    "# Getting clean reviews from training set\n",
    "counter = 0.\n",
    "for review in labeled_train[\"review\"]:\n",
    "    clean_review = review_to_clean_review(review, remove_numbers=False)\n",
    "    train_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Train review %d of %d\" % (counter, len(labeled_train[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "# train_clean_reviews = map(\n",
    "#     lambda review: review_to_clean_review(review, remove_numbers=False),\n",
    "#     labeled_train[\"review\"])\n",
    "\n",
    "test_clean_reviews = []\n",
    "# Getting clean review from testing set\n",
    "counter = 0.\n",
    "for review in test[\"review\"]:\n",
    "    clean_review = review_to_clean_review(review, remove_numbers=False)\n",
    "    test_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Test review %d of %d\" % (counter, len(test[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "# test_clean_reviews = map(\n",
    "#     lambda review: review_to_clean_review(review, remove_numbers=False),\n",
    "#     test[\"review\"])\n",
    "\n",
    "all_clean_reviews = train_clean_reviews + test_clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_clean_reviews[0:2])\n",
    "# print(test_clean_reviews[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We vectorize the text corpus by turning each text into a sequence of integers\n",
    "# Each integer is the index of a token in the dictionary\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NUM_WORDS_FOR_KERAS_TOKENIZER = 200000\n",
    "#\n",
    "# num_words: the maximum number of words to keep, based on frequency.\n",
    "keras_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS_FOR_KERAS_TOKENIZER)\n",
    "#\n",
    "# fit_on_texts accepts a list of strings, a generator of strings or \n",
    "# a list of list of strings. In the last case, it assumes each entry of the lists to be a token.\n",
    "# Here we provide a list of strings.\n",
    "keras_tokenizer.fit_on_texts(all_clean_reviews)\n",
    "word_index = keras_tokenizer.word_index\n",
    "print('Keras Tokenizer found %s unique tokens' % len(word_index))\n",
    "#\n",
    "# texts_to_sequences transforms each text in texts to a sequence of integers.\n",
    "train_sequences = keras_tokenizer.texts_to_sequences(train_clean_reviews)\n",
    "test_sequences = keras_tokenizer.texts_to_sequences(test_clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.choice(word_index.keys(), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pad all text sequences to the same length.\n",
    "# By default zeros are padded at the front.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set max length for each review sequence.\n",
    "MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN = 500\n",
    "\n",
    "train_pad_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)\n",
    "test_pad_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare word embedding matrix\n",
    "\n",
    "# Choose the smaller number of the two as column length of the matrix\n",
    "num_words = min(MAX_NUM_WORDS_FOR_KERAS_TOKENIZER, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "words_not_found = []\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv.vocab:\n",
    "        embedding_matrix[i] = word2vec_model.wv.get_vector(word)\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "# Null word embeddings are words that don't exist in the embedding matrix\n",
    "# and are therefore represented as zero vectors.\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.random.choice(words_not_found, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split train_sequences into train and validation. Ratio: 80/20\n",
    "VALIDATION_SPLIT = 0.2\n",
    "np.random.seed(1234)\n",
    "\n",
    "# \n",
    "perm = np.random.permutation(len(train_sequences))\n",
    "index_train = perm[:int(len(train_sequences)*(1-VALIDATION_SPLIT))]\n",
    "index_val = perm[int(len(train_sequences)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "x_train = train_pad_sequences[index_train]\n",
    "x_val = train_pad_sequences[index_val]\n",
    "y_train = labeled_train[\"sentiment\"][index_train].tolist()\n",
    "y_val = labeled_train[\"sentiment\"][index_val].tolist()\n",
    "\n",
    "print('Randomly split %d pad sequences for training, %d for validation' % (len(x_train) ,len(x_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_pad_sequences\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Conv1D, MaxPooling1D ,GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer1_cnn(dropout=0.2, num_filters=64, kernel_size=2):\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer3_cnn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    NUM_FILTERS = 64\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_cnn_model = layer1_cnn()\n",
    "layer1_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_cnn_model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_cnn_model = layer3_cnn()\n",
    "layer3_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_cnn_model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_cnn_model_dropout5 = layer1_cnn(dropout=0.5)\n",
    "layer1_cnn_model_dropout5.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_layer1_cnn = layer1_cnn_model.predict(x_test)\n",
    "y_test_pred_layer1_cnn_dropout5 = layer1_cnn_model_dropout5.predict(x_test)\n",
    "y_test_pred_layer3_cnn = layer3_cnn_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(predicts):\n",
    "    return map(lambda predict: 1 if predict > 0.5 else 0, predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_layer1_cnn = to_binary(y_test_pred_layer1_cnn)\n",
    "y_test_pred_layer1_cnn_dropout5 = to_binary(y_test_pred_layer1_cnn_dropout5)\n",
    "y_test_pred_layer3_cnn = to_binary(y_test_pred_layer3_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_auc_score(model_name, y_test_pred):\n",
    "    print(\"The AUC score for %s is : %.4f.\" % (model_name, roc_auc_score(y_test, y_test_pred)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_auc_score(\"layer1 cnn\", y_test_pred_layer1_cnn)\n",
    "print_auc_score(\"layer1 cnn dropout 0.5\", y_test_pred_layer1_cnn_dropout5)\n",
    "print_auc_score(\"layer3 cnn\", y_test_pred_layer3_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_cnn_32f_5d = layer1_cnn(dropout=0.5, num_filters=128)\n",
    "layer1_cnn_32f_5d.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer1_cnn_d(dropout=0.2, num_filters=64, kernel_size=2):\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "#     model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_cnn_d_0 = layer1_cnn_d()\n",
    "layer1_cnn_d_0.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_layer1_cnn_d_0 = layer1_cnn_d_0.predict(x_test)\n",
    "y_test_pred_layer1_cnn_d_0 = to_binary(y_test_pred_layer1_cnn_d_0)\n",
    "print_auc_score(\"layer1 cnn_d\", y_test_pred_layer1_cnn_d_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1_cnn_d_5d = layer1_cnn_d(dropout=0.5)\n",
    "layer1_cnn_d_5d.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_layer1_cnn_d_5d = layer1_cnn_d_5d.predict(x_test)\n",
    "y_test_pred_layer1_cnn_d_5d = to_binary(y_test_pred_layer1_cnn_d_5d)\n",
    "print_auc_score(\"layer1 cnn_d\", y_test_pred_layer1_cnn_d_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer3_cnn_1p(dropout=0.2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    NUM_FILTERS = 64\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer3_cnn_1p_0 = layer3_cnn_1p(dropout=0.5)\n",
    "layer3_cnn_1p_0.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
