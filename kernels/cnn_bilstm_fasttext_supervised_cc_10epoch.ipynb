{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeled_train_clean_reviews.csv'\n",
    "TEST_DF = BASE_DIR + 'test_clean_reviews.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read 25000 labeled train reviewsand 25000 test reviews\n"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0)\n",
    "test = pd.read_csv(TEST_DF, header = 0)\n",
    "labeled_train[\"review\"] = labeled_train[\"review\"].astype(str)\n",
    "test[\"review\"] = test[\"review\"].astype(str)\n",
    "print \"Read %d labeled train reviews\" \\\n",
    "          \"and %d test reviews\" % (labeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import fastText\n",
    "EMBEDDING_DIM = 300  # Word vector dimensionality\n",
    "fasttext_model = fastText.load_model(\"../input/fasttext_300features_40minwords_10context_pretrained_cc.bin\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_clean_reviews = labeled_train[\"review\"].tolist()\n",
    "test_clean_reviews = test[\"review\"].tolist()\n",
    "\n",
    "all_clean_reviews = train_clean_reviews + test_clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['everything was better in past days  even children s television  and fraggle rock proves my point quite easily  at the time of writing this comment i am fourteen years old but even in my teen years i can t resist the charm of fraggle rock  for those of you that have indeed been living under a rock  haha    fraggle rock is about a horde of playful and goofy creatures called fraggles who live amazingly in a rock  but they re not the only creatures  the rock is inhabited with many other species like the hardworking doozers and countless living plants  outside the rock on one side live inventor scientist doc and his dog sprocket  who later befriends gobo fraggle   on the other side a family of gorgs supposed rulers of the universe  the five main fraggles gobo  fearless leader   mokey  arty and peaceful   wembley  indecisive and a friend to gobo   boober  a pessimistic domestic god  and red  loves anything to do with sport and general feistyness get caught up in some strange situations each episode while at the same time sing and dance their cares away fraggle rock is definitely a family show the plots may have intricate details that infants may not follow well  but the song and dance routines will hold their attention  the characters are strong and likable  their conflicts believable and their adventures thrilling  the gorgs are frightening  doc and sprocket enlightening  uncle travelling matt hilarious  the postcard segments are very  eight  zero s   and the final episode  change of address  genuinely touching  let s go down to fraggle rock again']\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(all_clean_reviews, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Tokenizer found 101376 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# We vectorize the text corpus by turning each text into a sequence of integers\n",
    "# Each integer is the index of a token in the dictionary\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NUM_WORDS_FOR_KERAS_TOKENIZER = 200000\n",
    "#\n",
    "# num_words: the maximum number of words to keep, based on frequency.\n",
    "keras_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS_FOR_KERAS_TOKENIZER)\n",
    "#\n",
    "# fit_on_texts accepts a list of strings, a generator of strings or \n",
    "# a list of list of strings. In the last case, it assumes each entry of the lists to be a token.\n",
    "# Here we provide a list of strings.\n",
    "keras_tokenizer.fit_on_texts(all_clean_reviews)\n",
    "word_index = keras_tokenizer.word_index\n",
    "print('Keras Tokenizer found %s unique tokens' % len(word_index))\n",
    "#\n",
    "# texts_to_sequences transforms each text in texts to a sequence of integers.\n",
    "train_sequences = keras_tokenizer.texts_to_sequences(train_clean_reviews)\n",
    "test_sequences = keras_tokenizer.texts_to_sequences(test_clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pad all text sequences to the same length.\n",
    "# By default zeros are padded at the front.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set max length for each review sequence.\n",
    "MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN = 500\n",
    "\n",
    "train_pad_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)\n",
    "test_pad_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 1\n"
     ]
    }
   ],
   "source": [
    "# Prepare word embedding matrix\n",
    "\n",
    "# Choose the smaller number of the two as column length of the matrix\n",
    "num_words = min(MAX_NUM_WORDS_FOR_KERAS_TOKENIZER, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_matrix[i] = fasttext_model.get_word_vector(word)\n",
    "# Null word embeddings are words that don't exist in the embedding matrix\n",
    "# and are therefore represented as zero vectors.\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n",
    "# Null word embeddings is one because index 0 does not match any tokens. keras tokenizer uses 1-based index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly split 20000 pad sequences for training, 5000 for validation\n"
     ]
    }
   ],
   "source": [
    "# Split train_sequences into train and validation. Ratio: 80/20\n",
    "VALIDATION_SPLIT = 0.2\n",
    "np.random.seed(1234)\n",
    "\n",
    "# \n",
    "perm = np.random.permutation(len(train_sequences))\n",
    "index_train = perm[:int(len(train_sequences)*(1-VALIDATION_SPLIT))]\n",
    "index_val = perm[int(len(train_sequences)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "x_train = train_pad_sequences[index_train]\n",
    "x_val = train_pad_sequences[index_val]\n",
    "y_train = labeled_train[\"sentiment\"][index_train].tolist()\n",
    "y_val = labeled_train[\"sentiment\"][index_val].tolist()\n",
    "\n",
    "print('Randomly split %d pad sequences for training, %d for validation' % (len(x_train) ,len(x_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_pad_sequences\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Bidirectional, Conv1D, MaxPooling1D ,GlobalMaxPooling1D\n",
    "from keras.initializers import Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_rnn_model(dropout_cnn=0.5, dropout_rnn=0.2, num_filters=64, kernel_size=2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    LSTM_UNITS = 32\n",
    "    LSTM_DROPOUT = 0.\n",
    "    LSTM_RECCURENT_DROPOUT = 0.\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    # Cannot use GlobalMaxPooling since you're feeding it into RNN\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout_cnn))\n",
    "    model.add(Bidirectional(LSTM(LSTM_UNITS, \n",
    "                                 dropout=LSTM_DROPOUT, \n",
    "                                 recurrent_dropout=LSTM_RECCURENT_DROPOUT\n",
    "                                )))\n",
    "    model.add(Dropout(dropout_rnn))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = cnn_rnn_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 300)          30413100  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 499, 64)           38464     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 249, 32)           2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 249, 32)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 64)                16640     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 30,470,349\n",
      "Trainable params: 57,249\n",
      "Non-trainable params: 30,413,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 295s 15ms/step - loss: 0.5847 - acc: 0.6905 - val_loss: 0.4240 - val_acc: 0.8236\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 291s 15ms/step - loss: 0.4008 - acc: 0.8317 - val_loss: 0.3486 - val_acc: 0.8536\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 291s 15ms/step - loss: 0.3354 - acc: 0.8624 - val_loss: 0.3570 - val_acc: 0.8464\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 292s 15ms/step - loss: 0.3047 - acc: 0.8762 - val_loss: 0.2981 - val_acc: 0.8798\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 290s 14ms/step - loss: 0.2870 - acc: 0.8853 - val_loss: 0.2795 - val_acc: 0.8844\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 291s 15ms/step - loss: 0.2886 - acc: 0.8816 - val_loss: 0.3373 - val_acc: 0.8564\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 292s 15ms/step - loss: 0.2808 - acc: 0.8877 - val_loss: 0.3147 - val_acc: 0.8702\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 292s 15ms/step - loss: 0.2679 - acc: 0.8932 - val_loss: 0.2656 - val_acc: 0.8930\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 290s 15ms/step - loss: 0.2615 - acc: 0.8979 - val_loss: 0.3408 - val_acc: 0.8614\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 290s 15ms/step - loss: 0.2562 - acc: 0.8980 - val_loss: 0.2565 - val_acc: 0.8994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa94c3d0f90>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch_size: number of samples per gradient update\n",
    "model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_binary = map(lambda predict: 1 if predict > 0.5 else 0, y_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for CNN-BiLSTM model is : 0.90176.\n"
     ]
    }
   ],
   "source": [
    "print(\"The AUC score for CNN-BiLSTM model is : %.5f.\" %roc_auc_score(y_test, y_test_pred_binary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote to cnn_bilstm_fasttext_supervised_cc.csv\n"
     ]
    }
   ],
   "source": [
    "# Write the test results\n",
    "output = pd.DataFrame(data={\"id\": test[\"id\"], \"sentiment\": y_test_pred_binary})\n",
    "output.to_csv(os.path.join('../', 'output', \"cnn_bilstm_fasttext_supervised_cc.csv\"), index=False, quoting=3)\n",
    "print \"Wrote to cnn_bilstm_fasttext_supervised_cc.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
