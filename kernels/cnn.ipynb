{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['word2vec_model_300dim_40minwords_10context', 'sampleSubmission.csv', 'labeled_train_clean_reviews.csv', 'word2vec_model_300dim_40minwords_10context_stemmed', 'labeledTrainData.tsv', 'sentences_for_word2vec.csv', 'test_submission.csv', 'testData.tsv', 'unlabeledTrainData.tsv', 'test_clean_reviews.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "import os, re\n",
    "import nltk\n",
    "BASE_DIR = '../input/'\n",
    "LABELED_TRAIN_DF = BASE_DIR + 'labeled_train_clean_reviews_stemmed.csv'\n",
    "TEST_DF = BASE_DIR + 'test_clean_reviews_stemmed.tsv'\n",
    "print(os.listdir(BASE_DIR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File ../input/labeled_train_clean_reviews_stemmed.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-738513ae8488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlabeled_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLABELED_TRAIN_DF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTEST_DF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0;34m\"Read %d labeled train reviews\"\u001b[0m           \u001b[0;34m\"and %d test reviews\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlabeled_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"review\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, doublequote, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    676\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    438\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    785\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 787\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    788\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    789\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1012\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1706\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1708\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1709\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1710\u001b[0m         \u001b[0mpassed_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnames\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File ../input/labeled_train_clean_reviews_stemmed.csv does not exist"
     ]
    }
   ],
   "source": [
    "labeled_train = pd.read_csv(LABELED_TRAIN_DF, header = 0)\n",
    "test = pd.read_csv(TEST_DF, header = 0)\n",
    "print \"Read %d labeled train reviews\" \\\n",
    "          \"and %d test reviews\" % (labeled_train[\"review\"].size, test[\"review\"].size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data leakage\n",
    "\n",
    "Check if test[\"sentiment\"] is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test[\"sentiment\"] = test[\"id\"].map(lambda x: 1 if int(x.strip('\"').split(\"_\")[1]) >= 5 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the word2vec model vocabulary: 16490\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "EMBEDDING_DIM = 300  # Word vector dimensionality\n",
    "MIN_WORD_COUNT = 40  # Minimum word count. Kaggle set to 40, to avoid attaching too much importance to individual movie titles.\n",
    "NUM_THREADS = 4  # Number of threads to run in parallel\n",
    "CONTEXT = 10  # Context window size\n",
    "DOWNSAMPLING = 1e-3  # Downsample setting for frequent words\n",
    "WORD2VEC_MODEL_FILE = BASE_DIR + \\\n",
    "    \"word2vec_model_\" + \\\n",
    "    str(EMBEDDING_DIM) + \"dim_\" + \\\n",
    "    str(MIN_WORD_COUNT) + \"minwords_\" + \\\n",
    "    str(CONTEXT) + \"context\"\n",
    "\n",
    "word2vec_model = Word2Vec.load(WORD2VEC_MODEL_FILE)\n",
    "\n",
    "# If you don't plan to train the model any further, calling\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "word2vec_model.init_sims(replace=True)\n",
    "\n",
    "print(\"Number of words in the word2vec model vocabulary: %d\" % len(word2vec_model.wv.vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train review 0 of 25000\n",
      "Train review 5000 of 25000\n",
      "Train review 10000 of 25000\n",
      "Train review 15000 of 25000\n",
      "Train review 20000 of 25000\n",
      "Test review 0 of 25000\n",
      "Test review 5000 of 25000\n",
      "Test review 10000 of 25000\n",
      "Test review 15000 of 25000\n",
      "Test review 20000 of 25000\n"
     ]
    }
   ],
   "source": [
    "train_clean_reviews = []\n",
    "# Getting clean reviews from training set\n",
    "counter = 0.\n",
    "for review in labeled_train[\"review\"]:\n",
    "    clean_review = review_to_clean_review(review, remove_numbers=False)\n",
    "    train_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Train review %d of %d\" % (counter, len(labeled_train[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "# train_clean_reviews = map(\n",
    "#     lambda review: review_to_clean_review(review, remove_numbers=False),\n",
    "#     labeled_train[\"review\"])\n",
    "\n",
    "test_clean_reviews = []\n",
    "# Getting clean review from testing set\n",
    "counter = 0.\n",
    "for review in test[\"review\"]:\n",
    "    clean_review = review_to_clean_review(review, remove_numbers=False)\n",
    "    test_clean_reviews.append(clean_review)\n",
    "    if counter % 5000. == 0.:\n",
    "        print \"Test review %d of %d\" % (counter, len(test[\"review\"]))\n",
    "    counter = counter + 1.\n",
    "\n",
    "# test_clean_reviews = map(\n",
    "#     lambda review: review_to_clean_review(review, remove_numbers=False),\n",
    "#     test[\"review\"])\n",
    "\n",
    "all_clean_reviews = train_clean_reviews + test_clean_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(train_clean_reviews[0:2])\n",
    "# print(test_clean_reviews[0:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keras Tokenizer found 101376 unique tokens\n"
     ]
    }
   ],
   "source": [
    "# We vectorize the text corpus by turning each text into a sequence of integers\n",
    "# Each integer is the index of a token in the dictionary\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "MAX_NUM_WORDS_FOR_KERAS_TOKENIZER = 200000\n",
    "#\n",
    "# num_words: the maximum number of words to keep, based on frequency.\n",
    "keras_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS_FOR_KERAS_TOKENIZER)\n",
    "#\n",
    "# fit_on_texts accepts a list of strings, a generator of strings or \n",
    "# a list of list of strings. In the last case, it assumes each entry of the lists to be a token.\n",
    "# Here we provide a list of strings.\n",
    "keras_tokenizer.fit_on_texts(all_clean_reviews)\n",
    "word_index = keras_tokenizer.word_index\n",
    "print('Keras Tokenizer found %s unique tokens' % len(word_index))\n",
    "#\n",
    "# texts_to_sequences transforms each text in texts to a sequence of integers.\n",
    "train_sequences = keras_tokenizer.texts_to_sequences(train_clean_reviews)\n",
    "test_sequences = keras_tokenizer.texts_to_sequences(test_clean_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'mezzanine' u'douche' u'nimbar' u'washroom' u'stoke' u'cryptic' u'ocar'\n",
      " u'kywildflower' u'revisionists' u'vasty' u'gaughan' u'unbowed' u'sonya'\n",
      " u'philharmagic' u'troublesome' u'telescoping' u'mooching' u'terrorizers'\n",
      " u'bubbling' u'naysayers' u'encyclopedic' u'criticizes' u'maraglia'\n",
      " u'arkin' u'pilloried' u'intimacy' u'bookcase' u'shadix' u'unsurvivable'\n",
      " u'skunked' u'topiary' u'catapulted' u'falsified' u'mrudul'\n",
      " u'contaminants' u'braggart' u'castrating' u'unveiled' u'watchowski'\n",
      " u'maude' u'freeform' u'subconsciously' u'brugge' u'monette' u'constained'\n",
      " u'mitsugoro' u'labouredly' u'homestretch' u'hierarchy' u'commandeering'\n",
      " u'jelling' u'materializer' u'lards' u'stupefy' u'mainframes' u'gazes'\n",
      " u'fok' u'olympians' u'stepan' u'sleestak' u'overdramatized' u'gawkers'\n",
      " u'bonding' u'arthurs' u'equalled' u'galatea' u'cookies' u'sobbing'\n",
      " u'cocco' u'anthropomorhic' u'purticularly' u'c' u'zappati' u'unlicensed'\n",
      " u'francophiles' u'jenniferbeals' u'reevaluate' u'bustiers' u'witful'\n",
      " u'hamlets' u'becca' u'erections' u'dimitrova' u'windup' u'mansions'\n",
      " u'pemberley' u'plainspoken' u'playgirl' u'stellas' u'merengie'\n",
      " u'cronenbergs' u'medicals' u'interviewee' u'somethingoranother' u'year'\n",
      " u'davoli' u'wacko' u'part' u'adon' u'lodged' u'urbervilles' u'goggles'\n",
      " u'bombshell' u'flailed' u'pollyanna' u'realest' u'barberini' u'subgroup'\n",
      " u'sturm' u'french' u'ridged' u'porcupine' u'overdramatic' u'woddy'\n",
      " u'stds' u'lathrop' u'fluttery' u'watts' u'refilled' u'landscaper'\n",
      " u'longest' u'miffed' u'relationsip' u'balloonatic' u'sipus' u'johan'\n",
      " u'lassick' u'lizzie' u'biloxi' u'emptiveness' u'netting' u'soviets'\n",
      " u'allllllllllllllllllllllllllllll' u'decentralized' u'entrail'\n",
      " u'vampyres' u'forgave' u'claustophobic' u'jivetalking' u'breathless'\n",
      " u'wild' u'theakston' u'achance' u'striken' u'zebra' u'convictions' u'hrr'\n",
      " u'catherines' u'repute' u'advision' u'selleca' u'boojum' u'minimised'\n",
      " u'gaggling' u'cambell' u'lazers' u'garafalo' u'softball' u'tosca'\n",
      " u'mench' u'rescueman' u'yacking' u'harrow' u'clothespin' u'muttons'\n",
      " u'headly' u'grow' u'quatre' u'bringer' u'broth' u'licked' u'sportswriter'\n",
      " u'rhum' u'perving' u'costas' u'previn' u'honest' u'clifford' u'gorily'\n",
      " u'spiderweb' u'barmaid' u'trues' u'issue' u'ginga' u'reprisals' u'ocar'\n",
      " u'bestia' u'mocumentaries' u'karzis' u'jurgens' u'taffy' u'nilson'\n",
      " u'lottake' u'jews' u'blairwitch' u'curette' u'jubilee' u'espisode'\n",
      " u'sheathing' u'treize' u'retrospectives' u'lambropoulou' u'twop'\n",
      " u'reflection' u'unforgetable' u'overnite' u'deprecation' u'engletine'\n",
      " u'rebuen' u'hideous' u'messing' u'trickery' u'limaye' u'conservatives'\n",
      " u'ehelhell' u'dazed' u'gillis' u'murvyn' u'viginia' u'zillion'\n",
      " u'entangled' u'castello' u'ziltch' u'pervert' u'korine' u'gunjuro'\n",
      " u'respirator' u'conniving' u'sidestep' u'persecution' u'whop' u'everseen'\n",
      " u'unatmospherically' u'obsessed' u'dorff' u'tushes' u'requirements'\n",
      " u'troama' u'fallen' u'haddonfeild' u'taxpayers' u'speakman' u'intendend'\n",
      " u'berti' u'candle' u'trio' u'mcgowen' u'emerald' u'itinerary' u'palagi'\n",
      " u'summfield' u'pillow' u'malkovitch' u'absoluter' u'sniffle' u'nigerian'\n",
      " u'exacerbated' u'mopsy' u'heifers' u'psm' u'cutbacks' u'kadar'\n",
      " u'lackadaisical' u'hoya' u'vivienne' u'descendant' u'moors'\n",
      " u'fictionalizes' u'munteans' u'bartin' u'guiana' u'almghandi'\n",
      " u'seyfriend' u'tolly' u'casanova' u'beachcombers' u'breweries' u'boiling'\n",
      " u'sure' u'almanzar' u'cletus' u'unfeeling' u'expositionary' u'mazar'\n",
      " u'typifies' u'belieived' u'ofizi' u'agressive' u'kanwaljeet'\n",
      " u'shortfalls' u'uranium' u'cloches' u'nuke' u'bestseller' u'utilized'\n",
      " u'colorful' u'cartwrights' u'filmiing' u'indiefest' u'receiving'\n",
      " u'roleoutside' u'magnific' u'bedknobs' u'vomitting' u'crimp' u'zanuck'\n",
      " u'everygirl' u'blanc' u'jenifer' u'gimmicky' u'luiz' u'thais'\n",
      " u'pamanteni' u'wipers' u'moru' u'jeb' u'immobilize' u'blanding' u'udita'\n",
      " u'schoolgirls' u'thunderous' u'furtherance' u'charlene' u'girolamo'\n",
      " u'pinata' u'hillsides' u'falsified' u'noche' u'seatbelts' u'omigosh'\n",
      " u'megaphone' u'emilius' u'indio' u'guessable' u'thirteen' u'klezmer'\n",
      " u'musseum' u'seaside' u'traffickers' u'nascar' u'rootless' u'edtv'\n",
      " u'paco' u'linehan' u'gentlewomen' u'sten' u'undercard' u'holobands'\n",
      " u'ijajkasif' u'doofuses' u'skitzoid' u'kas' u'thora' u'oppresses'\n",
      " u'divorce' u'islamicist' u'imbalanced' u'lenghth' u'arkan'\n",
      " u'desthpicable' u'helium' u'yipes' u'betraysome' u'manuevers'\n",
      " u'washington' u'cpa' u'joys' u'joggers' u'bardem' u'languid'\n",
      " u'chesterton' u'quigly' u'senta' u'defies' u'crouched' u'kyrano'\n",
      " u'quentin' u'rakhi' u'buitoni' u'orchestrate' u'gustavsson' u'adds'\n",
      " u'jonesesque' u'emotionally' u'novo' u'nigger' u'commendable' u'maganac'\n",
      " u'shrew' u'spencer' u'heaven' u'recalled' u'hindering' u'bible'\n",
      " u'highlites' u'econoline' u'ophelia' u'borgesian' u'platefull'\n",
      " u'urbaniak' u'conchatta' u'crediblity' u'horn' u'powderkeg' u'victrion'\n",
      " u'stank' u'curvy' u'thursday' u'pinkus' u'stratham' u'sightly'\n",
      " u'fatherhood' u'snippets' u'communicable' u'ameliorate' u'buyrate'\n",
      " u'rhymed' u'baskets' u'ctu' u'jergen' u'iv' u'diagnoses' u'yeeeeah'\n",
      " u'struthers' u'aishwariya' u'disrupt' u'frederick' u'nardini'\n",
      " u'helfgotts' u'mccallum' u'severinsen' u'talledega' u'savvy' u'casts'\n",
      " u'waldau' u'grower' u'narrows' u'cordobes' u'ubernerds' u'stagehands'\n",
      " u'cruella' u'matarazzo' u'rt' u'paintbrush' u'chushingura' u'mummifies'\n",
      " u'eisenstein' u'suppose' u'peaking' u'evertime' u'bastard'\n",
      " u'gorgeousness' u'favino' u'neighbours' u'disabled' u'dorf'\n",
      " u'mystification' u'thisnightmare' u'guy' u'tringtignat' u'gunsels'\n",
      " u'rebelion' u'riedelsheimer' u'dispositioned' u'regret' u'gailard'\n",
      " u'kindling' u'enthralled' u'dessert' u'moynes' u'banjo' u'probation'\n",
      " u'nadir' u'beegees' u'kathe' u'lovelife' u'americain' u'uniformed'\n",
      " u'aknowledge' u'ingrate' u'nattily' u'disemboweled' u'zhuravli' u'nalo'\n",
      " u'unquestioning' u'harrowed' u'misjudgement' u'supertexts' u'dantons'\n",
      " u'overdirection' u'pakeezah' u'sergeants' u'wonger' u'attracted' u'ceety'\n",
      " u'spiraling' u'mesquide' u'tatiana' u'tvs' u'kimono' u'clouseau'\n",
      " u'lancie' u'miner' u'skirting' u'droll' u'entertain' u'bulette' u'llamas'\n",
      " u'dyeing' u'ryholite' u'subgroup' u'acupuncturist' u'rtd' u'intereaction'\n",
      " u'territorial' u'nakata' u'paparazzi' u'evergreen' u'pinoccio' u'hash'\n",
      " u'actually' u'smilla' u'commentaryby' u'witt' u'newlywed' u'outerlimits'\n",
      " u'disrepectfully' u'berghe' u'amphitheatre' u'refrences' u'bejo'\n",
      " u'damping' u'amrohi' u'forceable' u'shahid' u'aldofo' u'pinho'\n",
      " u'wealthier' u'routh' u'beeping' u'vonneguty' u'waterfalls' u'crouch'\n",
      " u'quill' u'sleeker' u'andstunning' u'too' u'moviemaker' u'penny'\n",
      " u'lawyerly' u'timecolumbo' u'pheeeuuuh' u'steffan' u'ethnically'\n",
      " u'digitisation' u'slaptick' u'nagasaki' u'cycles' u'mccarthy' u'borrower'\n",
      " u'slob' u'ironicaly' u'annis' u'govt' u'fairchild' u'steamroll'\n",
      " u'devotions' u'champ' u'garantee' u'sappiest' u'afrikaaner' u'lisette'\n",
      " u'addresses' u'teenagery' u'flannelette' u'summering' u'victors'\n",
      " u'prepared' u'shmos' u'gloria' u'riots' u'aweful' u'motivational'\n",
      " u'launchers' u'mirkovich' u'izuruha' u'jar' u'mckay' u'conehead'\n",
      " u'nemsis' u'baxley' u'motor' u'demonico' u'sensors' u'chuggin' u'corrine'\n",
      " u'yay' u'hunchbacks' u'saxophone' u'blain' u'hepton' u'puppetmaster'\n",
      " u'stil' u'saiyan' u'otome' u'honks' u'rib' u'freewill' u'terminated'\n",
      " u'talliban' u'juliana' u'ironside' u'hawaldar' u'cajones' u'reprieves'\n",
      " u'clowned' u'madagasga' u'clericism' u'bated' u'frieze' u'liu'\n",
      " u'pistoleers' u'waterfall' u'sake' u'theremy' u'zuckert' u'gadi'\n",
      " u'sesilia' u'mili' u'insatiably' u'roping' u'isten' u'wired' u'hadass'\n",
      " u'pleantly' u'crale' u'slur' u'realquickly' u'concorde' u'indisputable'\n",
      " u'jiminey' u'towney' u'savini' u'paramore' u'daumier' u'vina' u'disarm'\n",
      " u'nooooooo' u'assurances' u'vegence' u'hassles' u'lessening'\n",
      " u'achievable' u'fostered' u'hypnotist' u'illegitimate' u'hippo'\n",
      " u'humourless' u'striped' u'greist' u'tannin' u'rationally' u'garibaldi'\n",
      " u'batmite' u'booby' u'kowtow' u'merylstreep' u'ppvs' u'takeover'\n",
      " u'convinced' u'pastoral' u'talkier' u'camra' u'tweet' u'antes' u'odor'\n",
      " u'eammon' u'mocks' u'jancie' u'inclusions' u'caleigh' u'lyu'\n",
      " u'philosophizing' u'downfalls' u'daym' u'provo' u'glacially' u'billions'\n",
      " u'watership' u'unfaltering' u'swaggered' u'decipherable' u'ingrates'\n",
      " u'qualitatively' u'hammering' u'acuteness' u'thornier' u'mainstage'\n",
      " u'kerala' u'tutoyer' u'bodice' u'leeches' u'diestl' u'permnanet'\n",
      " u'eccleston' u'looting' u'winded' u'unforunately' u'transmitter'\n",
      " u'beeper' u'tezuka' u'royles' u'templates' u'lue' u'quotations' u'bosch'\n",
      " u'odaka' u'workers' u'wambini' u'spheeris' u'macks' u'perusal' u'chuppa'\n",
      " u'leydoyen' u'chasidic' u'aimless' u'steuerman' u'flica' u'hurry'\n",
      " u'cemetery' u'moshe' u'extravaganzas' u'herders' u'garde' u'wolodarksy'\n",
      " u'allman' u'andromeda' u'orinteresting' u'mezzogiorno' u'tvg' u'zonker'\n",
      " u'solvable' u'signorellistarring' u'recrudescence' u'kennif' u'ingvar'\n",
      " u'elijah' u'cdc' u'contortions' u'distractive' u'cheerleading'\n",
      " u'brochure' u'waddington' u'iceman' u'gentile' u'schiavo' u'unneccessary'\n",
      " u'blown' u'tally' u'deknight' u'freshness' u'chearator' u'shasta'\n",
      " u'mccinsey' u'workmen' u'shifted' u'trebor' u'cenix' u'wamb' u'frolic'\n",
      " u'lozano' u'inexpressive' u'milion' u'whits' u'martyrs' u'qahar'\n",
      " u'basely' u'hickland' u'foreclosing' u'unintentionally' u'architects'\n",
      " u'naaahhh' u'bluetooth' u'fdr' u'paramore' u'overrunning' u'glimmering'\n",
      " u'eb' u'scruno' u'police' u'nondenominational' u'lowly' u'adolescence'\n",
      " u'spf' u'recruited' u'stingray' u'detonador' u'stalemated' u'paralytic'\n",
      " u'defaults' u'justthe' u'standard' u'phd' u'nad' u'astounds' u'datd'\n",
      " u'cheesiness' u'gliss' u'dunneare' u'bunsen' u'backstabber' u'sighed'\n",
      " u'venger' u'intellects' u'misere' u'charisma' u'clony' u'jest'\n",
      " u'pdmoviereview' u'correli' u'anuses' u'stalker' u'savingtheday'\n",
      " u'postponed' u'suras' u'schoenburn' u'watchers' u'steelers' u'bhhaaaad'\n",
      " u'ionically' u'microbudget' u'arrrrgh' u'battleaxe' u'mammoths'\n",
      " u'oederek' u'tolls' u'seaman' u'raph' u'marlowe' u'cornbluth'\n",
      " u'primatologists' u'harriers' u'updyke' u'comedus' u'climactic'\n",
      " u'subtracting' u'lemesurier' u'kage' u'veg' u'overqualified'\n",
      " u'credentials' u'fuelled' u'striken' u'ouimet' u'anyoneactually'\n",
      " u'whitechapel' u'higgens' u'paledouros' u'nilamben' u'opertaion'\n",
      " u'movement' u'broadaxe' u'dhawan' u'phonograph' u'obstetrician'\n",
      " u'orthopedic' u'screener' u'possessiveness' u'monoan' u'titties' u'ethic'\n",
      " u'shakiness' u'examp' u'birthparents' u'whiteys' u'winds' u'pathedic'\n",
      " u'doctor' u'afaik' u'tames' u'lorden' u'toever' u'livvy' u'dinoshark'\n",
      " u'russwill' u'categorize' u'motw' u'swope' u'zaljko' u'pragmistism'\n",
      " u'mouthwash' u'smuggler' u'gabreil' u'law' u'romand' u'electromagnetic'\n",
      " u'deepika' u'tenet' u'provocing' u'carabiners' u'sione' u'yogi'\n",
      " u'unintellectual' u'competitiveness' u'spines' u'indolently' u'isten'\n",
      " u'galindez' u'integers' u'jaihind' u'butches' u'pelucidar' u'bogosian'\n",
      " u'salvages' u'rowan' u'docter' u'suppressant' u'baha' u'rivalry'\n",
      " u'pharma' u'embarrasment' u'baez' u'forgiveable' u'zagros' u'halloway'\n",
      " u'queti' u'reassemble' u'iowan' u'cartload' u'gerry' u'thuds' u'gregoli'\n",
      " u'interurban' u'scin' u'lesser' u'freewheeling' u'lolcons' u'estrange'\n",
      " u'honesty' u'sndtrk' u'lade' u'pimlico' u'surcease' u'discoveries'\n",
      " u'nicols' u'privatizing' u'stigmatizing' u'dexterously' u'zappruder'\n",
      " u'unended' u'masacre' u'churning' u'hairdryers' u'aboutboul' u'nagel'\n",
      " u'crewdson' u'happed' u'soccoro' u'lawanda' u'gaelic' u'commence'\n",
      " u'hiatt' u'albertine' u'wisconsite' u'clossius' u'cubic' u'brimstone'\n",
      " u'booh' u'jaffer' u'springtime' u'viginia' u'uhodim' u'castigated'\n",
      " u'incestous' u'propagandistic' u'misconceptualization' u'heiki'\n",
      " u'comical' u'cerebral' u'sach' u'sculpted' u'sidestepped' u'weatherly'\n",
      " u'breakout' u'thundiiayil' u'priestley' u'shroud' u'retards' u'hiya'\n",
      " u'sethi' u'beggining' u'miou' u'toth' u'thge' u'hope' u'dissociates'\n",
      " u'alienates' u'sapphic' u'iy' u'agreed']\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(word_index.keys(), 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pad all text sequences to the same length.\n",
    "# By default zeros are padded at the front.\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set max length for each review sequence.\n",
    "MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN = 500\n",
    "\n",
    "train_pad_sequences = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)\n",
    "test_pad_sequences = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 84920\n"
     ]
    }
   ],
   "source": [
    "# Prepare word embedding matrix\n",
    "\n",
    "# Choose the smaller number of the two as column length of the matrix\n",
    "num_words = min(MAX_NUM_WORDS_FOR_KERAS_TOKENIZER, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\n",
    "words_not_found = []\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec_model.wv.vocab:\n",
    "        embedding_matrix[i] = word2vec_model.wv.get_vector(word)\n",
    "    else:\n",
    "        words_not_found.append(word)\n",
    "# Null word embeddings are words that don't exist in the embedding matrix\n",
    "# and are therefore represented as zero vectors.\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'cockily' u'ricochet' u'antebellum' u'shepherdesses' u'jaundiced'\n",
      " u'meiyan' u'mackie' u'crackd' u'newmar' u'aviva' u'alternated'\n",
      " u'straightedge' u'delaware' u'krog' u'cocteau' u'gerardo' u'catillon'\n",
      " u'shortest' u'cariie' u'hoovered' u'bannacheck' u'zabalza' u'reined'\n",
      " u'glendale' u'tamarind' u'tipple' u'miamis' u'colluded' u'initself'\n",
      " u'paunchy' u'vel' u'disneyish' u'gerards' u'tarn' u'rosana' u'vanne'\n",
      " u'hibernia' u'hollinghurst' u'understanda' u'lightsabre' u'fantasma'\n",
      " u'shriveled' u'crispon' u'xtianity' u'ruggles' u'gushed' u'actally'\n",
      " u'martijn' u'zoeys' u'allcharacters' u'mua' u'attilla' u'mabuse'\n",
      " u'petitions' u'gazelles' u'antonik' u'concussive' u'opinionated'\n",
      " u'salinger' u'swink' u'xp' u'accumulator' u'mammothly' u'bosomy' u'prs'\n",
      " u'keeanu' u'chakotay' u'comraderie' u'omaha' u'linguine' u'ombra'\n",
      " u'inferiors' u'dwervick' u'disconsolate' u'valientes' u'sahib' u'ganges'\n",
      " u'experimentality' u'sens' u'rambunctious' u'froqu' u'alva'\n",
      " u'steenburgen' u'weaselly' u'overhauling' u'plowman' u'polk' u'duhhhhh'\n",
      " u'doggy' u'senatorial' u'rumpy' u'youngson' u'crossface' u'ooohs'\n",
      " u'burne' u'hurter' u'beets' u'crustaceans' u'berta' u'balconys'\n",
      " u'underscripted' u'mindy' u'particularlly' u'cogitate' u'storied'\n",
      " u'habitually' u'jolliness' u'zizte' u'sweeden' u'ansons' u'naushads'\n",
      " u'wakefulness' u'reclusive' u'cabana' u'eulogized' u'madams' u'parlour'\n",
      " u'ick' u'alternativa' u'meaninglessness' u'oppress' u'rayburn' u'rekka'\n",
      " u'schizophrenics' u'coursing' u'begat' u'refocused' u'geometrically'\n",
      " u'devra' u'governmental' u'senn' u'baskette' u'benches' u'sleekly'\n",
      " u'roost' u'nostalghia' u'asssociated' u'rationally' u'helmond' u'ousmane'\n",
      " u'xplanation' u'valse' u'justto' u'chrstmas' u'facile' u'edda' u'ckman'\n",
      " u'chinnarasu' u'umte' u'streetlights' u'lorded' u'disrupted' u'gaggles'\n",
      " u'kunst' u'allingham' u'kutsher' u'celi' u'couldnt' u'scuzzlebut'\n",
      " u'gentlemens' u'uninstructive' u'fondo' u'pinfold' u'contraceptives'\n",
      " u'paradoxes' u'talton' u'chaffing' u'eeriest' u'shallowest'\n",
      " u'sherlockian' u'invoking' u'defendants' u'wagoneer' u'reunites' u'inbo'\n",
      " u'daff' u'copes' u'totty' u'parallax' u'trubshawe' u'hamnet' u'cowman'\n",
      " u'thriftiness' u'weakened' u'lunkhead' u'fiume' u'bregovic' u'conundrum'\n",
      " u'forbearance' u'pinheadplot' u'nymphos' u'constitutions' u'indecisive'\n",
      " u'madtrapper' u'kinng' u'larmina' u'cartographers' u'falkland' u'cecille'\n",
      " u'veli' u'caboose' u'recoding' u'tangibility' u'coinsidence' u'travelled'\n",
      " u'germogel' u'homecomings' u'distributing' u'wiggling' u'skiles'\n",
      " u'responsabilities' u'pollinated' u'shafts' u'potenta' u'margraet' u'fax'\n",
      " u'bugling' u'dateing' u'shockwaves' u'macedo' u'spitied' u'pip'\n",
      " u'worlock' u'jamrom' u'siemens' u'expressionally' u'performace'\n",
      " u'hahahahhaaa' u'asanine' u'dube' u'withdrawal' u'industrious'\n",
      " u'convenant' u'funkier' u'evaluating' u'ggrobar' u'glaze' u'scots'\n",
      " u'stair' u'versed' u'seale' u'golani' u'hairdewed' u'penney' u'goksal'\n",
      " u'pacify' u'kurz' u'interven' u'miniguns' u'sexscenes' u'impecunious'\n",
      " u'delineation' u'saliva' u'cullinan' u'olympians' u'receeds'\n",
      " u'horrorfilms' u'bumiller' u'rusler' u'latterman' u'oppel'\n",
      " u'massachussetts' u'falsities' u'amaerican' u'madrigal' u'provacative'\n",
      " u'vitagraph' u'lifelike' u'blanca' u'cinemtrophy' u'pixote' u'wyle'\n",
      " u'contorted' u'deprecate' u'keely' u'extermination' u'homosapiens'\n",
      " u'spurned' u'harking' u'julianne' u'recommendeda' u'ortolani' u'infidel'\n",
      " u'reveille' u'malones' u'geniousness' u'adventists' u'excperience'\n",
      " u'wholesale' u'samara' u'empowered' u'gandhiji' u'briliant'\n",
      " u'misperceived' u'downloading' u'commodities' u'roaches' u'untwining'\n",
      " u'introvert' u'thereabouts' u'takahashi' u'folksy' u'basestar'\n",
      " u'anaesthetic' u'reprogramming' u'pluckin' u'superabundant' u'pojar'\n",
      " u'crocky' u'televangelism' u'expenditure' u'reimann' u'grimaced'\n",
      " u'misconception' u'marketplace' u'enrol' u'chalte' u'fariy' u'horserace'\n",
      " u'zephram' u'soulplane' u'adicts' u'belitski' u'cooling' u'rigoberta'\n",
      " u'suschitzky' u'shutdown' u'defininitive' u'kemono' u'riven'\n",
      " u'supernovas' u'smartens' u'outstading' u'canonized' u'unholiest'\n",
      " u'dicretcion' u'timeouts' u'accorded' u'renaldo' u'tisa' u'wahtever'\n",
      " u'oiled' u'appliances' u'battlecry' u'beastmaster' u'incorporating'\n",
      " u'icb' u'unadaptability' u'morcillo' u'strathearn' u'braden' u'testators'\n",
      " u'luftwaffe' u'amoung' u'huck' u'homesteading' u'holnists' u'feuding'\n",
      " u'akosua' u'delousing' u'toothless' u'kiyomasa' u'greenish' u'dammes'\n",
      " u'oozed' u'sylvie' u'awwww' u'inheriting' u'counciling' u'agamemnon'\n",
      " u'gamma' u'dustman' u'qwest' u'wht' u'wambini' u'ozdemir' u'applebloom'\n",
      " u'arrogated' u'donahoe' u'couches' u'relent' u'morimoto' u'thugees'\n",
      " u'incompetente' u'repairwoman' u'sassquatch' u'soundstage' u'botch'\n",
      " u'prenez' u'rediculousness' u'predating' u'chagrined' u'carpentry'\n",
      " u'yelnats' u'nihon' u'heating' u'jalees' u'ttmay' u'inhaler' u'frazier'\n",
      " u'volunteered' u'penetentiary' u'rudimentary' u'milhalovitch' u'gleb'\n",
      " u'lodgers' u'booms' u'nightingale' u'abolish' u'philosophise' u'transfix'\n",
      " u'andean' u'declaim' u'godlessness' u'alamo' u'executing' u'rangi'\n",
      " u'doofuses' u'antinori' u'urged' u'wycherly' u'alleviated' u'hyksos'\n",
      " u'chili' u'gobble' u'kolonel' u'blemishes' u'neigh' u'bounders'\n",
      " u'mollecular' u'ssing' u'pudney' u'lingual' u'jt' u'avventura' u'musing'\n",
      " u'mists' u'accustom' u'streaking' u'tolerably' u'luj' u'hapi' u'lionell'\n",
      " u'sleepover' u'sparked' u'roistering' u'simmering' u'carapace' u'swine'\n",
      " u'multiplicity' u'kushiata' u'gerde' u'littlesearch' u'theisen'\n",
      " u'binomial' u'shockless' u'decries' u'jokingly' u'employers'\n",
      " u'butterworth' u'ravishingly' u'pucci' u'playroom' u'seagual'\n",
      " u'charactercan' u'notorius' u'satires' u'illustrations' u'bedford'\n",
      " u'drying' u'branching' u'riiiight' u'fsb' u'blouses' u'mjyoung'\n",
      " u'scontrived' u'miming' u'excrete' u'spectacles' u'tragical' u'bwahahha'\n",
      " u'flush' u'hallward' u'buchinsky' u'luisa' u'duces' u'goyer'\n",
      " u'starkfield' u'ghostlike' u'plainclothes' u'preparating' u'schweiger'\n",
      " u'burlesks' u'getas' u'speckhahn' u'corpsman' u'cheezoid' u'fraggles'\n",
      " u'ofcorse' u'borkowski' u'beet' u'disabilities' u'hrer' u'soundless'\n",
      " u'pinky' u'neseri' u'nietzcheans' u'tamaras' u'pourri' u'rotne'\n",
      " u'maynamar' u'suckotrocity' u'meme' u'sunbed' u'surmounting' u'atenism'\n",
      " u'poacher' u'countermanded' u'inculcate' u'hurdes' u'karmas' u'pambieri'\n",
      " u'harlotry' u'uninterestingly' u'stagiest' u'josey' u'yammering'\n",
      " u'maigret' u'impale' u'finito' u'slitting' u'eklund' u'historia'\n",
      " u'sunrising' u'razing' u'phoenicia' u'ec' u'obliterated' u'grrrrrrrrrr'\n",
      " u'rheumy' u'incapacitated' u'gooodie' u'mammary' u'countrywoman'\n",
      " u'ribaldry' u'hrishita' u'opportunites' u'earphones' u'browbeaten'\n",
      " u'coercion' u'doon' u'xenia' u'buzzed' u'throughoutnick' u'unerotic'\n",
      " u'henchpeople' u'lascivious' u'recognzie' u'mankin' u'beeman' u'woodrow'\n",
      " u'fangorn' u'approxiately' u'jowett' u'wort' u'clenches' u'hollandish'\n",
      " u'philippians' u'firma' u'lexa' u'nesting' u'mochanian' u'bigamous'\n",
      " u'dirties' u'ritualistically' u'sherawali' u'principalled' u'prolongs'\n",
      " u'winkimation' u'substation' u'remorselessly' u'stereos' u'pretence'\n",
      " u'unbeatable' u'videobox' u'militar' u'maginnis' u'cropping' u'sutra'\n",
      " u'immaculacy' u'schwedt' u'floozie' u'precluded' u'reelers' u'gorged'\n",
      " u'belenguer' u'typhoon' u'lisp' u'feasts' u'nare' u'thundercloud'\n",
      " u'mindedly' u'jigging' u'elivra' u'lambencies' u'griebe' u'buzby'\n",
      " u'stupiditiy' u'broadhurst' u'phili' u'tummy' u'missionaries' u'pledged'\n",
      " u'espace' u'ninagawa' u'snubs' u'vaugier' u'mutilates' u'maleness'\n",
      " u'dalloway' u'repute' u'atonal' u'progressional' u'zaitung' u'crialese'\n",
      " u'mettler' u'willets' u'hanibal' u'puzzu' u'sweltering' u'dobbs'\n",
      " u'walkleys' u'belaney' u'sipho' u'suckhole' u'usmoviegoers' u'febuary'\n",
      " u'mccullum' u'crossroad' u'hel' u'hyperrealism' u'honeyed' u'rooshus'\n",
      " u'jumble' u'insectoid' u'heffalump' u'brooms' u'scherfig' u'dredges'\n",
      " u'initiating' u'drusse' u'grayish' u'zmeu' u'sine' u'straightforwardly'\n",
      " u'paternal' u'upholding' u'marum' u'spur' u'chaps' u'vauxhall'\n",
      " u'gratuity' u'correspondents' u'gelin' u'epstein' u'halarious' u'sweeny'\n",
      " u'mandolin' u'pluckin' u'deeming' u'iteration' u'waltzer' u'consultants'\n",
      " u'duquenne' u'samuari' u'severeid' u'reallypicking' u'heartthrobs'\n",
      " u'masturbatory' u'maddern' u'kvc' u'strobes' u'humanism' u'pleantly'\n",
      " u'geocentric' u'minimised' u'enterieur' u'nbr' u'sukumari' u'choregraphy'\n",
      " u'dopiness' u'paracetamol' u'tablets' u'roadhouse' u'handcuffs'\n",
      " u'charger' u'relocations' u'discriminated' u'waaaaaayyyy' u'recruitment'\n",
      " u'accord' u'goldwait' u'ravishment' u'sunbow' u'gorezone' u'crackhouse'\n",
      " u'mythically' u'waif' u'boozing' u'arcy' u'walthall' u'cmon' u'zannetti'\n",
      " u'hasebe' u'begbie' u'lubezki' u'llareggub' u'protestation' u'eminem'\n",
      " u'cocktails' u'creakiness' u'giornata' u'himmler' u'hitchhiking' u'mutti'\n",
      " u'sixsome' u'haruhi' u'girolomo' u'zapp' u'dizziness' u'evermore'\n",
      " u'wismaster' u'koolhoven' u'maud' u'baths' u'tlr' u'bama' u'tumba'\n",
      " u'gair' u'meridian' u'undergirder' u'expositions' u'sumatra' u'aubrac'\n",
      " u'jucier' u'steamroller' u'macclannough' u'jingle' u'stetson'\n",
      " u'enduringly' u'starcrash' u'rosanne' u'abrahimi' u'charries' u'rigors'\n",
      " u'windmills' u'sitka' u'coached' u'ableto' u'extentions'\n",
      " u'charactersmickey' u'nobdy' u'streamwood' u'uncronological' u'dissuaded'\n",
      " u'awayin' u'appendectomy' u'euthenized' u'consacrates' u'rosiland'\n",
      " u'cegonhas' u'joshuu' u'swardson' u'evict' u'payday' u'reconstruct'\n",
      " u'maoism' u'rigamarole' u'moneywise' u'logothetis' u'wujing' u'sfxthere'\n",
      " u'unravelled' u'melfi' u'chodorov' u'woodenhead' u'rebedahl' u'jeeze'\n",
      " u'gubbels' u'kadie' u'picturazation' u'cuse' u'hawkeye' u'gaita'\n",
      " u'donovon' u'orangeish' u'cass' u'guire' u'application' u'stinkbombs'\n",
      " u'deadpool' u'clutch' u'gencon' u'direness' u'straws' u'boro' u'hem'\n",
      " u'suspensefully' u'trequel' u'flashier' u'ridgeley' u'sirin' u'excorsist'\n",
      " u'elysee' u'principalled' u'tokar' u'theyremember' u'witer' u'alternates'\n",
      " u'kitt' u'clyton' u'unrevealed' u'whispery' u'ericson' u'surroundsound'\n",
      " u'harve' u'underlie' u'cornier' u'giannopoulos' u'blueray' u'virtuouso'\n",
      " u'groaner' u'megahit' u'chiropractics' u'congratulates' u'microcosm'\n",
      " u'rade' u'tenfold' u'forchupacabras' u'comyn' u'boylen' u'craptown'\n",
      " u'thuds' u'trachea' u'vitamin' u'prfessionalism' u'sentinels'\n",
      " u'boymyself' u'sleezeball' u'marmalade' u'udders' u'tarte' u'soledad'\n",
      " u'mercedez' u'lovethis' u'camilo' u'squadroom' u'aiieeee' u'swanston'\n",
      " u'bickley' u'undeservedlyfirst' u'mctaketh' u'leonida' u'tatty' u'mma'\n",
      " u'firefly' u'arrant' u'ignoti' u'faker' u'wastefully' u'tomes'\n",
      " u'reinventing' u'ashamedly' u'harborfest' u'buffing' u'thade' u'ryov'\n",
      " u'libs' u'atmos' u'paparatzi' u'inessential' u'eelam' u'gamestation'\n",
      " u'rican' u'doped' u'kolb' u'hotheaded' u'posest' u'upping' u'hemisphere'\n",
      " u'pitiable' u'maglite' u'chorale' u'oases' u'whoring' u'peepants'\n",
      " u'grappled' u'genuis' u'banham' u'mesmerize' u'burley' u'lamotte'\n",
      " u'mcaffee' u'sereia' u'crewdson' u'lucklily' u'characterless' u'mepretty'\n",
      " u'kaishakunin' u'amrutlal' u'venezuelan' u'rovner' u'penning' u'spanks'\n",
      " u'piranhas' u'mirna' u'whitby' u'doghi' u'geometrically' u'millar'\n",
      " u'despited' u'pts' u'osric' u'shekels' u'democracies' u'crayons'\n",
      " u'monette' u'monotonal' u'receptive' u'flabbergasted' u'limousine'\n",
      " u'convincedness' u'redbird' u'upton' u'bagdarasian' u'crystallises'\n",
      " u'vuyire' u'bafta' u'overfed' u'berdalh' u'tucsos' u'recomendations'\n",
      " u'codger' u'masonite' u'malpractice' u'aaa' u'solstice' u'truncheon'\n",
      " u'jeesh' u'malmstein' u'derogatory' u'jonesie' u'untenable' u'coud'\n",
      " u'opportunites' u'kowalkski' u'dintenfass' u'miscegenation' u'amine'\n",
      " u'roguishly' u'ua' u'pazira' u'jaayen' u'groovin' u'comely'\n",
      " u'crapsterpiece' u'columbos' u'lollilove' u'hoarder' u'kamm'\n",
      " u'mongolians' u'barthody' u'cudney' u'havok' u'terrell' u'wooer'\n",
      " u'carnet' u'anabel' u'kajlich' u'brophy' u'flanigan' u'assante'\n",
      " u'graphed' u'monograph' u'zombiekilla' u'sonoma' u'brokering'\n",
      " u'instalments' u'ingnorant' u'forsake' u'pla' u'vivien' u'bardot'\n",
      " u'diddy' u'mio' u'baja' u'grilling' u'degrade' u'hasebe' u'skookum']\n"
     ]
    }
   ],
   "source": [
    "print(np.random.choice(words_not_found, 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly split 20000 pad sequences for training, 5000 for validation\n"
     ]
    }
   ],
   "source": [
    "# Split train_sequences into train and validation. Ratio: 80/20\n",
    "VALIDATION_SPLIT = 0.2\n",
    "np.random.seed(1234)\n",
    "\n",
    "# \n",
    "perm = np.random.permutation(len(train_sequences))\n",
    "index_train = perm[:int(len(train_sequences)*(1-VALIDATION_SPLIT))]\n",
    "index_val = perm[int(len(train_sequences)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "x_train = train_pad_sequences[index_train]\n",
    "x_val = train_pad_sequences[index_val]\n",
    "y_train = labeled_train[\"sentiment\"][index_train].tolist()\n",
    "y_val = labeled_train[\"sentiment\"][index_val].tolist()\n",
    "\n",
    "print('Randomly split %d pad sequences for training, %d for validation' % (len(x_train) ,len(x_val)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test_pad_sequences\n",
    "y_test = test[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, Conv1D, MaxPooling1D ,GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer1_cnn(dropout=0.2, num_filters=64, kernel_size=2):\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer3_cnn():\n",
    "    model = Sequential()\n",
    "    \n",
    "    NUM_FILTERS = 64\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 500, 300)          30413100  \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 499, 64)           38464     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,453,677\n",
      "Trainable params: 40,577\n",
      "Non-trainable params: 30,413,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer1_cnn_model = layer1_cnn()\n",
    "layer1_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 5s 255us/step - loss: 0.4418 - acc: 0.7960 - val_loss: 0.3181 - val_acc: 0.8682\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.2932 - acc: 0.8796 - val_loss: 0.3031 - val_acc: 0.8720\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2594 - acc: 0.8953 - val_loss: 0.2861 - val_acc: 0.8792\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2230 - acc: 0.9132 - val_loss: 0.2588 - val_acc: 0.8944\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.1964 - acc: 0.9245 - val_loss: 0.2571 - val_acc: 0.8944\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.1746 - acc: 0.9335 - val_loss: 0.2606 - val_acc: 0.8948\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.1526 - acc: 0.9444 - val_loss: 0.2644 - val_acc: 0.8914\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.1340 - acc: 0.9525 - val_loss: 0.2705 - val_acc: 0.8914\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.1120 - acc: 0.9638 - val_loss: 0.3025 - val_acc: 0.8872\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.0921 - acc: 0.9712 - val_loss: 0.2925 - val_acc: 0.8924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda6d013f90>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_cnn_model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_2 (Embedding)      (None, 500, 300)          30413100  \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 499, 64)           38464     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 249, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 248, 64)           8256      \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 124, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 123, 64)           8256      \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 30,470,189\n",
      "Trainable params: 57,089\n",
      "Non-trainable params: 30,413,100\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "layer3_cnn_model = layer3_cnn()\n",
    "layer3_cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 6s 307us/step - loss: 0.4339 - acc: 0.7896 - val_loss: 0.3334 - val_acc: 0.8586\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 5s 247us/step - loss: 0.2989 - acc: 0.8774 - val_loss: 0.3070 - val_acc: 0.8736\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 5s 248us/step - loss: 0.2567 - acc: 0.8985 - val_loss: 0.2905 - val_acc: 0.8804\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 5s 248us/step - loss: 0.2339 - acc: 0.9083 - val_loss: 0.2764 - val_acc: 0.8862\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 5s 249us/step - loss: 0.1972 - acc: 0.9257 - val_loss: 0.2814 - val_acc: 0.8846\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 5s 249us/step - loss: 0.1693 - acc: 0.9397 - val_loss: 0.2913 - val_acc: 0.8874\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 5s 249us/step - loss: 0.1464 - acc: 0.9490 - val_loss: 0.3399 - val_acc: 0.8736\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 5s 249us/step - loss: 0.1215 - acc: 0.9590 - val_loss: 0.3176 - val_acc: 0.8862\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 5s 248us/step - loss: 0.1072 - acc: 0.9641 - val_loss: 0.3786 - val_acc: 0.8770\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 5s 247us/step - loss: 0.0847 - acc: 0.9725 - val_loss: 0.3893 - val_acc: 0.8770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda6d5d6090>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer3_cnn_model.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 4s 212us/step - loss: 0.5096 - acc: 0.7496 - val_loss: 0.3463 - val_acc: 0.8566\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.3413 - acc: 0.8613 - val_loss: 0.2962 - val_acc: 0.8760\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.2897 - acc: 0.8867 - val_loss: 0.2728 - val_acc: 0.8880\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2601 - acc: 0.9014 - val_loss: 0.2664 - val_acc: 0.8876\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2373 - acc: 0.9133 - val_loss: 0.2612 - val_acc: 0.8920\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2167 - acc: 0.9229 - val_loss: 0.2621 - val_acc: 0.8944\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.1992 - acc: 0.9290 - val_loss: 0.2579 - val_acc: 0.8962\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.1810 - acc: 0.9378 - val_loss: 0.2607 - val_acc: 0.8946\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.1641 - acc: 0.9439 - val_loss: 0.2750 - val_acc: 0.8946\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 4s 197us/step - loss: 0.1465 - acc: 0.9518 - val_loss: 0.2859 - val_acc: 0.8928\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda341157d0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_cnn_model_dropout5 = layer1_cnn(dropout=0.5)\n",
    "layer1_cnn_model_dropout5.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_layer1_cnn = layer1_cnn_model.predict(x_test)\n",
    "y_test_pred_layer1_cnn_dropout5 = layer1_cnn_model_dropout5.predict(x_test)\n",
    "y_test_pred_layer3_cnn = layer3_cnn_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_binary(predicts):\n",
    "    return map(lambda predict: 1 if predict > 0.5 else 0, predicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_layer1_cnn = to_binary(y_test_pred_layer1_cnn)\n",
    "y_test_pred_layer1_cnn_dropout5 = to_binary(y_test_pred_layer1_cnn_dropout5)\n",
    "y_test_pred_layer3_cnn = to_binary(y_test_pred_layer3_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, roc_auc_score, roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_auc_score(model_name, y_test_pred):\n",
    "    print(\"The AUC score for %s is : %.4f.\" % (model_name, roc_auc_score(y_test, y_test_pred)))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for layer1 cnn is : 0.8946.\n",
      "The AUC score for layer1 cnn dropout 0.5 is : 0.8958.\n",
      "The AUC score for layer3 cnn is : 0.8770.\n"
     ]
    }
   ],
   "source": [
    "print_auc_score(\"layer1 cnn\", y_test_pred_layer1_cnn)\n",
    "print_auc_score(\"layer1 cnn dropout 0.5\", y_test_pred_layer1_cnn_dropout5)\n",
    "print_auc_score(\"layer3 cnn\", y_test_pred_layer3_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 5s 272us/step - loss: 0.4690 - acc: 0.7767 - val_loss: 0.3150 - val_acc: 0.8688\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 5s 247us/step - loss: 0.3098 - acc: 0.8764 - val_loss: 0.2832 - val_acc: 0.8804\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 5s 246us/step - loss: 0.2658 - acc: 0.8969 - val_loss: 0.2642 - val_acc: 0.8904\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 5s 246us/step - loss: 0.2322 - acc: 0.9105 - val_loss: 0.2500 - val_acc: 0.8952\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 5s 247us/step - loss: 0.2009 - acc: 0.9278 - val_loss: 0.2478 - val_acc: 0.8940\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 5s 247us/step - loss: 0.1788 - acc: 0.9364 - val_loss: 0.2539 - val_acc: 0.8998\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 5s 246us/step - loss: 0.1469 - acc: 0.9519 - val_loss: 0.2592 - val_acc: 0.9016\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 5s 247us/step - loss: 0.1213 - acc: 0.9610 - val_loss: 0.2616 - val_acc: 0.9022\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 5s 246us/step - loss: 0.0985 - acc: 0.9698 - val_loss: 0.2650 - val_acc: 0.9018\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 5s 246us/step - loss: 0.0826 - acc: 0.9759 - val_loss: 0.2772 - val_acc: 0.9008\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda120f04d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_cnn_32f_5d = layer1_cnn(dropout=0.5, num_filters=128)\n",
    "layer1_cnn_32f_5d.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer1_cnn_d(dropout=0.2, num_filters=64, kernel_size=2):\n",
    "    model = Sequential()\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "#     model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "#     model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "#     model.add(MaxPooling1D(2))\n",
    "    model.add(Conv1D(filters=num_filters, kernel_size=kernel_size, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/10\n",
      "20000/20000 [==============================] - 4s 219us/step - loss: 0.5107 - acc: 0.7386 - val_loss: 0.3429 - val_acc: 0.8556\n",
      "Epoch 2/10\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.3586 - acc: 0.8453 - val_loss: 0.3032 - val_acc: 0.8696\n",
      "Epoch 3/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.3223 - acc: 0.8645 - val_loss: 0.2859 - val_acc: 0.8776\n",
      "Epoch 4/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2897 - acc: 0.8799 - val_loss: 0.2887 - val_acc: 0.8780\n",
      "Epoch 5/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2607 - acc: 0.8947 - val_loss: 0.2709 - val_acc: 0.8850\n",
      "Epoch 6/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2495 - acc: 0.9004 - val_loss: 0.2855 - val_acc: 0.8768\n",
      "Epoch 7/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2259 - acc: 0.9123 - val_loss: 0.2655 - val_acc: 0.8882\n",
      "Epoch 8/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2094 - acc: 0.9200 - val_loss: 0.2649 - val_acc: 0.8912\n",
      "Epoch 9/10\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.1996 - acc: 0.9239 - val_loss: 0.2732 - val_acc: 0.8876\n",
      "Epoch 10/10\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.1845 - acc: 0.9303 - val_loss: 0.2702 - val_acc: 0.8880\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda10f4c310>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_cnn_d_0 = layer1_cnn_d()\n",
    "layer1_cnn_d_0.fit(x_train, y_train, batch_size=64, epochs=10, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for layer1 cnn_d is : 0.8910.\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_layer1_cnn_d_0 = layer1_cnn_d_0.predict(x_test)\n",
    "y_test_pred_layer1_cnn_d_0 = to_binary(y_test_pred_layer1_cnn_d_0)\n",
    "print_auc_score(\"layer1 cnn_d\", y_test_pred_layer1_cnn_d_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 4s 221us/step - loss: 0.6255 - acc: 0.6331 - val_loss: 0.4290 - val_acc: 0.8296\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 4s 193us/step - loss: 0.4607 - acc: 0.7943 - val_loss: 0.3544 - val_acc: 0.8548\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.3976 - acc: 0.8306 - val_loss: 0.3178 - val_acc: 0.8698\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.3594 - acc: 0.8496 - val_loss: 0.3022 - val_acc: 0.8750\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 4s 193us/step - loss: 0.3376 - acc: 0.8623 - val_loss: 0.3034 - val_acc: 0.8728\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.3188 - acc: 0.8688 - val_loss: 0.2804 - val_acc: 0.8810\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.3037 - acc: 0.8781 - val_loss: 0.2817 - val_acc: 0.8798\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2922 - acc: 0.8839 - val_loss: 0.2732 - val_acc: 0.8858\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2834 - acc: 0.8881 - val_loss: 0.2770 - val_acc: 0.8810\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2759 - acc: 0.8893 - val_loss: 0.2647 - val_acc: 0.8868\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.2683 - acc: 0.8946 - val_loss: 0.2625 - val_acc: 0.8926\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.2580 - acc: 0.9011 - val_loss: 0.2614 - val_acc: 0.8882\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 4s 193us/step - loss: 0.2525 - acc: 0.9014 - val_loss: 0.2613 - val_acc: 0.8918\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 4s 196us/step - loss: 0.2457 - acc: 0.9049 - val_loss: 0.2643 - val_acc: 0.8864\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 4s 196us/step - loss: 0.2382 - acc: 0.9066 - val_loss: 0.2632 - val_acc: 0.8868\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 4s 194us/step - loss: 0.2306 - acc: 0.9108 - val_loss: 0.2563 - val_acc: 0.8932\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2327 - acc: 0.9096 - val_loss: 0.2577 - val_acc: 0.8934\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 4s 191us/step - loss: 0.2276 - acc: 0.9108 - val_loss: 0.2673 - val_acc: 0.8908\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2224 - acc: 0.9141 - val_loss: 0.2625 - val_acc: 0.8952\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 4s 192us/step - loss: 0.2119 - acc: 0.9166 - val_loss: 0.2587 - val_acc: 0.8954\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda10da7310>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer1_cnn_d_5d = layer1_cnn_d(dropout=0.5)\n",
    "layer1_cnn_d_5d.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The AUC score for layer1 cnn_d is : 0.8926.\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_layer1_cnn_d_5d = layer1_cnn_d_5d.predict(x_test)\n",
    "y_test_pred_layer1_cnn_d_5d = to_binary(y_test_pred_layer1_cnn_d_5d)\n",
    "print_auc_score(\"layer1 cnn_d\", y_test_pred_layer1_cnn_d_5d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer3_cnn_1p(dropout=0.2):\n",
    "    model = Sequential()\n",
    "    \n",
    "    NUM_FILTERS = 64\n",
    "\n",
    "    embedding_layer = Embedding(\n",
    "            num_words,\n",
    "            EMBEDDING_DIM,\n",
    "            weights=[embedding_matrix],\n",
    "            input_length=MAX_SEQUENCE_LENGTH_FOR_KERAS_RNN,\n",
    "            trainable=False)\n",
    "    output_layer = Dense(1, activation='sigmoid')\n",
    "    \n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(Conv1D(filters=NUM_FILTERS, kernel_size=2, padding='valid', activation='relu', strides=1))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(output_layer)\n",
    "    \n",
    "    model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/20\n",
      "20000/20000 [==============================] - 7s 346us/step - loss: 0.4937 - acc: 0.7537 - val_loss: 0.3577 - val_acc: 0.8416\n",
      "Epoch 2/20\n",
      "20000/20000 [==============================] - 6s 301us/step - loss: 0.3437 - acc: 0.8643 - val_loss: 0.2932 - val_acc: 0.8764\n",
      "Epoch 3/20\n",
      "20000/20000 [==============================] - 6s 301us/step - loss: 0.2962 - acc: 0.8852 - val_loss: 0.2764 - val_acc: 0.8874\n",
      "Epoch 4/20\n",
      "20000/20000 [==============================] - 6s 302us/step - loss: 0.2589 - acc: 0.8980 - val_loss: 0.2732 - val_acc: 0.8852\n",
      "Epoch 5/20\n",
      "20000/20000 [==============================] - 6s 302us/step - loss: 0.2345 - acc: 0.9125 - val_loss: 0.2707 - val_acc: 0.8846\n",
      "Epoch 6/20\n",
      "20000/20000 [==============================] - 6s 300us/step - loss: 0.2161 - acc: 0.9182 - val_loss: 0.2760 - val_acc: 0.8894\n",
      "Epoch 7/20\n",
      "20000/20000 [==============================] - 6s 302us/step - loss: 0.1923 - acc: 0.9281 - val_loss: 0.2798 - val_acc: 0.8888\n",
      "Epoch 8/20\n",
      "20000/20000 [==============================] - 6s 301us/step - loss: 0.1674 - acc: 0.9394 - val_loss: 0.2898 - val_acc: 0.8886\n",
      "Epoch 9/20\n",
      "20000/20000 [==============================] - 6s 302us/step - loss: 0.1570 - acc: 0.9424 - val_loss: 0.3026 - val_acc: 0.8900\n",
      "Epoch 10/20\n",
      "20000/20000 [==============================] - 6s 302us/step - loss: 0.1343 - acc: 0.9530 - val_loss: 0.3177 - val_acc: 0.8850\n",
      "Epoch 11/20\n",
      "20000/20000 [==============================] - 6s 301us/step - loss: 0.1159 - acc: 0.9602 - val_loss: 0.3436 - val_acc: 0.8866\n",
      "Epoch 12/20\n",
      "20000/20000 [==============================] - 6s 302us/step - loss: 0.1078 - acc: 0.9637 - val_loss: 0.4284 - val_acc: 0.8648\n",
      "Epoch 13/20\n",
      "20000/20000 [==============================] - 6s 300us/step - loss: 0.0860 - acc: 0.9701 - val_loss: 0.4073 - val_acc: 0.8838\n",
      "Epoch 14/20\n",
      "20000/20000 [==============================] - 6s 302us/step - loss: 0.0721 - acc: 0.9752 - val_loss: 0.4725 - val_acc: 0.8698\n",
      "Epoch 15/20\n",
      "20000/20000 [==============================] - 6s 301us/step - loss: 0.0656 - acc: 0.9782 - val_loss: 0.4567 - val_acc: 0.8840\n",
      "Epoch 16/20\n",
      "20000/20000 [==============================] - 6s 302us/step - loss: 0.0575 - acc: 0.9815 - val_loss: 0.5094 - val_acc: 0.8814\n",
      "Epoch 17/20\n",
      "20000/20000 [==============================] - 6s 300us/step - loss: 0.0622 - acc: 0.9788 - val_loss: 0.4821 - val_acc: 0.8846\n",
      "Epoch 18/20\n",
      "20000/20000 [==============================] - 6s 301us/step - loss: 0.0476 - acc: 0.9843 - val_loss: 0.5298 - val_acc: 0.8856\n",
      "Epoch 19/20\n",
      "20000/20000 [==============================] - 6s 302us/step - loss: 0.0411 - acc: 0.9870 - val_loss: 0.5750 - val_acc: 0.8862\n",
      "Epoch 20/20\n",
      "20000/20000 [==============================] - 6s 302us/step - loss: 0.0420 - acc: 0.9857 - val_loss: 0.5975 - val_acc: 0.8858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fda1181f890>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer3_cnn_1p_0 = layer3_cnn_1p(dropout=0.5)\n",
    "layer3_cnn_1p_0.fit(x_train, y_train, batch_size=64, epochs=20, validation_data=[x_val, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
